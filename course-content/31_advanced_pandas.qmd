---
title: "Advanced Pandas"
teaching: 45
exercises: 10
questions:
- "Fill this in!"
objectives:
- "Fill this in too!"
keypoints:
- "Same verse!"
---

<center>
    <img src="images/advanced-pandas/advanced_pandas.jpeg" 
    width="60%" style="margin:auto"/>
    <p style="text-align: center">
        Advanced. Pandas.
    </p>
</center>

In this section we will go over some Pandas functionality that is slightly more involved than the prior sessions. We hope to cover, or at least provide reference for operations such as:

 * Conditional Column Assignment
 * Advanced grouping techniques
 * Rolling and cumulative functions
 * Reshaping our data
 * Iterating over a dataframe 

## Conditional Column Assignment

TBD (.where and .mask)

## Advanced Grouping

In our previous section on Pandas, we explored the basic tools we need to group our data by various columns, apply a host of statistical aggregation operations, and look at these outcomes. While incredibly powerful as is, these tools lack some functionality that you might wish to have. Some examples include:

 * Combining row level values and group level aggregations in a single command.
 * Filtering and subsetting a dataframe based on group values. 
 * Combining aggregations across different columns. 

In order to handle these operations, we need to introduce two new methods: `transform` and `apply`. 

:::{.callout-tip}
## Apply... again?

You might question whether `apply` is actually a new function, seeing as we have used it previously when creating new columns in a dataframe. In fact, the effect of `apply` within pandas is subtly different depending on how and where we apply it. The effect of `apply` is different when used on:

 * A dataframe as a whole - `dataframe.apply(...)`
 * A single series - `series.apply(...)`
 * A grouped dataframe object - `dataframe.groupby('column').apply(...)`

Make sure you keep track of what kind of apply you are using when doing your dataframe manipulations, as identical functions in different scenarios can lead to unusually different results! In this section, we are focusing on the **grouped dataframe objects**. 
:::

Both `transform` and `apply` can be used on a grouped dataframe object. This is the object that is created when we use the `.groupby()` method. Let's create a small dataset and see what this object looks like:
```{python}
import pandas as pd

df = pd.DataFrame(
    {
        'unit' : [
            'Blaster', 'Blaster', 'Blaster', 
            'Lightsaber', 'Lightsaber', 'Lightsaber', 'Lightsaber', 
            'Stick', 'Stick', 'Stick'
            ], 
        'cost' : [42, 60, 40, 900, 4000, 2000, 100, 10, 1, 5],
        'sale_price' : [50, 75, 42, 1000, 5000, 2000, 4242, 4, 2, 20]
    }
)

df
```

Here we have a column that looks categorical (`unit`), and a couple that look numerical (`cost` and `sale_price`). We are going to focus on grouping our data according to the categorical column, and see what we can do with the numerical columns. 

```{python}
grouped_df = df.groupby('unit')
grouped_df
```

Note that this object is no longer a dataframe, but some sort of `DataFrameGroupBy` object. By itself, it is not very useful, but we can use it to get all sorts of interesting information. Recall briefly how we used this earlier. Maybe we want the average buy and sell prices of each type of unit:

```{python}
grouped_df.mean().reset_index()
```

What if we wanted to know how much each particular unit contributed to it's groups overall sale? Or perhaps how much each group profited, on average? This is where `transform` and `apply` come in. Let's look at what each of these do.

### Transform

 * Used to apply a function to a dataframe that produces a new dataframe with the same shape as the original.
 * While an entire dataframe can be passed to transform, it only ever sees (operates on) a single column/series at a time. 
 * The function must either return a scalar value or a sequence that is the **same length** as the original dataframe. 
 * Useful for getting groups aggregations back into the original dataframe
 * Useful for filtering based on group values 


#### Ex. 1: Percent of Group
Let's see each this in action. We will use transform to determine how much each unit contributed to the total sales of its group. 

```{python}
pct_of_sales = (
    grouped_df
    .sale_price
    .transform(lambda series_: series_/series_.sum())
)
pct_of_sales
```

Notice here that we used the `lambda` function again, which allows us to define a function inline acting on a given argument (in this case, the argument is named `series_`). This let us allow the value of the series to be the sum of the series. Because we are acting on a *grouped* object, Pandas knows that the sum we wish to employ is that of the group, not the overall sum! This new object will be in the same order (have the same indexing) as our initial dataframe, and so we can add it back to the dataframe as an extra column if we wish. This works explicitly because the transform method is required to return a series of the same length as the original object, giving us exactly what we want! 

```{python}
df['percent_of_sales'] = pct_of_sales
df
```

#### Ex 2: Grouped Aggregate
Transform can be used exclusively with an aggregation function as well, but it will still return a value for every row of the initial dataset. This allows us to add grouped level values as columns to the data:

```{python}
df['average_unit_cost'] = grouped_df.cost.transform('mean')
df
```

#### Ex 3: Multiple Series
Instead of passing a single column to the transform method, we could also pass the entire dataframe (or some subset of the columns in the dataframe), and calculate values for each series:

```{python}
grouped_df.transform(lambda series_: (series_-series_.mean())/series_.std())
```

This has created two columns that have both had the same function applied. Note that if we wanted to merge these in with the original dataframe, we should be careful to rename the columns so that they do not overlap!

#### Ex 4: Filtering Original Dataframe
Another use case for transform is in filtering a dataframe. Maybe we want to zero in on those items where the sale price of the object was less than the average unit cost. Let's filter our dataframe to only those rows:

```{python}
condition = df.sale_price < grouped_df.cost.transform(lambda x: x.mean())
df[condition]
```


### Apply

 * Used to apply a function (aggregated or otherwise) across multiple columns
 * Implicitly passes all the columns of the dataframe *as* a dataframe to the function, allowing for column interactions
 * The function can return a scalar or a sequence of any length.

#### Ex 1. Aggregate over Multiple Columns
Let's try something with apply. What if we want to know the average overall profit of each group. We could produce a profit column, group up on the unit, and then calculate the mean. 

```{python}
df['profit'] = df.sale_price - df.cost
df.groupby('unit').profit.mean()
```

We could also do this using the apply function, applied to our grouped object:

```{python}
grouped_df.apply(lambda df_: (df_.sale_price - df_.cost).mean())
```

Again, we made use of that `lambda` function, this time applying it to a `df_` argument. You might notice that here I used a `df_` argument instead of `series_`: this is merely notation, and we could have used anything (`series_`, `df_`, `x`, `this_is_my_argument` would all work the same). However, we have used `df_` and `series_` so that we can remind ourselves exactly what type of object we are acting on. 

To see how transform differs from apply, let's try to do that exact same operation:

```{python}
#| error: true
grouped_df.transform(lambda _df: (_df.sale_price - _df.cost).mean())
```

This fails because the object being acted on inside transform is itself just a series object, not the entire dataframe! As such, it has no attribute for `sale_price` or `buy_price` like our original dataframe does. Instead, it acts on the `sale_price` series, then the `buy_price` series, and returns the results.

#### Ex 2. Mix Row/Aggregate Levels
While the first apply example returns a rolled up aggregated dataframe, we can also use apply to return the individual rows of the dataframe by mixing aggregation functions with row level functions. 

```{python}
grouped_df.apply(lambda df_: df_.sale_price - df_.cost.mean())
```

In this way we have to be careful: transform will **always** return a dataframe that is the same size as the original, while apply will return something that varies with the type of function we have utilized. 

#### Ex 3. Partial Aggregates
Apply lets us play some cool tricks as well. Suppose we only wanted to know about the two most expensive sales in each category. How could we filter to show this? We can use the `nlargest` (or smallest) method in conjunction with apply. `nlargest` does exactly what we might expect, returning the rows with the n largest values according to the provided column(s):

```{python}
df.nlargest(2, 'sale_price')
```

But if we mix this with apply and our grouping dataframe, we can get the largest for each group!

```{python}
grouped_df.apply(lambda df_: df_.nlargest(2, 'sale_price'))
```

Note that when apply produces some different sized aggregate than the original dataframe, it tacks on an extra index indicating what the grouper was. We do not always care about this, and can eliminate it in the initial creation of our grouped dataframe via the `group_keys` argument:

```{python}
(
    df
    .groupby('unit', group_keys=False)
    .apply(lambda df_: df_.nlargest(2, 'sale_price'))
)
```

#### Ex 4. Create Multiple Columns
We can use the dataframe behaviour of apply to create multiple additional columns all at once within an apply. Let's create a function that will act on each dataframe group, create new values, and return the dataframe.

```{python}
def create_multiple_columns(df_):
    df_['average_group_profit'] = (df_.sale_price - df_.cost).mean()
    df_['profit_relative_to_group'] = df_['profit']/df_['average_group_profit']
    return df_

grouped_df.apply(create_multiple_columns)
```

<br>
When done this way, we are explicitly returning the dataframe, regardless of if it was grouped or not. Even if all of the new columns are aggregates, this will still produce a non-aggregated output: 

```{python}
def create_multiple_aggregated_columns(df_):
    df_['average_group_profit'] = (df_.sale_price - df_.cost).mean()
    df_['average_sale_price'] = df_.sale_price.mean()
    return df_

grouped_df.apply(create_multiple_aggregated_columns)
```

## Rolling and Cumulative Functions 

TBD... (.rolling, .cumsum, .shift)

## Reshaping Data

<center>
    <img src="images/advanced-pandas/wide_vs_long.png" 
    width="60%" style="margin:auto"/>
    <p style="text-align: center">
        Wide or Long?
    </p>
</center>

Another fairly common task in data analysis is pivoting our data. Typically this means we are creating a **wide-form** table that has more columns than originally. This may be near the end of a particular analysis and we want to prepare a final table for a report, or perhaps we wish to pick out specific values from a column for more detailed inspection. Another common use of a pivot table might be for timeseries data, and we wish to separate different variables into their own individual columns. In these cases, we can use the `pivot` and `pivot_table` methods within Pandas. 

In the other direction, we may wish to 'unpivot' a dataset. This will often occur early on in an analysis, where we wish to create a **long-form** table where multiple columns have been combined into a single set of id and value columns. This will be done with the `melt` method. 

Let's explore these ideas with the gapminder dataset. 

```{python}
url = "https://raw.githubusercontent.com/bcgov/"\
    "ds-intro-to-python/main/data/gapfinder.csv"

gapminder = pd.read_csv(url)
gapminder.head()
```

### Long to Wide

<center>
    <img src="images/advanced-pandas/reshaping_pivot.png" 
    width="60%" style="margin:auto"/>
    <p style="text-align: center">
        Pivoting a Table
    </p>
</center>

#### pivot
The first thing we may wish to do is a simple pivot in which we make the table *wider* than it was before. The simplest way to do this is by the `pivot` method. This takes as input three arguments:

 * `columns`: the only **required** argument! Column(s) we wish to use for the new columns of our dataframe. If more than one is provided, a hierarchy of columns is created. 
 * `index`: the column(s) we wish to use for the index of our new dataframe. This is entirely optional, and will just default to the pre-existing index if not supplied.
 * `values`: the value(s) we wish to retain for the new dataframe. This is also optional, and will use all leftover columns if nothing is provided. If more than one column is used here, it again creates a hierarchy in the columns. 
 
The output of the `pivot` method is a new dataframe that has the requested index/columns, with the corresponding value associated with each index-column pair in the dataframe.

For instance, perhaps we wish to explore the population of each country individually overtime. In this case, we can pivot our gapminder dataset so that each of the countries in the original country column become their own column, and compare the population value against the year:

```{python}
gapminder_year_country = gapminder.pivot(
    index='year',
    columns='country',
    values='pop'
)
gapminder_year_country.head()
```

Note that doing this moves 'year' into the index of the dataframe, and gives the columns the name of 'country'. We can move 'year' back into the core of the dataframe using `reset_index()`, and remove the column name using the `name` attribute of the `columns`:
```{python}
gapminder_year_country = gapminder_year_country.reset_index()
gapminder_year_country.columns.name = None
gapminder_year_country.head()
```

We could make a slightly more complex dataset by including the continent as well in our new column scheme:

```{python}
gapminder_year_country_continent = gapminder.pivot(
    index='year',
    columns=['country', 'continent'],
    values='pop'
)
gapminder_year_country_continent.head()
```

Or by insisting that we retain information about more than just the population:
```{python}
gapminder_year_country_two_vals = gapminder.pivot(
    index='year',
    columns='country',
    values=['pop', 'lifeExp']
)
gapminder_year_country_two_vals.head()
```

What if we wish to know about the average population of each continent over time? If we try the same approach as above, we will immediately run into an issue:

```{python}
#| error: true
gapminder_year_continent = gapminder.pivot(
    index='year',
    columns='continent',
    values='pop'
)
```

Here we find that there is more than one value associated with each year-continent pairing, and so the `pivot` method does not know how to assign a value to the dataframe. This is because `pivot` is for unique values only. While it is an excellent simple-use tool, we can go one step further to deal with non-unique values. 

#### pivot_table

To address the year-continent question above, we could first group our data by continent, calculate the average population every year, and then pivot the resultant dataframe. This is a completely valid method! However, pandas also offers a built in method that will do aggregations and pivots all at once. This is similar to the functionality of Excel pivot tables, and includes some additional functionality we would not find in groupby alone. Some of the important arguments that we can use in the `pivot_table` method include: 

 * `columns`: the only **required** argument! Column(s) (and values within) we wish to use for the resultant dataframes columns.
 * `values`: the column(s) we wish to aggregate. If no columns are provided, all leftover (non column/index columns) columns will be used that make 'sense'. For example, the average value of a list of strings does not make sense, but the 'max' value of a string list could be calculated. 
 * `index`: the column(s) we wish to use for the resultant dataframes index
 * `aggfunc`: the method of aggregation we wish to use. Default is to calculate the average.
 * `fill_value`: the value to replace missing values with (after aggregation). 

Let's go through some examples with increasing complexity. The simplest thing we can do is pivot a single column, to which we will get an output for the averages of all numerical columns. 
```{python}
gapminder.pivot_table(
    columns='continent'
)
```

Next we may wish to add an index, and focus on a single value for our output. This matches the question we asked above and could not answer with the `pivot` method:

```{python}
gapminder_year_continent = gapminder.pivot_table(
    index='year',
    columns='continent',
    values='pop'
)
gapminder_year_continent.head()
```

If we tried to do the same calculation, but use a string column instead of a numerical column for the values we will get a warning about trying to use invalid columns:

```{python}
#| error: true
gapminder_year_continent_country = gapminder.pivot_table(
    index='year',
    columns='continent',
    values='country'
)
gapminder_year_continent_country.head()
```

However, we can switch our aggregation method to something that is valid for strings (using either string arguments for common aggregations such as 'max', or more complicated `numpy` or user-defined functions as well). 
```{python}
gapminder_year_continent_country = gapminder.pivot_table(
    index='year',
    columns='continent',
    values='country',
    aggfunc='max'
)
gapminder_year_continent_country.head()
```

Finally, similar to our groupby aggregations, we can also pass a dictionary to `aggfunc`. The dictionary contains the columns we wish to use for values as keys, and the type of aggregation(s) we wish to do as the values. 

```{python}
import numpy as np

gapminder_multiple_pivots = gapminder.pivot_table(
    index='year',
    columns='continent',
    aggfunc={
        'pop': [np.mean, np.max],
        'lifeExp': 'mean',
        'country': [min, max]
    }
)
gapminder_multiple_pivots.head()
```

### Wide to Long

<center>
    <img src="images/advanced-pandas/reshaping_melt.png" 
    width="60%" style="margin:auto"/>
    <p style="text-align: center">
        I'm meeelllting!
    </p>
</center>

#### melt

The opposite of pivoting is melting (...obviously?). This is often used to get data back into long-form. Long-form data will typically have all different possible categories for a single measure in a single column (having a country column instead of Algeria, Albania, Canada... Zimbabwe columns). Long-form data is typically the preferred data style when doing your actual analysis. For example, many of the Seaborns methods we considered for creating stunning visuals assume that the data is in long-form, and is optimized to split out categories internally instead of pulling from multiple different columns. 

However, as our data does not always come in the way we want it, we might have to unpivot (melt) our data to get it into long-form first. The `melt` method is how we attack this in Pandas, and it uses the following arguments:

 * `id_vars`: This is a **required** argument! Column(s) that will be used as identifiers. These are columns that we do **not** wish to unpivot, but leave as is. 
 * `value_vars`: Column(s) to unpivot. If left out, the default is to use all non-id columns. 

Let's look at one of the pivoted tables we made earlier, and try to get it back into long form:

```{python}
gapminder_year_country.head()
```

```{python}
gapminder_year_country.melt(id_vars='year').head()
```

If we wish to only keep a subset of the countries, we can do that too. We can also change the name of the resultant `variable` and `value` columns:

```{python}
gapminder_year_country.melt(
    id_vars='year',
    value_vars = ['Albania', 'Canada', 'Zimbabwe'],
    var_name = 'country',
    value_name = 'pop'
).head()
```


:::{.callout-tip collapse="true"}
## Overmelting!

When melting our data, we want to be careful, and not 'overmelt'! Think back to the original gapminder dataset:
 
```{python}
gapminder.head()
```

Would the following melt make sense?

```{python}
gapminder.melt(
    id_vars=['country', 'year', 'continent']
)
```

Probably not. Having the population, GDP, and life expectancy all in the same column might lead to confusing results. As a general rule of thumb, including multiple unique measures that have different units (population is measured in number of people, life expectancy in years, and GDP per capita in dollars per person) in the same column is typically avoided. Another way to think of it is this: if I take the average of the entire column, will that actually make sense? Try not to melt further than necessary! Striking a balance between machine-preferred (entirely long-form) and human-preferred (wide-form) is an important and useful skill when creating scripts to analyse our datasets! 
:::

These examples only touch the surface of methods of reshaping our data. To explore even further, Pandas provides a [user guide to reshaping.](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html) Check this out for even more reshaping functionality! 

## Iterating Over a Dataframe

TBD... (.iterrows)












