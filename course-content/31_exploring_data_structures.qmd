---
title: Exploring Data Structures
teaching: 45
author: Stuart Hemerling
exercises: 4
objectives:
  - to be able to understand and use some basic tools to manipulate dataframes
  - to be able to understand and use tools to create a subset of data based on logical filtering
  - to reinforce use of basic summarizing methods with the data
keypoints: null
jupyter: python3
---

# Exploring and Understanding Data

Getting a high level summary of the data is important but data is particularly valuable when refined. Your analysis will start to come alive when we start to do some slicing and dicing and grouping of data or even creating additional variables. In an Excel world, this is like when you use filter options for columns, or create pivot tables, or when you create a formula in a new column to create a new variable. In data science parlance, this is the kind of thing that is referred to as `data wrangling` - getting the (already cleaned) data you want in the form you want it in. 

In the world of python, this usually means working with a library or package you have already been introduced to called pandas. It lets you do so much!

The first dataset we'll look at is one that looks at data for countries around the world and shows population levels as well as gdp per capita over a number of years. It is a great data set to look at. Let's import the pandas library and then use the `.read_csv()` function to get some population by country data. 



```{python}
import pandas as pd
url = "https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/gapfinder.csv"
df = pd.read_csv(url)
```

It's usually a good idea right away to take a quick look at the data to make sure what we have read in makes sense. The `.info()` method prints the number of columns, column labels, column data types, memory usage, range index, and the number of cells in each column (non-null values).


:::{.callout-tip}
## Reminder: Naming variables!
When you create a variable or object, you can name it pretty much whatever you want. You will see widespread use of the name "df". Apart from "df", it is considered good practice to name dataframes something that is at the same time intuitive but not overly verbose. Also remember to not leave spaces in your names, and to always be consistent in the naming conventions you use (e.g. camelCase, underscore_case, etc.)
:::

```{python}
df.info()
```

So this data is looking good so far. We can see that we have created a pandas dataframe within our python environment. There are 1704 rows of data and six columns or variables. You can see there are only non-null values... so happily there is no missing data to worry about.

The `.head()` pandas method lets us look at actual data as one does in Excel.

```{python}
df.head(2)
```

This dataset has some high level country metrics about population and the economy going back to 1952. 

## Selecting columns (variables)

In the "real world" of data it is not uncommon to see hundreds or even thousands of columns in a single dataframe, so knowing how to pare down our dataset so it is not overly bloated is important. 

<center>
    <img src="images/exploring-data-structures/select_columns.png" 
    width=700 style="margin:auto"/>
    <p style="text-align: center">
       Selecting Columns
    </p>
    <br/>
</center>

First things first. If we want to look the contents of a single column, we could do it like this, specifying the column name after the . without parentheses:

```{python}
df.country
```


When we want to select more than one column to include in our dataframe, we use the convention df[['col1', 'col2']] to select the columns we want.

In our example below, we will create a new dataframe called "narrow_df", and we are primarily interested in getting the population column from the original df, but we want to have year, country, and continent in our new dataframe also.

```{python}
narrow_df = df[['country', 'year', 'pop', 'continent']]
narrow_df.head(2)
```

So now we have a more manageable dataset to work with called "narrow_df". The original dataframe "df" is still there, and still the same shape as before, we have not changed it.

Sometimes you might have many columns in a dataframe you want to keep and only a relatively few you might want to remove. In these instances, you might want to use the pandas `.drop()` method instead. We can achieve the same result as the example above by simply dropping two variables instead of naming the other four. With this method, we also need to add the specification `axis = 1`, which indicates that it is columns being referenced for being dropped. 

```{python}
narrow_df = df.drop(['lifeExp', 'gdpPercap'], axis=1)
narrow_df.head(2)
```

:::{.callout-tip}
## Reminder on Assignment!
When you create a variable or dataframe object in python, to the left of the `=` sign you always put the name of the thing you want to make or modify. To the right, that's where you put the contents of what it is you want to create. It may feel counterintuitive! Especially if you are using the same variable or dataframe identifier on both sides of the equals sign.
:::
:::{.callout-warning icon="false"}

### Challenge 1
Let's say you just want to narrow down the dataset to include just the country and the GDP per capita. How would you do it? Don't forget to run your code so it also shows a view of the result so you can confirm the code worked as you wanted it to. 

:::
:::{.callout-note icon="false" collapse="true"}

### Solution to Challenge 1

First you would like to create a new object to hold this narrowed down version of the dataset, then just use the `.head()` to see some of the resulting data. 

```{python}
challenge1_df = df[['country', 'gdpPercap']]
challenge1_df.head(2)
```
:::

## Selecting rows

Of course, in data analysis we are usually interested in looking at just some rows of data, not all of it all the time. 

<center>
    <img src="images/exploring-data-structures/select_rows.png" 
    width=700 style="margin:auto"/>
    <p style="text-align: center">
       Selecting Rows
    </p>
    <br/>
</center>

So when we want to look at just selected rows (i.e. select rows that have certain values within a given column) we can supply a condition that must be met in that column (for example equal to, less than, etc) and we will get back just the rows that match that condition:

 - equal to `==`
 - not equal to `!=`
 - less than `<`
 - greater than `> `
 - less than or equal to `<=`
 - greater than or equal to `>=`

Note that we do not use the `=` operator when it comes to specifying values we want to find in a column. This is an important point in python. The `==` operator compares the value or equality of two objects and is the right operator to use. It is a `comparison` operator. In comparison, the `=` is an `assignment` operator. We use it, for example, when we create or assign variables or objects. In the example below we see both at work.

Likely the most common way in python to select rows is to use a condition inside selection brackets `[]`

```{python}
filtered_df = df[df['year'] == 1972]
filtered_df.head(2)
```

Let's look at what is happening under the hood.  We see that pandas checks each row to see which rows fulfill the condition specified.  

```{python}
df['year'] == 1972
```

We can also expand this concept by adding additional conditions that narrow or expand the number of rows filtered. Below we use the `&` condition to indicate that both parts of the equation need to be fulfilled. 

When combining multiple conditional statements as in the example below, each condition must be surrounded by parentheses `()` within the square brackets `[]`. You must use the "or" operator `|` and/or the "and" operator `&` for the code to work. 

```{python}
filtered_df = df[(df['year'] == 1972) & (df['country'] == 'Albania')]
filtered_df.head()
```

In the example above, there is only one row of data that matches the condition.

Or we can use the `|` which indicates that if *either* of the statements are true, then all the rows satisfying either condition are returned.

```{python}
filtered_df = df[(df['year'] == 1972) | (df['country'] == 'Albania')]
filtered_df.head()
```

In the example above, there are many rows that satisfy the condition.

:::{.callout-warning icon="false"}

## Challenge 2
Your director has come to you and asked if you know what the life expectancy has been in Canada since 1992.  How would you use pandas code to get the data you need? And after having run the code, what's the answer? 
:::
:::{.callout-note icon="false" collapse="true"}

### Solution to Challenge 2

First you would like to create a new object to hold this narrowed down version of the dataset, then use some method like `.print()` or `.head()` to see some of the resulting data. 


```{python}
challenge2_df = df[(df['year'] >= 1992) & (df['country'] == 'Canada')]
print(challenge2_df)
```
:::


You will find that it is very common within python/pandas to see the `.iloc()` method.

The `.iloc()` method identifies rows based on their index value (that column to the left of the dataframe when you call the <code>.head()</code> function, for example). It can be used to look at individual rows of data by their original index value.

```{python}
df.iloc[251]
```

Or it can be used to call multiple rows in a range:

```{python}
df.iloc[0:5]
```

## Sorting rows

So now you've mastered how to select rows and columns you want, congratulations! Instead of hunting and pecking for insights, one way to quickly make some sense of the data is to sort it - something you probably do in Excel all the time. 

In the challenge above we created a dataframe "challenge2_df" with just the one country in it and from 1992. So now let's sort the data by year, and then see if we can spot any insights. To do this, we use the `.sort_values()` method.

Let's go fetch and look at the dataframe, and use this method to sort by year, going from more recent to less recent:

```{python}
challenge2_df.sort_values('year', ascending=False)
```

## Putting multiple methods together (chaining)

You can also put multiple methods in the same line of code, as long as you separate each of the elements. In pandas, this is called "method chaining",  as you are essentially of creating a chain of actions to take place. 

In the example below, the chain consists of two links:

1. Select just the rows where the country is equal to Canada AND where the year is 1997 or greater
2. Take these rows and sort them by year, going from high to low values, top to bottom

```{python}
#| scrolled: true
chained_df = df[(df['country'] == 'Canada') & (df['year'] >= 1997)].sort_values('year', ascending = False)
chained_df.head(3)
```

Another way of chaining is to put each of the functions on multiple lines.  In the example below, each line of code modifies the dataframe to acheive the same result as above.

```{python}
chained2_df = df[(df['country'] == 'Canada')]
chained2_df = chained2_df[(chained2_df['year'] >= 1997)]
chained2_df = chained2_df.sort_values('year', ascending=False)
chained2_df.head(3)
```

This latter approach is very common, as it can be easier to read the code and interpret exactly what is happening. Each respective line to the right of the `=` contains the code that modifies the object assigned to the left of it. Note that you can use a combination of chaining and piping.  

Each line is executed in sequence, so be careful when constructing code that the order in which you modify your dataframe is right. Take the example below, in which we want to find European values for GDP per capita. In it, we first have a line of code that selects the rows that contain "Europe" in the first line, then looks at the column values for "year", "country", and "gdpPercap".

```{python}
year_country_gdp = df[(df['continent'] == 'Europe')]
year_country_gdp = year_country_gdp[['year', 'country', 'gdpPercap']]
year_country_gdp.head(3)
```

Had we reversed the lines of code, and selected the "year", "country", and "gdpPercap" columns first to put into our new object, we would not have been able to sort for "Europe" in the "continent" column, as the "continent" would have been essentially removed from the dataframe in the step above, so python would have thrown an error. 

:::{.callout-warning icon="false"}

## Challenge 3
Your director has come back to you and wondered about whether the life expectancy of people changed during the 1970s in Cambodia. Use what you know about selecting rows and sorting data to get the data you need to answer the question. 
:::
:::{.callout-note icon="false" collapse="true"}

### Solution to Challenge 3

Using code to select the country Cambodia, then call `sort_values()` method and chain them together. Looking at the data we see that in the 1970s there was a sharp decline in life expectancy in Cambodia. We also see, thankfully, that it has recovered strongly since then. 

```{python}
challenge3_df = df[(df['country'] == 'Cambodia')].sort_values('year', ascending = False)
print(challenge3_df)
```

:::

## Creating new columns of data

Very often we have some data in our dataset that we want to transform to give us additional information. In Excel this is something that is done all the time by creating a formula in a cell that refers to other columns and applies some sort of logic or mathematical expression to it. 

There are different ways you can go about this in pandas. The most straightforward way is to define a new column on the left of the `=` and then reference the existing column and whatever additional conditions you would like on the right. In the example below, the existing population variable “pop” is converted to a value that shows population in millions.

```{python}
new_cols_df = df
new_cols_df['pop_millions'] = new_cols_df['pop']/1000000
new_cols_df.head(2)
```

So now we can see which countries had the biggest GDP in 2007. Notice that the GDP data is in scientific notation (i.e. the decimal number times x number of zeros), so it's a bit hard to read. If we wanted readers to consume that data we would go ahead and change the data type for it. But for current purposes we'll leave that alone.

There is a special python function called `lambda`. It is known as an anonymous function, because it is not given a name other than lambda, and can take any number of arguments in an expression.

The `.assign()` method looks at this expression with lambda in it and returns the value it is asked to do. This is how it comes all together to give us actual GDP for each row in our dataframe:

```{python}
new_df = df.assign(gdp=lambda x: x['pop'] * x['gdpPercap'])
new_df.head(5)
```

Python allows us to create our own custom functions which we can use together with lambda.  We won’t create out own function here, but they we can make functions do any number of things, like double a number or convert celcius to fahrenheit, or whatever. Let’s assume that we have created a function and assigned it the name “myfunc”.

We could and add it inline with `lambda` together with the `.apply()` function as in this example, which applies the function to a particular column:

df[‘new_col’] = df[‘old_col’]`.apply`(`lambda` x: `myfunc`(x))

Finally, the example below would call, not just one column, but *all columns* (i.e. axis=1) in a dataframe to produce the new column result:

df[‘new_col’] = df`.apply`(`lambda` x: `myfunc`(x), axis=1)



## Joining datasets together

One of the most important tasks in data analysis is to be able to join multiple datasets together. With pandas, there are functions called `.merge()` and `.join()` that are similar to each other. As `.merge()` is perhaps the more used, intuitive, and powerful of the two, we will introduce that method in this tutorial in some depth. There is also a cool function called `.concat()` that will be introduced below as well.

But first, let’s get a second file that gives us country size in square kilometers by country. We will use this data to put together with our df dataframe.

```{python}
countrysize_url = "https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/countrysize.csv"
countrysize = pd.read_csv(countrysize_url, encoding= 'unicode_escape')
countrysize.info()
countrysize.sample(2)
```

Ok, this `countrysize` dataset looks like we expect - a list of countries alongside the land size of that country in square kilometers. There are some omissions in this list - not all countries in our `df` dataset are present in this `countrysize` object. But for our present purposes this is ok. We just want to get that square kilometers data into a combined dataset and it is sufficient for that.

Essentially what we want to do is a classic "left-join" operation of the sort in the diagram below. Conceptually, the `df` dataset is like the purple table and the `countrysize` dataset is like the red one.

<center>
    <img src="images/exploring-data-structures/join_types.png" 
    width=700 style="margin:auto"/>
    <p style="text-align: center">
       Types of Joins
    </p>
    <br/>
</center>

The `.merge()` method (https://pandas.pydata.org/docs/reference/api/pandas.merge.html) works similarly to how table joining works in `SQL` or how the `VLOOKUP` function works in Excel. One needs to specify both dataframes, the key variables on which to join them, and the kind of join desired. 

So let's look at the example below to see how it all comes together in code. 

```{python}
combined_df = df.merge(countrysize, left_on='country', right_on='nation', how='left')
combined_df.head(2)
```


When you run the code above you will notice that both the `nation` key column and the `area_square_kms` column have been joined together in the new `combined_df` object. One can keep that nation column in there for control purposes, or it can be removed by using the `.drop()` method we used earlier:

combined_df = combined_df.drop('nation', axis=1) 

Another innovative way to put “merge” data together in pandas is with the `.concat()` function. Conceptually you can think if it like “stacking” two data objects on top of each other or side-by-side as shown in the diagram below.


<center>
    <img src="images/exploring-data-structures/concat.png" 
    width=700 style="margin:auto"/>
    <p style="text-align: center">
       Ways to stack data
    </p>
    <br/>
</center>



To illustrate, let’s fetch two simple dataframes. Each contains the average scores for three subjects by year for two separate schools.

```{python}
school1_url  = "https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/school1.csv"
school2_url  = "https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/school2.csv"
school1_df = pd.read_csv(school1_url)
school2_df = pd.read_csv(school2_url)
```

Let's take a quick look at the two dataframes.

```{python}
school1_df
```

```{python}
school2_df
```

We call `.concat()` and pass in the objects that we want to stack vertically. This is a similiar operation to `union` in `SQL`.

```{python}
vertical_stack_df = pd.concat([school1_df, school2_df])
vertical_stack_df
```

It is also possible to stack the data horizontally. Here it is necessary to specify the columnar axis (axis=1) as the default setting is for rows (axis=0).

```{python}
horizontal_stack = pd.concat([school1_df, school2_df], axis=1)
horizontal_stack
```

This introduction only scratches the surface of how to leverage this way of joining datasets together. But it can be a powerful tool in the toolkit for the right use case. More detail found here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html

## Grouping and summarizing

Sometimes of course you would prefer to group rows together for the purpose of summarizing them in various ways.

We can accomplish this using the `.groupby()` method. A `.groupby()` operation involves some combination of splitting the object, applying a function, and combining the results.  It is used together with one or more aggregation functions:

* `count()`: Total number of items
* `first()`, `last()`: First and last item
* `mean()`, `median()`: Mean and median
* `min()`, `max()`: Minimum and maximum
* `std()`, `var()`: Standard deviation and variance
* `mad()`: Mean absolute deviation
* `prod()`: Product of all items
* `sum()`: Sum of all items

With our data, let's use these tools to get population mean, min and max by continent. Here's how it works. 

First we place "continent" in the `.groupby()` function, then we pass the aggregation functions "mean", "min", and "max" as a dictionary within the `.agg()` function. This dictionary takes the column that we are aggregating - in this case life expectancy - as a key and the aggregation functions as its value.

```{python}
grouped_single = df.groupby('continent').agg({'lifeExp': ['mean', 'min', 'max']})
grouped_single.columns = ['lifeExp_mean', 'lifeExp_min', 'lifeExp_max']
grouped_single.head(10)
```

We can nest additional "groups within groups" by creating a list of column names and passing that to the `.groupby()` function instead of passing a single string value.  The example below adds more granularity with the introduction of 'country' and the creation of a list to hold both 'continent' and 'country'.  

```{python}
grouped_multiple = df.groupby(['continent', 'country']).agg({'pop': ['mean', 'min', 'max']})
grouped_multiple.columns = ['pop_mean', 'pop_min', 'pop_max']
grouped_multiple.head(10)
```

:::{.callout-warning icon="false"}

## Challenge 4
You would like to summarize life expectancy by year for each continent so you can get an idea of trends from 1997 to 2007. Pick some aggregations that would make sense to look at for this task.

:::
:::{.callout-note icon="false" collapse="true"}

### Solution to Challenge 4

Using what we learned about how to select rows, we should limit the dataframe to rows where the year is greater than or equal to 1997. Next we should create a multicolumn call to the `.groupby()` function. Finally we should select some aggregations such as mean, min, and max among others could make sense here.

```{python}
df_current = df[df['year'] >= 1997]
cont_year = df_current.groupby(['continent', 'year']).agg({'lifeExp': ['mean', 'median', 'min', 'max']})
cont_year.columns = ['lifeExp_mean', 'lifeExp_median', 'lifeExp_min', 'lifeExp_max']
cont_year.head(20)
```
:::



By now you should be able to select the rows and columns, join together dataframes, and create some basic summarizations of data. The next section will look at some of the more sophisticated and elegant tools for understanding and presenting your data. But the building blocks you have just learned about are the meat and potatoes of data analysis in the python world.

