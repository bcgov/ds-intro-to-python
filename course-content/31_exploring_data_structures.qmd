---
title: Exploring Data Structures
teaching: 45
author: Stuart Hemerling
exercises: 4
objectives:
  - to be able to understand and use some basic tools to manipulate dataframes
  - to be able to understand and use tools to create a subset of data based on logical filtering
  - to reinforce use of basic summarizing methods with the data
keypoints: null
jupyter: python3
---

# Exploring and Understanding Data

Getting a high level summary of the data is important but data is particularly valuable when refined. Your analysis will start to come alive when we start to do some slicing and dicing and grouping of data or even creating additional variables. In an Excel world, this is like when you use filter options for columns, or create pivot tables, or when you create a formula in a new column to create a new variable. In data science parlance, this is the kind of thing that is referred to as `data wrangling` - getting the (already cleaned) data you want in the form you want it in. 

In the world of python, this usually means working with a library or package you have already been introduced to called pandas. It lets you do so much!

The first dataset we'll look at is one that looks at data for countries around the world and shows population levels as well as gdp per capita over a number of years. Let's import the pandas library and then use the `.read_csv()` function to get some population by country data.  We will read the data into an object that will call `country_metrics`, so that it is easier for us to remember what it consists of.

```{python}
import pandas as pd
url = "https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/gapfinder.csv"
country_metrics = pd.read_csv(url)
```

It's usually a good idea right away to take a quick look at the data to make sure what we have read in makes sense. The `.info()` method prints the number of columns, column labels, column data types, memory usage, range index, and the number of cells in each column (non-null values).


:::{.callout-tip}
## Reminder: Naming variables!
When you create a variable or object, you can name it pretty much whatever you want. You will see widespread use of the name "df" on the internet. However, as it is good practice to name your data something that relates to the contents. Objects you create should be some noun that is at the same time intuitive but not overly verbose. Also remember to not leave spaces in your names, and to always be consistent in the naming conventions you use (e.g. camelCase, underscore_case, etc.). 
:::

```{python}
country_metrics.info()
```

So this data is looking good so far. We can see that we have created a pandas dataframe within our python environment. There are 1704 rows of data and six columns or variables. You can see there are only non-null values... so happily there is no missing data to worry about.

The `.head()` pandas method lets us look at actual data as one does in Excel.

```{python}
country_metrics.head(2)
```

This dataset has some high level country metrics about population and the economy going back to 1952. 
<br>

## Selecting columns (variables)

In the "real world" of data it is not uncommon to see hundreds or even thousands of columns in a single dataframe, so knowing how to pare down our dataset so it is not overly bloated is important. 

<center>
    <img src="images/exploring-data-structures/select_columns.png" 
    width=700 style="margin:auto"/>
    <p style="text-align: center">
       Selecting Columns
    </p>
    <br/>
</center>

First things first. If we want to look the contents of a single column, we could do it like this, specifying the column name after the `.` without parentheses:

```{python}
country_metrics.country
```


When we want to select more than one column to include in our dataframe, we use the convention df[['col1', 'col2']] to select the columns we want.

In our example below, we will create a new dataframe called "narrow_country_metrics".We primarily interested in getting the population metric from the original dataset, and since we want to be able to analyze it by year, country, and continent, we will add those into our new dataframe also. So the dataset will be more narrow than the one it was created from. 

There are two ways to approach this. The first way to approach is to identify all the columns you want to keep. 

```{python}
narrow_country_metrics = country_metrics[['country', 'year', 'pop', 'continent']]
narrow_country_metrics.head(2)
```

The second way is use the pandas `.drop()` method to drop the columns from the original object that you do not want to keep. In the end we can achieve the same result as the example above by simply dropping two variables instead of naming four. With this method, we also need to add the specification `axis = 1`, which indicates that it is columns (and not rows) being referenced for being dropped. 
 
```{python}
narrow_country_metrics = country_metrics.drop(['lifeExp', 'gdpPercap'], axis=1)
narrow_country_metrics.head(2)
```

Either way we have a more manageable dataset to work with called "narrow_country_metrics". The original dataframe "country_metrics" is still there, and it still the same shape as before, we have not changed it.

However, it is important to realize that in python it is very common to change an object *by referring to that object on both sides of the assignment operator*. In the code below, we narrow the "narrow_country_metrics" dataset down even further by removing the column `pop` as well. 

```{python}
narrow_country_metrics = narrow_country_metrics.drop(['pop'], axis=1)
narrow_country_metrics.head(2)
```

If we want to have the `pop` variable back in the narrow_country_metrics object, we will need to recreate it from the source it came from, where `pop` was still intact.

:::{.callout-tip}
## Reminder on Assignment!
When you create a variable or dataframe object in python, to the left of the `=` sign you always put the name of the thing you want to make or modify. To the right, that's where you put the contents of what it is you want to create. It may feel counterintuitive! Especially if you are using the same variable or dataframe identifier on both sides of the equals sign.
:::
:::{.callout-warning icon="false"}

### Challenge 1
Let's say you just want to narrow down the dataset to include just the country and the GDP per capita. How would you do it? Don't forget to run your code so it also shows a view of the result so you can confirm the code worked as you wanted it to. 

:::
:::{.callout-note icon="false" collapse="true"}

### Solution to Challenge 1

First you would like to create a new object to hold this narrowed down version of the dataset, then use the `.head()` to see some of the resulting data. 

```{python}
challenge1_df = country_metrics[['country', 'gdpPercap']]
challenge1_df.head(2)
```
:::

## Selecting rows

Of course, in data analysis we are usually interested in looking at just some rows of data, not all the rows all the time. 

<center>
    <img src="images/exploring-data-structures/select_rows.png" 
    width=700 style="margin:auto"/>
    <p style="text-align: center">
       Selecting Rows
    </p>
    <br/>
</center>

So when we want to look at just selected rows (i.e. select rows that have certain values within a given column) we can supply a condition that must be met in that column (for example equal to, less than, etc) and we will get back just the rows that match that condition:

 - equal to `==`
 - not equal to `!=`
 - less than `<`
 - greater than `> `
 - less than or equal to `<=`
 - greater than or equal to `>=`

Note that we do not use the `=` operator when it comes to specifying values we want to find in a column. This is an important point in python. The `==` operator compares the value or equality of two objects and is the right operator to use. It is a `comparison` operator. In contrast, the `=` is the `assignment` operator. We use it, for example, when we create or assign variables or objects. In the example below we see both at work.

The most common way in python to select rows is to use a condition inside selection brackets `[]`

```{python}
filtered_country_metrics = country_metrics[country_metrics['year'] == 1972]
filtered_country_metrics.head(2)
```

Let's look at what is happening under the hood.  If we look at the code inside the square brackets, we see that pandas checks each row to see which rows fulfill the condition specified.  

```{python}
country_metrics['year'] == 1972
```

Those rows where this check returns `True` are then selected from the data frame.

We can also expand this concept by adding additional conditions that narrow or expand the number of rows filtered. Below we use the `&` condition to indicate that both parts of the equation need to be fulfilled. 

When combining multiple conditional statements as in the example below, each condition must be surrounded by parentheses `()` within the square brackets `[]`. You must use the "or" operator `|` and/or the "and" operator `&` for the code to work. 

```{python}
filtered_country_metrics = country_metrics[(df['year'] == 1972) & (country_metrics['country'] == 'Albania')]
filtered_country_metrics.head()
```

In the example above, there is only one row of data that matches the condition.

Or we can use the `|` which indicates that if *either* of the statements are true, then all the rows satisfying either condition are returned.

```{python}
filtered_country_metrics = country_metrics[(country_metrics['year'] == 1972) | (country_metrics['country'] == 'Albania')]
filtered_country_metrics.head()
```

In the example above, there are many rows that satisfy the condition.

:::{.callout-warning icon="false"}

## Challenge 2
Your director has come to you and asked if you know what the life expectancy has been in Canada since 1992.  How would you use pandas code to get the data you need? And after having run the code, what's the answer? 
:::
:::{.callout-note icon="false" collapse="true"}

### Solution to Challenge 2

First you would like to create a new object to hold this narrowed down version of the dataset, then use some method like `.print()` or `.head()` to see some of the resulting data. 


```{python}
challenge2_df = country_metrics[(country_metrics['year'] >= 1992) & (country_metrics['country'] == 'Canada')]
print(challenge2_df)
```
:::

You will find that it is very common within python/pandas to see the `.iloc()` function used, so it is worthwhile to give that a brief introduction also.

This function enables the selection of data from a particular row or column, according to the integer based index as shown in the image below:

<center>
    <img src="images/exploring-data-structures/pandasdf.png" 
    width=300 style="margin:auto"/>
    <p style="text-align: center">
      Row and column values iloc() refers to
    </p>
    <br/>
</center>

By referring to these locations, we can use `iloc()` to retrieve the exact cells we want to see. For example, if we just want the row of data corresponding to the index integer value 5, we would run the following code:

```{python}
country_metrics.iloc[5]
```

We can also modify the above call to look at column(s)! For example, we specify the row(s) of interest to the left of a `comma` in the square brackets, and the column(s) of interest to the right of it.  In the example below, the code will retrieve just the content found at the intersection of row 5 and column 3:

```{python}
country_metrics.iloc[5,3]
```

We can also retrieve *ranges* of rows and columns respectively by employing a `colon` along with the starting and ending rows or columns we want. In the example below, we want to get rows "0" through "2" and  the columns "0" through "3": 

```{python}
country_metrics.iloc[0:2,0:3]
```

It is a flexible tool that can be very helpful as you build and review your code. 
<br>

## Sorting rows

So now you've mastered how to select rows and columns you want, congratulations! Instead of hunting and pecking for insights, one way to quickly make some sense of the data is to sort it - something you probably do in Excel all the time. 

In the challenge above we created a dataframe "challenge2_df" with just the one country in it and from 1992. So now let's sort the data by year, and then see if we can spot any insights. To do this, we use the `.sort_values()` method.

Let's go fetch and look at the dataframe, and use this method to sort by year, going from more recent to less recent:

```{python}
challenge2_df.sort_values('year', ascending=False)
```

That was pretty straightforward and it helps us get more insights. 

<br>

## Putting multiple methods together 

The next step is usually to bring several of these commands together to get a nicely refined look at the data. Essentially you will need to invoke a number of calls sequentially to some object, with each one in turn performing some action on it. 

A common approach is to put the object name on each sucessive line of code along with the `=` operator as well as the modifying code to the right.  Imagine we were give the following task:

1. Select just the rows where the country is equal to Canada AND where the year is 1997 or greater
2. Take these rows and sort them by year, going from high to low values, top to bottom

The example below shows how we would employ the line-by-line approach:

```{python}
chained2_df = country_metrics[(country_metrics['country'] == 'Canada')]
chained2_df = chained2_df[(chained2_df['year'] >= 1997)]
chained2_df = chained2_df.sort_values('year', ascending=False)
chained2_df.head(3)
```

You can read fairly clearly what is happening in each line. 

You can also put multiple methods in the same line of code, as long as you separate each of the elements. In pandas, this is sometimes referred to as joining or chaining, as you are essentially creating a joining/chaining actions together.

The code below accomplishes what the more verbose code above does but in a more efficient way:

```{python}
#| scrolled: true
chained_df = country_metrics[(country_metrics['country'] == 'Canada') &     (country_metrics['year'] >= 1997)].sort_values('year', ascending = False)
chained_df.head(3)
```

Each line is executed in sequence, so be careful when constructing code that the order in which you modify your dataframe is what you intend. Take the example below, in which we want to find European values for GDP per capita. In it, we first have a line of code that selects the rows that contain "Europe" in the first line, then looks at the column values for "year", "country", and "gdpPercap".

```{python}
year_country_gdp = country_metrics[(country_metrics['continent'] == 'Europe')]
year_country_gdp = year_country_gdp[['year', 'country', 'gdpPercap']]
year_country_gdp.head(3)
```

Had we reversed the lines of code, and selected the "year", "country", and "gdpPercap" columns first to put into our new object, we would not have been able to sort for "Europe" in the "continent" column, as the "continent" would have been essentially removed from the dataframe in the step above, so python would have thrown an error. 

:::{.callout-warning icon="false"}

## Challenge 3
Your director has come back to you and wondered about whether the life expectancy of people changed during the 1970s in Cambodia. Use what you know about selecting rows and sorting data to get the data you need to answer the question. 
:::
:::{.callout-note icon="false" collapse="true"}

### Solution to Challenge 3

Using code to select the country Cambodia, then call `sort_values()` method and chain them together. Looking at the data we see that in the 1970s there was a sharp decline in life expectancy in Cambodia. We also see, thankfully, that it has recovered strongly since then. 

```{python}
challenge3_df = country_metrics[(country_metrics['country'] == 'Cambodia')].sort_values('year', ascending = False)
print(challenge3_df)
```

:::

## Creating new columns of data

Very often we have some data in our dataset that we want to transform to give us additional information. In Excel this is something that is done all the time by creating a formula in a cell that refers to other columns and applies some sort of logic or mathematical expression to it. 

There are different ways you can go about this in pandas. The most straightforward way is to define a new column on the left of the `=` and then reference the existing column and whatever additional conditions you would like on the right. In the example below, the existing population variable “pop” is converted to a value that shows population in millions.

```{python}
new_cols_df = country_metrics
new_cols_df['pop_millions'] = new_cols_df['pop']/1000000
new_cols_df.head(2)
```

There is also a special python function called `lambda`. It is known as an anonymous function, because it is not given a name other than lambda.  It can take any number of arguments in an expression.

The `.assign()` method looks at this expression with lambda in it and returns the value it is asked to do. This is how it comes all together to give us actual GDP for each row in our dataframe:

```{python}
GDP_df = country_metrics.assign(GDP=lambda x: x['pop'] * x['gdpPercap'])
GDP_df.head(5)
```

So now we can see what the GDP was for each country in our new object. Notice that the GDP data is in scientific notation (i.e. the decimal number times x number of zeros), so it's a bit hard to read. If we wanted readers to consume that data we would go ahead and change the data type for it. But for current purposes we'll leave that alone.

<br>

## Joining datasets together

One of the most important tasks in data analysis is to be able to join multiple datasets together. With pandas, there are functions called `.merge()` and `.join()` that are similar to each other. As `.merge()` is perhaps the more used, intuitive, and powerful of the two, we will introduce that method in this tutorial in some depth. There is also a cool function called `.concat()` that will be introduced below as well.

But first, let’s get a second file that gives us country size in square kilometers by country. We will use this data to put together with our df dataframe.

```{python}
country_size_url = "https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/countrysize.csv"
country_size = pd.read_csv(country_size_url, encoding= 'unicode_escape')
country_size.info()
country_size.sample(2)
```

Ok, this `country_size` dataset looks like we expect - a list of countries alongside the land size of that country in square kilometers. There are some omissions in this list - not all countries in our `country_metrics` dataset are present in this `country_size` object. But for our present purposes this is ok. We just want to get that square kilometers data into a combined dataset and it is sufficient for that.

Essentially what we want to do is a classic "left-join" operation of the sort in the diagram below. Conceptually, the `country_metrics` dataset is like the purple table and the `country_size` dataset is like the red one.

<center>
    <img src="images/exploring-data-structures/join_types.png" 
    width=700 style="margin:auto"/>
    <p style="text-align: center">
       Types of Joins
    </p>
    <br/>
</center>

The `.merge()` method [(with reference material available here)](https://pandas.pydata.org/docs/reference/api/pandas.merge.html) works similarly to how table joining works in `SQL` or how the `VLOOKUP` function works in Excel. One needs to specify both dataframes, the key variables on which to join them, and the kind of join desired. 

So let's look at the example below to see how it all comes together in code. 

```{python}
combined_df = country_metrics.merge(country_size, left_on='country', right_on='nation', how='left')
combined_df.head(2)
```


When you run the code above you will notice that both the `nation` key column and the `area_square_kms` column have been joined together in the new `combined_df` object. One can keep that nation column in there for control purposes, or it can be removed by using the `.drop()` method we used earlier:

combined_df = combined_df.drop('nation', axis=1) 

Another innovative way to put “merge” data together in pandas is with the `.concat()` function. Conceptually you can think if it like “stacking” two data objects on top of each other or side-by-side as shown in the diagram below.


<center>
    <img src="images/exploring-data-structures/concat.png" 
    width=700 style="margin:auto"/>
    <p style="text-align: center">
       Ways to stack data
    </p>
    <br/>
</center>



To illustrate, let’s fetch two simple dataframes. Each contains the average scores for three subjects by year for two separate schools.

```{python}
school1_url  = "https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/school1.csv"
school2_url  = "https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/school2.csv"
school1_df = pd.read_csv(school1_url)
school2_df = pd.read_csv(school2_url)
```

Let's take a quick look at the two dataframes.

```{python}
school1_df
```

```{python}
school2_df
```

We call `.concat()` and pass in the objects that we want to stack vertically. This is a similiar operation to `union` in `SQL`.

```{python}
vertical_stack_df = pd.concat([school1_df, school2_df])
vertical_stack_df
```

It is also possible to stack the data horizontally. Here it is necessary to specify the columnar axis (axis=1) as the default setting is for rows (axis=0).

```{python}
horizontal_stack = pd.concat([school1_df, school2_df], axis=1)
horizontal_stack
```

This introduction only scratches the surface of how to leverage this way of joining datasets together. But it can be a powerful tool in the toolkit for the right use case. [More detail found here.](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html)

<br>

## Grouping and summarizing

Sometimes of course you would prefer to group rows together for the purpose of summarizing them in various ways.

We can accomplish this using the `.groupby()` method. A `.groupby()` operation involves some combination of splitting the object, applying a function, and combining the results.  It is used together with one or more aggregation functions:

* `count()`: Total number of items
* `first()`, `last()`: First and last item
* `mean()`, `median()`: Mean and median
* `min()`, `max()`: Minimum and maximum
* `std()`, `var()`: Standard deviation and variance
* `mad()`: Mean absolute deviation
* `prod()`: Product of all items
* `sum()`: Sum of all items

First let us look at a simple example where we want to get the mean life expectancy for each continent in the data. To do this, we would use the `groupby()` function to call the appropriate segment (i.e. continent), metric (i.e. lifeExp), and type of aggregation (i.e. mean):

```{python}
simple_mean = country_metrics.groupby('continent').lifeExp.mean()
simple_mean.head(10)
```

Fortunately, if we want to look at more aggregations at once, we can do that too. To extend our example above, we would put "continent" in the `.groupby()` function, then pass additional aggregation functions (in our case, we will add  "mean", "min", and "max") as a dictionary within the `.agg()` function. This dictionary takes the column that we are aggregating - in this case life expectancy - as a key and the aggregation functions as its value.

```{python}
grouped_single = country_metrics.groupby('continent').agg({'lifeExp': ['mean', 'min', 'max']})
grouped_single.columns = ['lifeExp_mean', 'lifeExp_min', 'lifeExp_max']
grouped_single.head(5)
```

We can nest additional "groups within groups" by creating a list of column names and passing that to the `.groupby()` function instead of passing a single string value.  The example below adds more granularity with the introduction of 'country' and the creation of a list to hold both 'continent' and 'country'.  

```{python}
grouped_multiple = country_metrics.groupby(['continent', 'country']).agg({'pop': ['mean', 'min', 'max']})
grouped_multiple.columns = ['pop_mean', 'pop_min', 'pop_max']
#grouped_multiple = grouped_multiple.reset_index()
grouped_multiple.head(5)
```

Notice that in the codeblock above there is a commented out line of code that calls a function called `reset_index()`. This is a step that resets the index to an integer based index. If we uncomment the comment and rerun the code, we can see the index numbering. 


:::{.callout-warning icon="false"}

## Challenge 4
You would like to summarize life expectancy by year for each continent so you can get an idea of trends from 1997 to 2007. Pick some aggregations that would make sense to look at for this task.

:::
:::{.callout-note icon="false" collapse="true"}

### Solution to Challenge 4

Using what we learned about how to select rows, we should limit the dataframe to rows where the year is greater than or equal to 1997. Next we should create a multicolumn call to the `.groupby()` function. Finally we should select some aggregations such as mean, min, and max among others could make sense here.

```{python}
df_current = country_metrics[country_metrics['year'] >= 1997]
cont_year = df_current.groupby(['continent', 'year']).agg({'lifeExp': ['mean', 'median', 'min', 'max']})
cont_year.columns = ['lifeExp_mean', 'lifeExp_median', 'lifeExp_min', 'lifeExp_max']
cont_year.head(20)
```
:::

<br>

By now you should be able to select the rows and columns, join together dataframes, and create some basic summarizations of data. The next section will look at some of the more sophisticated and elegant tools for understanding and presenting your data. But the building blocks you have just learned about are the meat and potatoes of data analysis in the python world.

