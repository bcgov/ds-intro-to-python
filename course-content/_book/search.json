[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science using Python",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\n\nThe Messy Side of Data\n\n\n\n\n\nThe Messy Side of Data"
  },
  {
    "objectID": "31_exploring_data_structures.html",
    "href": "31_exploring_data_structures.html",
    "title": "2  Exploring Data Structures",
    "section": "",
    "text": "Getting a high level summary of the data is important but data is particularly valuable when refined. Your analysis will start to come alive when we start to do some slicing and dicing and grouping of data or even creating additional variables. In an Excel world, this is like when you use filter options for columns, or create pivot tables, or when you create a formula in a new column to create a new variable. In data science parlance, this is the kind of thing that is referred to as data wrangling - getting the (already cleaned) data you want in the form you want it in.\nIn the world of python, this usually means working with a library or package you have already been introduced to called pandas. It let’s you do so much!\nThe first dataset we’ll look at is one that looks at data for countries around the world and shows population levels as well as gdp per capita over a number of years. It is a great data set to look at. Let’s import the pandas library and then use the .read_csv() function to get some population by country data.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/gapfinder.csv\"\ndf = pd.read_csv(url)\n\nIt’s usually a good idea right away to take a quick look at the data to make sure what we have read in makes sense. The .info() method prints the number of columns, column labels, column data types, memory usage, range index, and the number of cells in each column (non-null values).\n\n\n\n\n\n\nReminder: Naming variables!\n\n\n\nWhen you create a variable or object, you can name it pretty much whatever you want. You will see widespread use of the name “df”. Apart from “df”, it is considered good practice to name dataframes something that is at the same time intuitive but not overly verbose. Also remember to not leave spaces in your names, and to always be consistent in the naming conventions you use (e.g. camelCase, underscore_case, etc.)\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1704 entries, 0 to 1703\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   country    1704 non-null   object \n 1   year       1704 non-null   int64  \n 2   pop        1704 non-null   float64\n 3   continent  1704 non-null   object \n 4   lifeExp    1704 non-null   float64\n 5   gdpPercap  1704 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 80.0+ KB\n\n\nSo this data is looking good so far. We can see that we have created a pandas dataframe within our python environment. There are 1704 rows of data and six columns or variables. You can see there are only non-null values… so happily there is no missing data to worry about.\nThe .head() pandas method let’s us look at actual data as one does in Excel.\n\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n  \n\n\n\n\nThis dataset has some high level country metrics about population and the economy going back to 1952.\n\n\nIn the “real world” of data it is not uncommon to see hundreds or even thousands of columns in a single dataframe, so knowing how to pare down our dataset so it is not overly bloated is important.\n\n\n\nSelecting Columns\n\n\n\nFirst things first. If we want to look the contents of a single column, we could do it like this, specifying the column name after the . without parentheses:\n\ndf.country\n\n0       Afghanistan\n1       Afghanistan\n2       Afghanistan\n3       Afghanistan\n4       Afghanistan\n           ...     \n1699       Zimbabwe\n1700       Zimbabwe\n1701       Zimbabwe\n1702       Zimbabwe\n1703       Zimbabwe\nName: country, Length: 1704, dtype: object\n\n\nWhen we want to select more than one column to include in our dataframe, we use the convention df[[‘col1’, ‘col2’]] to select the columns we want.\nIn our example below, we will create a new dataframe called “narrow_df”, and we are primarily interested in getting the population column from the original df, but we want to have year, country, and continent in our new dataframe also.\n\nnarrow_df = df[['country', 'year', 'pop', 'continent']]\nnarrow_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n    \n  \n\n\n\n\nSo now we have a more manageable dataset to work with called “narrow_df”. The original dataframe “df” is still there, and still the same shape as before, we have not changed it.\nSometimes you might have many columns in a dataframe you want to keep and only a relatively few you might want to remove. In these instances, you might want to use the pandas .drop() method instead. We can achieve the same result as the example above by simply dropping two variables instead of naming the other four. With this method, we also need to add the specification axis = 1, which indicates that it is columns being referenced for being dropped.\n\nnarrow_df = df.drop(['lifeExp', 'gdpPercap'], axis=1)\nnarrow_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n    \n  \n\n\n\n\n\n\n\n\n\n\nReminder on Assignment!\n\n\n\nWhen you create a variable or dataframe object in python, to the left of the = sign you always put the name of the thing you want to make or modify. To the right, that’s where you put the contents of what it is you want to create. It may feel counterintuitive! Especially if you are using the same variable or dataframe identifier on both sides of the equals sign.\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nLet’s say you just want to narrow down the dataset to include just the country and the GDP per capita. How would you do it? Don’t forget to run your code so it also shows a view of the result so you can confirm the code worked as you wanted it to.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nFirst you would like to create a new object to hold this narrowed down version of the dataset, then just use the .head() to see some of the resulting data.\n\nchallenge1_df = df[['country', 'gdpPercap']]\nchallenge1_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      779.445314\n    \n    \n      1\n      Afghanistan\n      820.853030\n    \n  \n\n\n\n\n\n\n\n\n\n\nOf course, in data analysis we are usually interested in looking at just some of rows of data, not all of it all the time.\n\n\n\nSelecting Rows\n\n\n\nSo when we want to look at just selected rows (i.e. values within a given column) we can specify the column(s) we want to select from, then we use == or a different operator as we require:\n\nnot equal to !=\nless than <\ngreater than >\nless than or equal to <=\ngreater than or equal to >=\n\nNote that we do not use the = operator when it comes to specifying values we want to find in a column. This is an important point in python. The == operator compares the value or equality of two objects and is the right operator to use. It is a comparison operator. In comparison, the = is an assignment operator. We use it, for example, when we create or assign variables or objects. In the example below we see both at work.\nLikely the most common way in python to select rows is to use a condition inside selection brackets []\n\nfiltered_df = df[df['year'] == 1972]\nfiltered_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n    \n      16\n      Albania\n      1972\n      2263554.0\n      Europe\n      67.690\n      3313.422188\n    \n  \n\n\n\n\nLet’s look at what is happening under the hood. We see that pandas checks each row to see which rows fulfill the condition specified.\n\ndf['year'] == 1972\n\n0       False\n1       False\n2       False\n3       False\n4        True\n        ...  \n1699    False\n1700    False\n1701    False\n1702    False\n1703    False\nName: year, Length: 1704, dtype: bool\n\n\nWe can also expand this concept by adding additional conditions that narrow or expand the number of rows filtered. Below we use the & condition to indicate that both parts of the equation need to be fulfilled.\nWhen combining multiple conditional statements as in the example below, each condition must be surrounded by parentheses () within the square brackets []. You must use the “or” operator | and/or the “and” operator & for the code to work.\n\nfiltered_df = df[(df['year'] == 1972) & (df['country'] == 'Albania')]\nfiltered_df.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      16\n      Albania\n      1972\n      2263554.0\n      Europe\n      67.69\n      3313.422188\n    \n  \n\n\n\n\nIn the example above, there is only one row of data that matches the condition.\nOr we can use the | which indicates that if either of the statements are true, then all the rows satisfying either condition are returned.\n\nfiltered_df = df[(df['year'] == 1972) | (df['country'] == 'Albania')]\nfiltered_df.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n    \n      12\n      Albania\n      1952\n      1282697.0\n      Europe\n      55.230\n      1601.056136\n    \n    \n      13\n      Albania\n      1957\n      1476505.0\n      Europe\n      59.280\n      1942.284244\n    \n    \n      14\n      Albania\n      1962\n      1728137.0\n      Europe\n      64.820\n      2312.888958\n    \n    \n      15\n      Albania\n      1967\n      1984060.0\n      Europe\n      66.220\n      2760.196931\n    \n  \n\n\n\n\nIn the example above, there are many rows that satisfy the condition.\n\n\n\n\n\n\nChallenge 2\n\n\n\nYour director has come to you and asked if you know what the life expectancy has been in Canada since 1992. How would you use pandas code to get the data you need? And after having run the code, what’s the answer?\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\nFirst you would like to create a new object to hold this narrowed down version of the dataset, then use some method like .print() or .head() to see some of the resulting data.\n\nchallenge2_df = df[(df['year'] >= 1992) & (df['country'] == 'Canada')]\nprint(challenge2_df)\n\n    country  year         pop continent  lifeExp    gdpPercap\n248  Canada  1992  28523502.0  Americas   77.950  26342.88426\n249  Canada  1997  30305843.0  Americas   78.610  28954.92589\n250  Canada  2002  31902268.0  Americas   79.770  33328.96507\n251  Canada  2007  33390141.0  Americas   80.653  36319.23501\n\n\n\n\n\nYou will find that it is very common within python/pandas to see the .iloc() method.\nThe .iloc() method identifies rows based on their index value (that column to the left of the dataframe when you call the .head() function, for example). It can be used to look at individual rows of data by their original index value.\n\ndf.iloc[251]\n\ncountry           Canada\nyear                2007\npop           33390141.0\ncontinent       Americas\nlifeExp           80.653\ngdpPercap    36319.23501\nName: 251, dtype: object\n\n\nOr it can be used to call multiple rows in a range:\n\ndf.iloc[0:5]\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n  \n\n\n\n\n\n\n\nSo now you’ve mastered how to select rows and columns you want, congratulations! Instead of hunting and pecking for insights, one way to quickly make some sense of the data is to sort it - something you probably do in Excel all the time.\nIn the challenge above we created a dataframe “challenge2_df” with just the one country in it and from 1997. So now let’s sort the data by year, and then see what can spot any insights. To do this, we use the .sort_values() method.\nLet’s go fetch and look at the dataframe, and use this method to sort by year, going from more recent to less recent:\n\nchallenge2_df.sort_values('year', ascending=False)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      251\n      Canada\n      2007\n      33390141.0\n      Americas\n      80.653\n      36319.23501\n    \n    \n      250\n      Canada\n      2002\n      31902268.0\n      Americas\n      79.770\n      33328.96507\n    \n    \n      249\n      Canada\n      1997\n      30305843.0\n      Americas\n      78.610\n      28954.92589\n    \n    \n      248\n      Canada\n      1992\n      28523502.0\n      Americas\n      77.950\n      26342.88426\n    \n  \n\n\n\n\n\n\n\nYou can also put multiple methods in the same line of code, as long as you separate each of the elements. In pandas, this is called “method chaining”, as you are essentially of creating a chain of actions to take place.\nIn the example below, the chain consists of two links:\n\nSelect just the rows where the country is equal to Canada AND where the year is 1997 or greater\nTake these rows and sort them by year, going from high to low values, top to bottom\n\n\nchained_df = df[(df['country'] == 'Canada') & (df['year'] >= 1997)].sort_values('year', ascending = False)\nchained_df.head(3)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      251\n      Canada\n      2007\n      33390141.0\n      Americas\n      80.653\n      36319.23501\n    \n    \n      250\n      Canada\n      2002\n      31902268.0\n      Americas\n      79.770\n      33328.96507\n    \n    \n      249\n      Canada\n      1997\n      30305843.0\n      Americas\n      78.610\n      28954.92589\n    \n  \n\n\n\n\nAnother way of chaining is to put each of the functions on multiple lines. In the example below, each line of code modifies the dataframe to acheive the same result as above.\n\nchained2_df = df[(df['country'] == 'Canada')]\nchained2_df = chained2_df[(chained2_df['year'] >= 1997)]\nchained2_df = chained2_df.sort_values('year', ascending=False)\nchained2_df.head(3)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      251\n      Canada\n      2007\n      33390141.0\n      Americas\n      80.653\n      36319.23501\n    \n    \n      250\n      Canada\n      2002\n      31902268.0\n      Americas\n      79.770\n      33328.96507\n    \n    \n      249\n      Canada\n      1997\n      30305843.0\n      Americas\n      78.610\n      28954.92589\n    \n  \n\n\n\n\nThis latter approach is very common. Each respective line to the right of the = contains the code that modifies the object assigned to the left of it. Note that you can use a combination of chaining and piping.\nEach line is executed in sequence, so be careful when constructing code that the order in which you modify your dataframe is right. Take the example below, in which we want to find European values for GDP per capita. In it, we first have a line of code that selects the rows that contain “Europe” in the first line, then looks at the column values for “year”, “country”, and “gdpPercap”.\n\nyear_country_gdp = df[(df['continent'] == 'Europe')]\nyear_country_gdp = year_country_gdp[['year', 'country', 'gdpPercap']]\nyear_country_gdp.head(3)\n\n\n\n\n\n  \n    \n      \n      year\n      country\n      gdpPercap\n    \n  \n  \n    \n      12\n      1952\n      Albania\n      1601.056136\n    \n    \n      13\n      1957\n      Albania\n      1942.284244\n    \n    \n      14\n      1962\n      Albania\n      2312.888958\n    \n  \n\n\n\n\nHad we reversed the lines of code, and selected the “year”, “country”, and “gdpPercap” columns first to put into our new object, we would not have been able to sort for “Europe” in the “continent” column, as the “continent” would have been essentially removed from the dataframe in the step above, so python would have thrown an error.\n\n\n\n\n\n\nChallenge 3\n\n\n\nYour director has come back to you and wondered about whether the life expectancy of people changed during the 1970s in Cambodia. Use what you know about selecting rows and sorting data to get the data you need to answer the question.\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nUsing code to select the country Cambodia, then call sort_values() method and chain them together. Looking at the data we see that in the 1970s there was a sharp decline in life expectancy in Cambodia. We also see, thankfully, that it has recovered strongly since then.\n\nchallenge3_df = df[(df['country'] == 'Cambodia')].sort_values('year', ascending = False)\nprint(challenge3_df)\n\n      country  year         pop continent  lifeExp    gdpPercap\n227  Cambodia  2007  14131858.0      Asia   59.723  1713.778686\n226  Cambodia  2002  12926707.0      Asia   56.752   896.226015\n225  Cambodia  1997  11782962.0      Asia   56.534   734.285170\n224  Cambodia  1992  10150094.0      Asia   55.803   682.303175\n223  Cambodia  1987   8371791.0      Asia   53.914   683.895573\n222  Cambodia  1982   7272485.0      Asia   50.957   624.475478\n221  Cambodia  1977   6978607.0      Asia   31.220   524.972183\n220  Cambodia  1972   7450606.0      Asia   40.317   421.624026\n219  Cambodia  1967   6960067.0      Asia   45.415   523.432314\n218  Cambodia  1962   6083619.0      Asia   43.415   496.913648\n217  Cambodia  1957   5322536.0      Asia   41.366   434.038336\n216  Cambodia  1952   4693836.0      Asia   39.417   368.469286\n\n\n\n\n\n\n\n\nVery often we have some data in our dataset that we want to transform to give us additional information. In Excel this is something that is done all the time by creating a formula in a cell that refers to other columns and applies some sort of logic or mathematical expression to it.\nThere are different ways you can go about this in pandas. The most straightforward way is to define a new column on the left of the = and then reference the existing column and whatever additional conditions you would like on the right. In the example below, the existing population variable “pop” is converted to a value that shows population in millions.\n\nnew_cols_df = df\nnew_cols_df['pop_millions'] = new_cols_df['pop']/1000000\nnew_cols_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n    \n  \n\n\n\n\nSo now we can see which countries had the biggest GDP in 2007. Notice that the GDP data is in scientific notation (i.e. the decimal number times x number of zeros), so it’s a bit hard to read. If we wanted readers to consume that data we would go ahead and change the data type for it. But for current purposes we’ll leave that alone.\nThere is a special python function called lambda. It is known as an anonymous function, because it is not given a name other than lambda, and can take any number of arguments in an expression.\nThe .assign() method looks at this expression with lambda in it and returns the value it is asked to do. This is how it comes all together to give us actual GDP for each row in our dataframe:\n\nnew_df = df.assign(gdp=lambda x: x['pop'] * x['gdpPercap'])\nnew_df.head(5)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n      gdp\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n      6.567086e+09\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n      7.585449e+09\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n      10.267083\n      8.758856e+09\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n      11.537966\n      9.648014e+09\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n      13.079460\n      9.678553e+09\n    \n  \n\n\n\n\nPython allows us to create our own custom functions which we can use together with lambda. We won’t create out own function here, but they we can make functions do any number of things, like double a number or convert celcius to fahrenheit, or whatever. Let’s assume that we have created a function and assigned it the name “myfunc”.\nWe could and add it inline with lambda together with the .apply() function as in this example, which applies the function to a particular column:\ndf[‘new_col’] = df[‘old_col’].apply(lambda x: myfunc(x))\nFinally, the example below would call, not just one column, but all columns (i.e. axis=1) in a dataframe to produce the new column result:\ndf[‘new_col’] = df.apply(lambda x: myfunc(x), axis=1)\n\n\n\nOne of the most important tasks in data analysis is to be able to join multiple datasets together. With pandas, there are functions called .merge() and .join() that are similar to each other. As .merge() is perhaps the more used, intuitive, and powerful of the two, we will introduce that method in this tutorial in some depth. There is also a cool function called .concat() that will be introduced below as well.\nBut first, let’s get a second file that gives us country size in square kilometers by country. We will use this data to put together with our df dataframe.\n\ncountrysize_url = \"https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/countrysize.csv\"\ncountrysize = pd.read_csv(countrysize_url, encoding= 'unicode_escape')\ncountrysize.info()\ncountrysize.sample(2)\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 235 entries, 0 to 234\nData columns (total 2 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   nation           235 non-null    object\n 1   area_square_kms  235 non-null    int64 \ndtypes: int64(1), object(1)\nmemory usage: 3.8+ KB\n\n\n\n\n\n\n  \n    \n      \n      nation\n      area_square_kms\n    \n  \n  \n    \n      41\n      Somalia\n      637657\n    \n    \n      53\n      Cameroon\n      475442\n    \n  \n\n\n\n\nOk, this countrysize dataset looks like we expect - a list of countries alongside the land size of that country in square kilometers. There are some omissions in this list - not all countries in our df dataset are present in this countrysize object. But for our present purposes this is ok. We just want to get that square kilometers data into a combined dataset and it is sufficient for that.\nEssentially what we want to do is a classic “left-join” operation of the sort in the diagram below. Conceptually, the df dataset is like the purple table and the countrysize dataset is like the red one.\n\n\n\nTypes of Joins\n\n\n\nThe .merge() method (https://pandas.pydata.org/docs/reference/api/pandas.merge.html) works similarly to how table joining works in SQL or how the VLOOKUP function works in Excel. One needs to specify both dataframes, the key variables on which to join them, and the kind of join desired.\nSo let’s look at the example below to see how it all comes together in code.\n\ncombined_df = df.merge(countrysize, left_on='country', right_on='nation', how='left')\ncombined_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n      nation\n      area_square_kms\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n      Afghanistan\n      652230.0\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n      Afghanistan\n      652230.0\n    \n  \n\n\n\n\nWhen you run the code above you will notice that both the nation key column and the area_square_kms column have been joined together in the new combined_df object. One can keep that nation column in there for control purposes, or it can be removed by using the .drop() method we used earlier:\ncombined_df = combined_df.drop(‘nation’, axis=1)\nAnother innovative way to put “merge” data together in pandas is with the .concat() function. Conceptually you can think if it like “stacking” two data objects on top of each other or side-by-side as shown in the diagram below.\n\n\n\nWays to stack data\n\n\n\nTo illustrate, let’s fetch two simple dataframes. Each contains the average scores for three subjects by year for two separate schools.\n\nschool1_url  = \"https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/school1.csv\"\nschool2_url  = \"https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/school2.csv\"\nschool1_df = pd.read_csv(school1_url)\nschool2_df = pd.read_csv(school2_url)\n\nLet’s take a quick look at the two dataframes.\n\nschool1_df\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      67\n      88\n      89\n    \n    \n      1\n      English\n      86\n      67\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      75\n    \n  \n\n\n\n\n\nschool2_df\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      32\n      88\n      90\n    \n    \n      1\n      English\n      88\n      44\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      53\n    \n  \n\n\n\n\nWe call .concat() and pass in the objects that we want to stack vertically. This is a similiar operation to union in SQL.\n\nvertical_stack_df = pd.concat([school1_df, school2_df])\nvertical_stack_df\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      67\n      88\n      89\n    \n    \n      1\n      English\n      86\n      67\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      75\n    \n    \n      0\n      Science\n      32\n      88\n      90\n    \n    \n      1\n      English\n      88\n      44\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      53\n    \n  \n\n\n\n\nIt is also possible to stack the data horizontally. Here it is necessary to specify the columnar axis (axis=1) as the default setting is for rows (axis=0).\n\nhorizontal_stack = pd.concat([school1_df, school2_df], axis=1)\nhorizontal_stack\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      67\n      88\n      89\n      Science\n      32\n      88\n      90\n    \n    \n      1\n      English\n      86\n      67\n      77\n      English\n      88\n      44\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      75\n      Math\n      84\n      73\n      53\n    \n  \n\n\n\n\nThis introduction only scratches the surface of how to leverage this way of joining datasets together. But it can be a powerful tool in the toolkit for the right use case. More detail found here: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html\n\n\n\nSometimes of course you would prefer to group rows together for the purpose of summarizing them in various ways.\nWe can accomplish this using the .groupby() method. A .groupby() operation involves some combination of splitting the object, applying a function, and combining the results. It is used together with one or more aggregation functions:\n\ncount(): Total number of items\nfirst(), last(): First and last item\nmean(), median(): Mean and median\nmin(), max(): Minimum and maximum\nstd(), var(): Standard deviation and variance\nmad(): Mean absolute deviation\nprod(): Product of all items\nsum(): Sum of all items\n\nWith our data, let’s use these tools to get population mean, min and max by continent. Here’s how it works.\nFirst we place “continent” in the .groupby() function, then we pass the aggregation functions “mean”, “min”, and “max” as a dictionary within the .agg() function. This dictionary takes the column that we are aggregating - in this case life expectancy - as a key and the aggregation functions as its value.\n\ngrouped_single = df.groupby('continent').agg({'lifeExp': ['mean', 'min', 'max']})\ngrouped_single.columns = ['lifeExp_mean', 'lifeExp_min', 'lifeExp_max']\ngrouped_single.head(10)\n\n\n\n\n\n  \n    \n      \n      lifeExp_mean\n      lifeExp_min\n      lifeExp_max\n    \n    \n      continent\n      \n      \n      \n    \n  \n  \n    \n      Africa\n      48.865330\n      23.599\n      76.442\n    \n    \n      Americas\n      64.658737\n      37.579\n      80.653\n    \n    \n      Asia\n      60.064903\n      28.801\n      82.603\n    \n    \n      Europe\n      71.903686\n      43.585\n      81.757\n    \n    \n      Oceania\n      74.326208\n      69.120\n      81.235\n    \n  \n\n\n\n\nWe can nest additional “groups within groups” by creating a list of column names and passing that to the .groupby() function instead of passing a single string value. The example below adds more granularity with the introduction of ‘country’ and the creation of a list to hold both ‘continent’ and ‘country’.\n\ngrouped_multiple = df.groupby(['continent', 'country']).agg({'pop': ['mean', 'min', 'max']})\ngrouped_multiple.columns = ['pop_mean', 'pop_min', 'pop_max']\ngrouped_multiple.head(10)\n\n\n\n\n\n  \n    \n      \n      \n      pop_mean\n      pop_min\n      pop_max\n    \n    \n      continent\n      country\n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      1.987541e+07\n      9279525.0\n      33333216.0\n    \n    \n      Angola\n      7.309390e+06\n      4232095.0\n      12420476.0\n    \n    \n      Benin\n      4.017497e+06\n      1738315.0\n      8078314.0\n    \n    \n      Botswana\n      9.711862e+05\n      442308.0\n      1639131.0\n    \n    \n      Burkina Faso\n      7.548677e+06\n      4469979.0\n      14326203.0\n    \n    \n      Burundi\n      4.651608e+06\n      2445618.0\n      8390505.0\n    \n    \n      Cameroon\n      9.816648e+06\n      5009067.0\n      17696293.0\n    \n    \n      Central African Republic\n      2.560963e+06\n      1291695.0\n      4369038.0\n    \n    \n      Chad\n      5.329256e+06\n      2682462.0\n      10238807.0\n    \n    \n      Comoros\n      3.616839e+05\n      153936.0\n      710960.0\n    \n  \n\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\nYou would like to summarize life expectancy by year for each continent so you can get an idea of trends from 1997 to 2007. Pick some aggregations that would make sense to look at for this task.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nUsing what we learned about how to select rows, we should limit the dataframe to rows where the year is greater than or equal to 1997. Next we should create a multicolumn call to the .groupby() function. Finally we should select some aggregations such as mean, min, and max among others could make sense here.\n\ndf_current = df[df['year'] >= 1997]\ncont_year = df_current.groupby(['continent', 'year']).agg({'lifeExp': ['mean', 'median', 'min', 'max']})\ncont_year.columns = ['lifeExp_mean', 'lifeExp_median', 'lifeExp_min', 'lifeExp_max']\ncont_year.head(20)\n\n\n\n\n\n  \n    \n      \n      \n      lifeExp_mean\n      lifeExp_median\n      lifeExp_min\n      lifeExp_max\n    \n    \n      continent\n      year\n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      1997\n      53.598269\n      52.7590\n      36.087\n      74.772\n    \n    \n      2002\n      53.325231\n      51.2355\n      39.193\n      75.744\n    \n    \n      2007\n      54.806038\n      52.9265\n      39.613\n      76.442\n    \n    \n      Americas\n      1997\n      71.150480\n      72.1460\n      56.671\n      78.610\n    \n    \n      2002\n      72.422040\n      72.0470\n      58.137\n      79.770\n    \n    \n      2007\n      73.608120\n      72.8990\n      60.916\n      80.653\n    \n    \n      Asia\n      1997\n      68.020515\n      70.2650\n      41.763\n      80.690\n    \n    \n      2002\n      69.233879\n      71.0280\n      42.129\n      82.000\n    \n    \n      2007\n      70.728485\n      72.3960\n      43.828\n      82.603\n    \n    \n      Europe\n      1997\n      75.505167\n      76.1160\n      68.835\n      79.390\n    \n    \n      2002\n      76.700600\n      77.5365\n      70.845\n      80.620\n    \n    \n      2007\n      77.648600\n      78.6085\n      71.777\n      81.757\n    \n    \n      Oceania\n      1997\n      78.190000\n      78.1900\n      77.550\n      78.830\n    \n    \n      2002\n      79.740000\n      79.7400\n      79.110\n      80.370\n    \n    \n      2007\n      80.719500\n      80.7195\n      80.204\n      81.235\n    \n  \n\n\n\n\n\n\n\nBy now you should be able to select the rows and columns, join together dataframes, and create some basic summarizations of data. The next section will look at some of the more sophisticated and elegant tools for understanding and presenting your data. But the building blocks you have just learned about are the meat and potatoes of data analysis in the python world."
  }
]