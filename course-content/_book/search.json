[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science using Python",
    "section": "",
    "text": "This is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\n\nThe Messy Side of Data\n\n\n\n\n\nThe Messy Side of Data"
  },
  {
    "objectID": "00_introduction_to_python.html",
    "href": "00_introduction_to_python.html",
    "title": "1  Introduction to Python",
    "section": "",
    "text": "Data has become interwined with the inner workings of nearly every facet of working within the BC Public Service. Whether you have to read an excel spreadsheet, prepare a report based on a survey, comb through csv files to find a specific data source, it is likely that you have worked with a dataset at some point in your career. However, the process of looking at and dealing with data can be messy, error-prone and hard to duplicate. Questions such as ‘wait, how did I get that number again?’ are all too common.\n\n\n\nThe Messy Side of Data\n\n\nThese lessons will teach you how to interact with data in a systematic way using the python ecosystem. By accessing and interpreting data through a set of prescribed methods (developed through the code written), our work with data becomes more accessible, repeatable, and ultimately insightful.\nDuring the course of these lessons, we hope to cover:\n\nPython preliminaries\nExploring and cleaning raw data\nUsing statistical methods\nPlotting results graphically\n\nIf we have time, we may touch on some more advanced python lessons, such as:\n\nPublishing reports\nAccessing the B.C. Data Catalog\nMachine learning in python"
  },
  {
    "objectID": "00_introduction_to_python.html#before-starting",
    "href": "00_introduction_to_python.html#before-starting",
    "title": "1  Introduction to Python",
    "section": "1.2 BEFORE STARTING!!",
    "text": "1.2 BEFORE STARTING!!\nSo that we can hit the ground running with this workshop, we are asking that everyone get some basic python tools downloaded and installed before the workshop starts. Tools that we will use include Anaconda (or Miniconda) as well as VSCode. A basic knowledge of the command line/powershell interface will be useful as well, but we will try to keep our use of this to a minimum.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are having issues installing anything we have requested prior to the start of the workshop, please let us know so we can work with you so that we can hit the ground running!\n\n\n\nAnaconda/Miniconda is used to download, install, and organize both python and any packages/libraries we use within python. The actual program doing the organizing is conda, while we will use an anaconda powershell to do the installs and interface with python.\nVSCode is a tool used to write, edit and test code known as an IDE (Integrated Development Environment). It is available for more languages than just python, and its versatility has made it a widespread tool within the BCPS.\n\n\nInstall our Python Tools\nIf you do not have administrative rights on your computer:\nDownload Anaconda and VSCode from the B.C. Government Software Centre:\n\nInstall Anaconda (Anaconda3X64 2022.05 Gen P0)\nInstall VSCode (VSCodeX64 1.55.2 Gen P0)\n\nIf you do have administrative rights on your computer:\n\nIf you have administrative rights on your computer, we suggest downloading the lightweight version of Anaconda called Miniconda.\n\nLink to instructions here!\n\nFind the latest version of VSCode here.\n\n\n\nInstall some Python Packages\nMost of the time, when using python we are not using it by itself, but in conjunction with powerful libraries that have already been built to make our data analysis easier. In order to use these tools, we have to install them as well. Using a package manager such as conda makes our life much easier, as we can safely install tools into local environments where every library installed is checked for compatability with every other library. By utilizing the local conda environments, we maintain clean working spaces that can be easily reproduced between workstations.\nLet’s run through the basic steps of setting up a conda environment, installing python and some packages, and testing that it worked!\n\nOpen an anaconda powershell prompt from your search bar.\n\n\n\n\nPowershell Prompt\n\n\n\nInside the anaconda powershell prompt, create a new local conda environment named ds-env using the following commands (hit Enter or type Y and hit Enter when asked to proceed):\n\n\n\nAnaconda Powershell Prompt\n\n> conda create --name ds-env\n> conda activate ds-env\n\n\n\n\nCreating a Conda Environment\n\n\n\nYou should notice that running this second command switches the name in brackets at the beginning of your prompt from (base) to (ds-env). This means we have successfully created a new, empty environment to work in.\n\nInstall python and some useful datascience packages by typing the following commands into the same powershell prompt window:\n\n\n\nAnaconda Powershell Prompt\n\n> conda install python=3.9\n> conda install notebook jupyterlab ipywidgets matplotlib seaborn numpy scikit-learn pandas openpyxl\n\n\nMake sure that python installed successfully. From the same anaconda powershell prompt, simply type python. If this causes no error, success! Try typing this command in the python environment that started to make sure the packages installed as well:\n\n\nimport pandas\npandas.__version__\n\n\n\n\nTesting the python installation\n\n\n\nIf this all works with no errors, python was successfully installed.\n\n\nSetup our VSCode Environment\nStill with me? Great. Here’s a cute otter as congratulations for making it this far.\n\n\n\nThe cutest.\n\n\n\nWe have just a few more steps to go.\n\nOpen the VSCode program.\nOn the left toolbar, find the extensions tab (It looks like 4 squares). Search for the python extension and install this extension.\nFor those using Windows computers, change your default terminal to the command prompt:\n\nFrom anywhere inside VSCode, hit Ctrl + Shift + P. This will open up the command palette.\nStart typing Terminal: Select Default Profile until this option pops up.\nChoose this option, and then click on Command Prompt\n\n\nThat’s it. We are ready to go!"
  },
  {
    "objectID": "00_introduction_to_python.html#hello-world",
    "href": "00_introduction_to_python.html#hello-world",
    "title": "1  Introduction to Python",
    "section": "1.3 Hello World",
    "text": "1.3 Hello World\ni.e. the how many different ways can we print Hello World! to our screen? section\n\n\n\nHello World!\n\n\n\nThere are many different ways in which we can interact with python. These include:\n\nFrom the command line\nInside a Jupyter Notebook\nFrom a file (inside VSCode)\n\nIn this next section, we are going to have a brief introduction to all of these methods of interaction.\n\n\n\n\n\n\nTip: Using the command line\n\n\n\nIt’s worth pointing out that the methods that we will focus on in this course will rely on using VSCode or JupyterLab and all of its inner workings. However, if you are comfortable with the command line, we can also access any of these methods directly from there as well, you just need to be able to move to directories before typing commands. If you use the command line, I recommend using an anaconda powershell prompt, as this allows for the easiest use and access to conda commands and only the smallest of headaches.\n\n\n\nStep 0\nIn all cases, we will want to have a folder from which we wish to work out of. Take some time to set up a folder somewhere you won’t lose it. For me, I’ve simply made a folder called Intro to Python on my C: drive that will hold any course materials we use/create here.\nNext, to make any interactions with python, we will want to open VSCode and work from here. When we first open VSCode, you should be prompted to open a folder. We are going to work out of that Intro to Python folder, so open it here. After doing this, we should now have a VSCode screen open that will look something like this:\n\n\n\nVS Code\n\n\n\nWe have 3 main areas that we can utilize:\n\nTo the left (area A): is the current folder and subfolder directory list. We can make new files directly from here.\nTo the right (area B): this is where files we are working on will live. For some file types, preview windows will be available as well.\nTo the bottom (area C): this is where we can open and run commands from the command line (or terminal).\n\nNow remember, we set up a special environment that contains python and our data science packages. We want to make sure we are always using this environment, so in the open terminal, re-type conda activate ds-env and this terminal will now be open in this environment. We also want to check that VSCode itself (and not just the terminal) will use the same environment. We again access the command palette with Ctrl + Shift + P, and begin typing Python: Select Interpreter. Click on this, and choose the ds-env option. We are good to go!\n\n\n\n\n\n\nConda Environments\n\n\n\nAlthough it does add an extra level of set-up whenever we start a python project, having these conda environments ends up being incredibly important for not only reproducibility, but making sure that packages work well together. When in doubt as to if you are using the correct environment, double check that the terminal you are using has (ds-env) in brackets at the start of a line.\n\n\n\n\nFrom the command line/terminal\nLet’s start with an easy one. To start a python session from a terminal, simply type python at the command line, and the terminal will automatically open a python interface. You will know you are inside the python interface if your command lines now start with >>>. Now, let’s do the classic Hello World command for python:\n\nprint('Hello World!')\n\nHello World!\n\n\nTo exit the python interface and return to the regular terminal, you can type exit() and return to the terminal.\n\n\nFrom a file (in VSCode)\nNext up, let’s run an entire python file to tell us hello. Inside our directory, create a new file called hello_world.py. Note that .py extension - this signifies that the content inside will be python code. Inside this file, let’s have two lines of code:\n\n\nhello_world.py\n\nprint(\"Hello World!\")\nprint(\"Otters are the best animal.\")\n\nTo run this code, first save the file, and then simply click the play button (triangle in the top right!). Note that this will display an output in a terminal at the bottom of VSCode. VSCode takes the python file you told it to run, and will run every line of code individually. Thus, we get two lines of output for the two print statements.\nBut wait, there’s more! In VSCode we can run individual lines of code within a file as well. Simply move your cursor to the line you wish to run, and hit Shift+Enter.\nNote the difference here. Instead of running the entire file, VSCode actually opened up a python window inside our terminal, and ran the single line of code, just like we did before.\n\nChallenge 1\n\nRun the other line of code, and then add and run third line of code that prints your favourite TV show.\n\n\nSolution to Challenge 1\n\n hit Shift+Enter while the cursor is on the other line of code. Add a line of code such as:\nprint(\"C'mon son! You know the best show is Psych!\")\nSave the file, and again press Shift+Enter while on this line.\n\n\n\n\n\nChallenge 2\n\nTry clicking the play button again. What happens here? Can you explain why?\n\n\nSolution to Challenge 2\n\n An error will occur in the terminal. This is because VSCode tried to run the entire program again, but from inside an already open python program (which we opened when we ran a single line using the Shift+Enter method). To fix this, we can either exit() the current python execution in the current terminal, or Kill/Delete the open terminal. This button can be found by hovering over the Python symbol at the right hand side of the terminals.\n\n\n\n\n\n\nFrom JupyterLab\nJupyterLab is an application with a web-based user interface that enables one to work with documents and activities such as Jupyter notebooks, text editors, terminals, and even custom components in a flexible, integrated, and extensible manner.\nTo start JupyterLab, you can use Anaconda Navigator, a GUI that comes packaged with Anaconda if you wish. However it is nearly always easier to access it from the command line. Inside VSCode, navigate to a new terminal.\n\n\n\n\n\n\nConda Reminder!\n\n\n\nMake sure that you double check that this new terminal is opened with the ds-env!\n\n\nFrom the terminal, to launch a new JupyterLab session, simply type:\n\n\nterminal\n\n> jupyter lab\n\nThis should open up a screen that looks something like this:\n\n\n\nJupyterLab\n\n\n\n\n\n\n\n\n\nFun Fact\n\n\n\nYou might have noticed by this point that the author of this section prefers dark mode. So if any of your programs are popping up in a different/lighter colour scheme, that’s okay!\n\n\nWe will be using the JupyterLab interface quite a bit, so let’s get used to the key pieces. Similar to VSCode, we have a few key areas to utilize:\n\nOn the left (area A), we have the sidebar that contains commonly used tabs. These include the file directory system (which defaults to the folder from which the session was launched), a list of open files, running kernels and terminals, a table of contents for any markdown that is written, and possibly a set of extensions.\nOn the right (area B), we have the main work area, where we can open new notebooks, terminals, files, etc. Here, when we have multiple open tabs, we can drag them around the main area to produce smaller panels displaying multiple pieces of work.\n\nGenerally speaking, we will be using Jupyter Notebooks, which have a .ipynb file extension. Like python files, these notebooks can run python code. However, they also support markdown (text that can be added to support source code with explanations) as well as inline viewing of data tables, plots and more. This allows us to mix source code, text and images all in one file that we can quickly use for anything such as:\n\nAnswering ‘how did I get this number in that report?’\nLooking at the number.\nUpdating the number with an updated dataset.\nPlotting the number with other numbers, and then looking at all of those numbers.\n\nLet’s open up a notebook (click the Python 3 icon underneath the Notebook heading). This will open up a new tab called Untitled.ipynb.\n\nChallenge 3\n\nRename the notebook to hello_world.ipynb.\n\n\nSolution to Challenge 3\n\n On either the tab to the top, or the filename that popped up on the left panel, right click the file Untitled.ipynb and click Rename notebook.... Enter your new name here.\n\n\n\nIn this new file, we have a single blue box (this is a single cell). We can type multiple lines of code inside a single cell. Within the blue box, pressing Enter will let you add a new line to the cell. To execute a cell, we press Shift+Enter. This will execute every line of code within that cell. If there is output to display, it will display in the space directly below the cell. Pressing the + at the top of the tab will add a new cell (as will running the bottom-most cell). The ‘active’ cell will always be the one that is highlighted with a blue box.\n\n\nChallenge 4\n\nPrint ‘Hello World’ inside the Jupyter Notebook.\n\n\nSolution to Challenge 4\n\n Inside the blue box, write the python code:\nprint('Hello World!')\nWhile still on this cell (highlighted in blue), press Shift+Enter. We should see the output directly below! \n\n\nWhen we are done with a JupyterLab session, we must shutdown the server. From the Menu Bar select the File menu and then choose Shut Down at the bottom of the dropdown menu. You will be prompted to confirm that you wish to shutdown the JupyterLab server (don’t forget to save your work!). Click Shut Down to shutdown the JupyterLab server.\n\n\n\n\n\n\nJupyterLab and Us!\n\n\n\nWe will be using JupyterLab for the vast majority of our work throughout this course because of the ease of use in writing and executing code all in one place. Make sure you are comfortable with:\n\nopening JupyterLab to a specific folder\ncreating and renaming notebooks\nadding python code to multiple cells within a notebook\nexecuting entire code blocks\n\nIf you are unsure about any of these pieces, please ask for help!"
  },
  {
    "objectID": "31_exploring_data_structures.html",
    "href": "31_exploring_data_structures.html",
    "title": "2  Exploring Data Structures",
    "section": "",
    "text": "Getting a high level summary of the data is important but data is particularly valuable when refined. Your analysis will start to come alive when we start to do some slicing and dicing and grouping of data or even creating additional variables. In an Excel world, this is like when you use filter options for columns, or create pivot tables, or when you create a formula in a new column to create a new variable. In data science parlance, this is the kind of thing that is referred to as data wrangling - getting the (already cleaned) data you want in the form you want it in.\nIn the world of python, this usually means working with a library or package you have already been introduced to called pandas. It let’s you do so much!\nThe first dataset we’ll look at is one that looks at data for countries around the world and shows population levels as well as gdp per capita over a number of years. It is a great data set to look at.\n\n\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/gapfinder.csv\"\ndf = pd.read_csv(url)\n\nIt’s usually a good idea right away to take a quick look at the data to make sure what we have read in makes sense. So let’s do that using some pandas methods that we covered earlier.\n\n\n\n\n\n\nRemember\n\n\n\nWhen you create a variable or object, you can name it pretty much whatever you want. But it is a best practice to name it something intuitive but not overly verbose. Also remember to not leave spaces in your names, and to always be consistent in the naming conventions you use (e.g. camelCase, underscore_case, etc.)\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1704 entries, 0 to 1703\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   country    1704 non-null   object \n 1   year       1704 non-null   int64  \n 2   pop        1704 non-null   float64\n 3   continent  1704 non-null   object \n 4   lifeExp    1704 non-null   float64\n 5   gdpPercap  1704 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 80.0+ KB\n\n\nSo this data is looking good so far. We can see that we have created a pandas dataframe within our python environment. There are 1704 rows of data and 6 columns or variables. You can see there are only non-null values… so happily there is no missing data to worry about.\n\ndf.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n  \n\n\n\n\nThe .head() method let’s us look at actual data like we would likely do in Excel. This dataset has some high level country metrics about population and the economy going back to 1952.\n\n\n\nIn the “real world” of data its not uncommon to see hundreds or thousands of columns, so knowing how to pare down our dataset so it is not overly bloated is important. Our dataset is just 6 columns wide, so not too big, but let’s go ahead and narrow it down anyway.\nLet’s create an object (in this case a dataframe) called “narrow_df” and then specify the names of columns that we want to have in our slimmed down dataset. We are primarily interested in population, but we want to have year, country, and continent in there too. After we create it let’s take a look at the new dataframe:\n\nnarrow_df = df[['country', 'year', 'pop', 'continent']]\nnarrow_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n    \n  \n\n\n\n\nSo now we have a more manageable dataset to work with called narrow_df. The original dataframe df is still there, and still the same size as before it we want to have a look though.\n\n\n\n\n\n\nImportant\n\n\n\nWhen you create a variable or dataframe in python, to the left of the = sign you always put the name of the thing you want to make or modify. To the right, that’s where you put the contents of what it is you want to create. It may feel counterintuitive! Especially if you are using the same variable or dataframe identifier on both sides of the equals sign.\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nLet’s say you just want to narrow down the dataset to include just the country and the GDP per capita. How would you do it? Don’t forget to run your code so it also shows a view of the result so you can confirm the code worked as you wanted it to.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nFirst you would like to create a new object to hold this narrowed down version of the dataset, then just use some method like .head() to see some of the resulting data.\nchallenge1_df = df[['country', 'gdpPercap']]\nchallenge1_df.head(2)\n\n\n\n\n\n\nOf course, in data analysis we are usually interested in looking at just some of rows of data, not all of it all the time. So when we want to look at just selected rows (i.e. values within a given column) there is a hand pandas function called .query() that we can leverage. It helps us with selecting the rows we want.\nWe set up an expression in pandas with .query() and it will evaluate it for us. For example, imagine we want to find just the rows where the year is 1972. Here’s how we do that, using == instead of just =:\n\nfiltered_df = df.query('year == 1972')\nfiltered_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n    \n      16\n      Albania\n      1972\n      2263554.0\n      Europe\n      67.690\n      3313.422188\n    \n  \n\n\n\n\nWe can even get more specific with string variables, looking for content by starting letter or letters. For example, here’s how we can look for specific countries:\n\nfiltered_df = df.query('country.str.startswith(\"C\") & year == 1972')\nfiltered_df.head(3)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      220\n      Cambodia\n      1972\n      7450606.0\n      Asia\n      40.317\n      421.624026\n    \n    \n      232\n      Cameroon\n      1972\n      7021028.0\n      Africa\n      47.049\n      1684.146528\n    \n    \n      244\n      Canada\n      1972\n      22284500.0\n      Americas\n      72.880\n      18970.570860\n    \n  \n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\nYour director has come to you and asked if you what the life expectancy was is in Canada in 2007. How would you use pandas code to get the data you need? And after having run the code, what’s the answer?\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\nFirst you would like to create a new object to hold this narrowed down version of the dataset, then just use some method like .print() or .head() to see some of the resulting data.\nchallenge2_df = df.query('country.str.startswith(\"Canada\") & year == 1972')\nprint(challenge2_df)\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThere is something else you should know about row selection. It is VERY common within python/pandas to see the .loc() and .iloc() codes to identify rows by index location. The index starts at 0 for the first row (yes, “0”, and not “1”)! And you can also use this method You may not ever need to use it, but it is good to know what it means when you come across it.\n\n\n\nx_df = df.loc[0:1]\nprint(x_df)\n\n       country  year        pop continent  lifeExp   gdpPercap\n0  Afghanistan  1952  8425333.0      Asia   28.801  779.445314\n1  Afghanistan  1957  9240934.0      Asia   30.332  820.853030\n\n\n\n\n\nSo now you’ve mastered how to select rows and columns you want, congratulations!\nInstead of hunting and pecking for insights, one way to quickly make some sense of the data is to sort it - something you probably do in Excel all the time.\nAbove, we have created a dataframe with just the one country in it and for all years above 1972. So now let’s sort the data by year, and then see what can spot any insights. To do this, we use the .sort_values() method.\nLet’s go fetch and look at the filtered_df dataframe from above, and use this method to sort by year, going from more recent to less recent:\n\nfiltered_df.sort_values('lifeExp', ascending=False)\n#\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      244\n      Canada\n      1972\n      22284500.0\n      Americas\n      72.88000\n      18970.570860\n    \n    \n      388\n      Cuba\n      1972\n      8831348.0\n      Americas\n      70.72300\n      5305.445256\n    \n    \n      400\n      Czech Republic\n      1972\n      9862158.0\n      Europe\n      70.29000\n      13108.453600\n    \n    \n      376\n      Croatia\n      1972\n      4225310.0\n      Europe\n      69.61000\n      9164.090127\n    \n    \n      352\n      Costa Rica\n      1972\n      1834796.0\n      Americas\n      67.84900\n      5118.146939\n    \n    \n      280\n      Chile\n      1972\n      9717524.0\n      Americas\n      63.44100\n      5494.024437\n    \n    \n      292\n      China\n      1972\n      862030000.0\n      Asia\n      63.11888\n      676.900092\n    \n    \n      304\n      Colombia\n      1972\n      22542890.0\n      Americas\n      61.62300\n      3264.660041\n    \n    \n      340\n      Congo Rep.\n      1972\n      1340458.0\n      Africa\n      54.90700\n      3213.152683\n    \n    \n      364\n      Cote d'Ivoire\n      1972\n      6071696.0\n      Africa\n      49.80100\n      2378.201111\n    \n    \n      316\n      Comoros\n      1972\n      250027.0\n      Africa\n      48.94400\n      1937.577675\n    \n    \n      232\n      Cameroon\n      1972\n      7021028.0\n      Africa\n      47.04900\n      1684.146528\n    \n    \n      328\n      Congo Dem. Rep.\n      1972\n      23007669.0\n      Africa\n      45.98900\n      904.896068\n    \n    \n      268\n      Chad\n      1972\n      3899068.0\n      Africa\n      45.56900\n      1104.103987\n    \n    \n      256\n      Central African Republic\n      1972\n      1927260.0\n      Africa\n      43.45700\n      1070.013275\n    \n    \n      220\n      Cambodia\n      1972\n      7450606.0\n      Asia\n      40.31700\n      421.624026\n    \n  \n\n\n\n\nHere we can read a few insights out of the data.\n\n\n\nYou can also put multiple methods in the same line of code, as long as you separate each of the elements. In pandas, this is called “method chaining”, as you are kind of creating a chain of actions to take place.\n\nfiltered_df = df.query('country.str.startswith(\"Ca\") & year == 2007').sort_values('year', ascending = False)\nfiltered_df.head(3)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      227\n      Cambodia\n      2007\n      14131858.0\n      Asia\n      59.723\n      1713.778686\n    \n    \n      239\n      Cameroon\n      2007\n      17696293.0\n      Africa\n      50.430\n      2042.095240\n    \n    \n      251\n      Canada\n      2007\n      33390141.0\n      Americas\n      80.653\n      36319.235010\n    \n  \n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\nYour director has come back to you and wondered aloud whether the life expectancy of people changed during the 1970s in Cambodia. Use what you know about selecting rows and sorting data to get the data you need to answer the question.\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nUsing the query() and sort.values() methods and by chaining them together. Looking at the data we see that in the 1970s there was a sharp decline in life expectancy in Cambodia. We also see, thankfully, that is has recovered strongly since then.\nchallenge3_df = df.query('country.str.startswith(\"Cambodia\") & year < 2008').sort_values('year', ascending = False)\nprint(challenge3_df)\n\n\n\n\n\n\nVery often we have some data in our dataset that we want to transform to give us additional information.\nIn Excel this is something that is done all the time by creating a formula in a cell that refers to other columns and applies some sort of logic or mathematical expression to it. In pandas, one handy tool to use is the .assign() method.\nImagine we want to know what the actual GDP is for a given country for a given year and not just the GDP per capita. Can we do this? Let’s take a look. It’s with the help of an additional function called lambda.\nWe create our own little function right inside of the code that allows us to create this column (or variable, or feature). The logic is:\n(new_variable_name = lambda x: some expression involving x and math and/or other variables)\nThe .assign() method looks at this expression and then returns the value it is asked to do. This is how it comes all together to give us actual GDP for each row in our dataframe:\n\nnew_df = df.assign(gdp=lambda x: x['pop'] * x['gdpPercap'])\nnew_df = new_df.query('year == 2007').sort_values('gdp', ascending = False)\nnew_df.head(5)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      gdp\n    \n  \n  \n    \n      1619\n      United States\n      2007\n      3.011399e+08\n      Americas\n      78.242\n      42951.653090\n      1.293446e+13\n    \n    \n      299\n      China\n      2007\n      1.318683e+09\n      Asia\n      72.961\n      4959.114854\n      6.539501e+12\n    \n    \n      803\n      Japan\n      2007\n      1.274680e+08\n      Asia\n      82.603\n      31656.068060\n      4.035135e+12\n    \n    \n      707\n      India\n      2007\n      1.110396e+09\n      Asia\n      64.698\n      2452.210407\n      2.722925e+12\n    \n    \n      575\n      Germany\n      2007\n      8.240100e+07\n      Europe\n      79.406\n      32170.374420\n      2.650871e+12\n    \n  \n\n\n\n\nSo now we can see which countries had the biggest GDP in 2007. Notice that the gdp data is in scientific notation (i.e. the decimal number times x number of zeros), so it’s a bit hard to read. If we wanted readers to consume that data we would go ahead and change the data type for it. But for current purposes we’ll leave that alone.\n\n\n\nOne of the most important tasks in data analysis is to be able to join multiple datasets together. There is a second file that we can get regarding country size in square kilometers. Let’s fetch that data and take a look.\n\nurl2 = \"https://raw.githubusercontent.com/bcgov/ds-intro-to-python/main/data/countrysize.csv\"\ndf2 = pd.read_csv(url2, encoding= 'unicode_escape')\ndf2.info()\ndf2.sample(2)\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 235 entries, 0 to 234\nData columns (total 2 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   nation           235 non-null    object\n 1   area_square_kms  235 non-null    int64 \ndtypes: int64(1), object(1)\nmemory usage: 3.8+ KB\n\n\n\n\n\n\n  \n    \n      \n      nation\n      area_square_kms\n    \n  \n  \n    \n      78\n      Guinea\n      245857\n    \n    \n      109\n      Hungary\n      93028\n    \n  \n\n\n\n\nOk, this new dataset looks like we expect - a list of countries alongside the land size in square kilometers.\nWhile pandas makes it possible to join two dataframes on their index, joining on a particular “key” column is also possible. Joining on a key column is likely more intuitive for those familiar with merging with vlookup in Excel or joining tables with SQL. So let’s go ahead and join this based on the “country” column from the original dataset and “nation” from the second one. In database management language, we are going to to a left join (perhaps the most common type of join).\n\n\n\n\n\n\nImportant\n\n\n\nIn pandas, there are two methods we can use to bring these datasets together, namely the .merge() and the .join() function. Both can accomplish many of the same tasks and you likely will come across both. But it probably makes sense to keep it simple and get familiar with one of them first, then move on to the next one later.\n\n\nIn the example below we will use the .join() function. We will think of “df” as being the original dataset on the left, and df2 as the other dataset that we want to merge data from. We use for both the variable ‘country’ as the key to join on. We use an inline .set_index() function in order to preserve a nice looking index column on the left of the dataframe, based on the index of df.\n\njoin_df = df.join(df2.set_index('nation'), on='country')\njoin_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      area_square_kms\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      652230.0\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      652230.0\n    \n  \n\n\n\n\nNow you want to see all of the columns in this new dataset but just for the year 2007 and you want to see it organized by gdp per capita.\n\n\n\n\n\n\nChallenge 4\n\n\n\nYour director has come back, yet again, and wants to know what top five countries were in terms GDP per capita in 2007.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nUsing the .query() method, you specfy that you want to return the values that are equivalent to the year of interest. Then you can chain along the .sort_values() method\nchallenge4_df = join_df.query('year == 2007').sort_values('gdpPercap', ascending = False)\nchallenge4_df.head(5)\n\n\n\n\nchallenge4_df = join_df.query('year == 2007').sort_values('gdpPercap', ascending = False)\nchallenge4_df.head(5)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      area_square_kms\n    \n  \n  \n    \n      1151\n      Norway\n      2007\n      4627926.0\n      Europe\n      80.196\n      49357.19017\n      323802.0\n    \n    \n      863\n      Kuwait\n      2007\n      2505559.0\n      Asia\n      77.588\n      47306.98978\n      17818.0\n    \n    \n      1367\n      Singapore\n      2007\n      4553009.0\n      Asia\n      79.972\n      47143.17964\n      710.0\n    \n    \n      1619\n      United States\n      2007\n      301139947.0\n      Americas\n      78.242\n      42951.65309\n      9372610.0\n    \n    \n      755\n      Ireland\n      2007\n      4109086.0\n      Europe\n      78.885\n      40675.99635\n      70273.0\n    \n  \n\n\n\n\n\n\n\nSometimes of course you would prefer to group rows together for the purpose of summarizing them somehow. With our current dataset, we would like to summarize it so that we can see the breakdown of the metrics for each year by country in alphabetical order.\nWe can accomplish this using the .groupby() method together with another mathematical function.\n\ngroup_df = join_df.groupby(by=['year']).sum()\ngroup_df.head(20)\n\n/var/folders/h1/1c4m6wcn3wqgwhgdzlbvj5kh0000gn/T/ipykernel_54719/3741548511.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.sum is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n  group_df = join_df.groupby(by=['year']).sum()\n\n\n\n\n\n\n  \n    \n      \n      pop\n      lifeExp\n      gdpPercap\n      area_square_kms\n    \n    \n      year\n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      2.406957e+09\n      6966.18200\n      5.289892e+05\n      105279084.0\n    \n    \n      1957\n      2.664405e+09\n      7314.05096\n      6.105160e+05\n      105279084.0\n    \n    \n      1962\n      2.899783e+09\n      7612.51336\n      6.710654e+05\n      105279084.0\n    \n    \n      1967\n      3.217478e+09\n      7906.31712\n      7.786787e+05\n      105279084.0\n    \n    \n      1972\n      3.576977e+09\n      8185.92888\n      9.613518e+05\n      105279084.0\n    \n    \n      1977\n      3.930046e+09\n      8458.96236\n      1.038470e+06\n      105279084.0\n    \n    \n      1982\n      4.289437e+09\n      8737.71400\n      1.067684e+06\n      105279084.0\n    \n    \n      1987\n      4.691477e+09\n      8976.19100\n      1.121931e+06\n      105279084.0\n    \n    \n      1992\n      5.110710e+09\n      9110.76800\n      1.158522e+06\n      105279084.0\n    \n    \n      1997\n      5.515204e+09\n      9232.08400\n      1.290805e+06\n      105279084.0\n    \n    \n      2002\n      5.886978e+09\n      9328.67900\n      1.408334e+06\n      105279084.0\n    \n    \n      2007\n      6.251013e+09\n      9515.05400\n      1.658570e+06\n      105279084.0\n    \n  \n\n\n\n\nIn the example above we see the sum of each of the metrics for all of the values in the respective year. So we see that world population grew from about 2.5 billion people in 1952 to over 6.25 billion in 2007.\n\n\n\nBy now you should be able to select the rows and columns of data you want directly and generate some quick on the fly insights as you need to. The next section will look at some of the more sophisticated and elegant tools for understanding and presenting your data. But the building blocks you have just learned about are the meat and potatoes of data analysis in the python world."
  }
]