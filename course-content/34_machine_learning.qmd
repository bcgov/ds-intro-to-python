---
title: Machine Learning
teaching: 45
author: Stuart Hemerling
exercises: 1
objectives:
  - to understand what machine learning is and how python relates to it
  - to be able to describe what the methods .fit() and .predict() will do in basic sklearn use case
  - to see and follow along with a simple use case
keypoints: null
jupyter: python3
---

## Intro to Machine Learning and Scikit Learn

This is primarily a course about Python and not machine learning. While we don't want to dive too deeply into what machine learning is and why it is popular, presenting an overview will help set the context ahead of the use case we'll walk through. 

Machine learning is a method of teaching computers to learn and make decisions on their own, without being explicitly programmed. It involves feeding a computer system a large amount of data and allowing the system to discover patterns and relationships in the data, and to use these patterns to make predictions or decisions. Machine learning is used in a wide range of applications, including image and speech recognition, natural language processing, and autonomous vehicles.

The above paragraph is itself an instance of AI at work! I asked the ChatGPT bot to "summarize machine learning in 100 words" and that's what it came up with. While one can quibble with parts of it, the essence is quite good, and certainly plausibly human sounding. AI and machine learning is all around us already, mostly without our awareness of it.  The image below shows the wide range of use cases that machine learning impacts today.


<center>
    <img src="images/machine-learning/ml_use_cases.PNG" width=100% style="margin:auto" border=1/>
    <p style="text-align: center">
    Types of machine learning use cases
    </p>
</center>


Scikit-learn - also known as sklearn - is an open-source machine learning library for the Python programming language and is the new library we will access in this tutorial. It is built on top of other popular Python libraries such as NumPy and pandas, and it provides a wide range of tools and algorithms for tasks such as classification, regression, clustering, and dimensionality reduction. 

While there are other libraries outside of sklearn that support machine learning in Python (e.g. PyTorch, Tensorflow), it is widely used in the data science community and is known for its ease of use and efficient implementation of algorithms. 

Now that you know some of the basics, the best way to see how machine learning is done is through tackling a meaningful problem with some real data. 


## Defining a Problem

Imagine you are a server at a restaurant and want to maximize your tip-earning potential. Being able to predict which customers are more likely to leave you a big tip would definitely be useful!  This is a nice, if simple, use case that machine learning can help us with.  

<center>
    <img src="images/machine-learning/tipping.PNG" width=90% style="margin:auto" border=1/>
    <p style="text-align: center">
    Decisions, decisions!
    </p>
</center>


It is also a classic "supervised" machine learning use case, where we will need to have both input and output data available to analyze. By **supervised**, we mean that in our data we have the target data, or outputs, that we want to be able to predict (see workflow for a supervised use case in the image below). You can also think of the workflow as a high level pipeline. 

<center>
    <img src="images/machine-learning/supervised_learning_workflow.PNG" width=70% style="margin:auto" border=1/>
    <p style="text-align: center">
        Supervised machine learning workflow
    </p>
</center>


## Getting necessary Data

We find out that there is a small, but thankfully clean and available dataset, made available by Seaborn. It contains restaurant visit data from 244 parties of customers to a given restaurant. Each row of data represents a restaurant visit by one party of one or more people. Below are the fields included in the dataset:

- tip in dollars <-- our target variable
- bill in dollars
- sex of the bill payer
- whether there were smokers in the party
- day of the week
- time of day
- size of the party

 We already know that there is no missing data, so that's great.  Let's run the `sample()` method on the dataframe to get a good balanced initial feel for the data. 

```{python}
# importing libraries for getting the data and pre-processing it

import seaborn as sns
import pandas as pd

tips_df = sns.load_dataset('tips')
tips_df.sample(10)
```




## Choosing between Regression and Classification 

With our tip predictor project, there are two approaches we could take and we must decide which one to take at the outset. One approach is to predict the *actual size of the tip* one would expect to get. This is what would be known as a regression type problem. The other way is to define *what constitutes a bigger tip* from the outset and to predict how likely you would be to get that. This is called a classification type of problem. There is no correct answer: the model used is use-case dependent, and each will have their own pros and cons. 

For this tutorial, we will structure the problem as a classification problem. One of the requirements of this is to have a binary classifier variable, which consists of a positive class and a negative class of the outcome we are wanting to predict. Time to start rolling up our coding sleeves and get into the pre-processing!

## Pre-processing the Data

 Let's first take a look at a summary of the numeric data in the dataset, with a particular eye on the "tip" column, as that is the data we will want to transform:


```{python}
tips_df.describe()
```

We see that the median tip is $2.90, so we will define anything above that amount as a big tip (positive class) and anything below as essentially not a big tip (negative class). That creates a nice balance, with roughly 50% of cases containing big tippers and the remaining 50% not. 

To accomplish this, let's run some code using `lambda` and `apply()` to compute a big tipper variable and then take another look at the dataframe. 

```{python}
tips_df['big_tipper'] = tips_df['tip'].apply(lambda x: 0 if x < 2.9 else 1)
tips_df.sample(5)
```

Well that seemed to work fine! The machine learning classification model that we will use can only use numeric data. You probably know from statistics that you can convert categorical variables into numeric ones by turning each of the subsets into discrete columns consisting of 1s and 0s. They are widely referred to as dummy variables. Dummies are like flags in the data, where 1 means a certain subset is "observed" and 0 means that particular subset is "absent". 

Fortunately, there is a method called `get_dummies()` in pandas that can transform all non-numeric variables easily. In the code below, we pass in the whole dataframe and it will do its thing.

```{python}
tips_df = pd.get_dummies(tips_df)
tips_df.head()
```
<br>
At this point, we could go ahead and eliminate some of these newly created columns. For example, we could drop "time_Lunch" as this is essentially computationally redundant to "time_Dinner", just the exact obverse (i.e. if you know whether a meal was lunch or not, you know whether it was dinner or not) But we'll leave that for now, their continued presence in the data will not harm the model.

## Identifying Target Variable and Features

The next step is to define which variables are to be considered the independent variables (or features, in machine learning terms) and which singular dependent variable is the target variable that we want to predict. Otherwise, the program won't know what variables are used to predict what. 

Independent variables are usually best thought of as those which are known at the point of decision. In our use case, we want to predict whether a party is likely to be a big tipper when they arrive at the restauarant.  At that point we likely know the number of people in the party, but not the total bill amount. So let's identify the independent variables or features by dropping the variables 'tip', 'total_bill', and 'big_tipper' from an object we will call "X". The dependent or target variable is easier to identify. That is simply "big_tipper". We will assign that a "y". Both "X" (typed usually as upper case X) and "y" (typed usually as lower case y) are widely used conventions in Python for these concepts. 

Let's use what we learned about the `drop()` method to create the features object and assign to y the single column containing the target "big_tipper" variable.

```{python}
X = tips_df.drop(['tip', 'total_bill', 'big_tipper'], axis=1)
y = tips_df['big_tipper']
```

One of the key requirements of supervised machine learning methods is to split the dataset up into training and test samples. For the training set, the "machine" will "learn" which restaurant parties are more likely to be big tippers and use that knowledge to create an algorithm that can be applied to the test set. 

Scikit-Learn makes it easy with its famous `train_test_split()` method:


```{python}
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=155, test_size=0.409)
```

The `train_test_split()` method takes the X and y that we defined above, and splits it randomly according to the relative size we want the test sample to be compared to the total sample size. In our example, we have chosen for the test sample to be 40.9% of the entire dataset.  The percentage size you will choose will depend on a number of factors relating to the use case. 

We now have four separate objects: train and test versions of X and train and test versions of y. It is a good idea to look at their shapes at this point so we can confirm that the sizes look correct.

```{python}
X_train.shape, y_train.shape, X_test.shape, y_test.shape
```

The training objects have 144 rows of the original dataframe and the test objects have 100 rows.  The X objects have 11 feature variables and the y objects consist of one-dimensional arrays, which show  as "blank" to the right of the comma when reviewing object `shape`.


## Fitting and Predicting 

With these preprocessing steps now complete we can apply a classifier method to the training data. Scikit-learn has many classifier methods. For our example, we will use a machine learning algorithm called `LogisticRegression()`. Despite it having "regression" in the name, it is implemented here for classification rather than regression. In logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). 

In our code, we first call `LogisticRegression()` and then `fit()` method then attempts to fit the independent variables to the target variable in the training dataset. In the code below, we run the fitting process and then take that fitted algorithm to product predictions for our test data. 


```{python}
from sklearn.linear_model import LogisticRegression

reg = LogisticRegression()
reg.fit(X_train, y_train)
```
<br>
Now our we have a trained model and we can run that model on our test dataset. We enter X_test as an argument following the `predict()` and `predict_proba()` methods as in the code below. Let us look at the first five cases in our array of predictions:

```{python}
y_predict =  reg.predict(X_test) # makes a decision based on probability
y_predict[:5] # first 5 prediction decisions in test dataset
```
The output gives us an array of predictions for the first five cases in sequence in the test dataset. A value of 1 means that the model predicts the party to be a big tipper, and 0 predicts that it will not be a big tipper. 

Each of the predictions is based on a discrete probability. The `predict_proba()` method returns the class probabilities for each data point.  The number of probabilities for each row is equal to the number of categories in target variable (2 in our case).

```{python}
y_predict_proba =  reg.predict_proba(X_test) # computes a probability per case
y_predict_proba[:5] # first 5 predictions in test dataset
```

We can see for each case a pair of probabilities, one for "0" and the other for "1". This detail helps us understand how the decisions with the `predict()` method were made. 


## Evaluating

Ok. We have our predictions... but are they good? Of course, what "good" means needs a bit of thought. There are many performance measures in data science and there are considerations based on the use cases themselves. How we would evaluate performace of an algorithm that attempts to diagnose serious disease would be different than if we were evaluating one that identified whether an email was spam or not. 

There are a number of standard evaluation tools that are commonly used, however, and we will introduce a few here. 

One of the most common and useful peformance scores is `accuracy_score()`, and it is fairly easy to interpret. It reflects that percentage of cases where the prediction of the model matches exactly the true labels of the sample.

```{python}
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test,y_predict))
```

In our case, it shows that 68% of the models guesses are correct. A standard comparison is to refer to the percentage of the most frequently occuring response in the training set. In our example, we designed that to be 50%. And since 68% > 50%, the model is doing something right!

Another common performance measuring tool is the so-called "confusion matrix". 

```{python}
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_predict)) 
```


<center>
    <img src="images/machine-learning/confusion_matrix.PNG" width=70% style="margin:auto" border=1/>
    <p style="text-align: center">
    Four quadrants of the confusion matrix
    </p>
</center>

The confusion matrix shows how many guesses the model gets "right", that is, the actual is true when we guess true (TP - True Positive) and false when we guess false (TN - True Negative). The remaining two quadrants show the errors. Type I error is the false positive error (FP - False Positive), where the model guesses true but reality is false. Type II error is where where the model guesses false but the reality is true (FN - False Negative).
<br>
<br>
Another widely used report is the classification report, which shows precision and recall, which are two widely reported metrics in machine learning. 

```{python}
from sklearn.metrics import classification_report
print(classification_report(y_test, y_predict))
```

**Precision** = True Positives / (True Positives + False Positives) <br>
**Recall (Sensitivity)** = True Positives / (True Positives + False Negatives)


:::{.callout-warning icon="false"}
## Challenge 1
Given this use case, what do you think is a more appropriate measure to use, precision or recall? 
:::

:::{.callout-note icon="false" collapse="true"}
## Solution to Challenge 1

It depends! If your goal is to try to make sure you can get a big tip each time you choose a customer, then precision is better. If your goal is wanting to avoid missing out on getting a big tipper, then recall is better. 
:::


So there we go, we have nice small proof of concept that we can likely develop a tool that will be helpful to meeting our needs. The next step could be to refine some of the code (e.g. to eliminate redundent columns) and to "tune the parameters" of the model, also creating test and train datasets of different sizes to optimize it. We might also want to improve the model using different estimators (e.g. something other than LogisticRegression) or perhaps several models together! And we ultimately will want to test how the model performs agains out of sample data as well. The development and refinement process can go on for some time before ultimately the development of a new data pipeline to automate the process and make it usable for end users. 
