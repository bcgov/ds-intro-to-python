[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science using Python",
    "section": "",
    "text": "This is a repository to house materials for a 2 day course introducing participants to data science using Python.\nThe goal of this workshop is to teach new-to-programming data professionals to import data, clean up and summarize a data set, and make some static data visualizations using Python. This is an introductory course to programming, specifically programming with Python. Python is a popular computer language for statistics and other scientific disciplines. It is commonly used for statistical analysis, machine-learning, generating high quality visualzations, and automating data workflows.\nThe workshop content will follow best practices for Python for data analysis, giving attendees a foundation in the fundamentals of Python and scientific computing."
  },
  {
    "objectID": "index.html#who-should-take-this-course",
    "href": "index.html#who-should-take-this-course",
    "title": "Introduction to Data Science using Python",
    "section": "Who should take this course?",
    "text": "Who should take this course?\n\nAnyone who works with data or who is interested in learning efficient ways to make meaning from data\nAnyone keen to learn a programming language (no prior experience necessary!)"
  },
  {
    "objectID": "index.html#workshop-schedule",
    "href": "index.html#workshop-schedule",
    "title": "Introduction to Data Science using Python",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\nDaily Schedule\n\n\n\nActivity\nStart Time\nEnd Time\n\n\n\n\nSession 1\n9:00\n10:30\n\n\nBreak\n10:30\n10:45\n\n\nSession 2\n10:45\n12:00\n\n\nLunch\n12:00\n1:00\n\n\nSession 3\n1:00\n2:30\n\n\nBreak\n2:30\n2:45\n\n\nSession 4\n2:45\n4:30\n\n\n\n\nPre-Course Work\n\n\n\n\n\n\nImportant!\n\n\n\nBefore the course starts, we ask that all attendees install Python and its associated packages required for data analysis! Instructions for how to do so are found on the next page. If anyone is having troubles getting Python up and running, please contact us before the course starts so that we can hit the ground running during the workshop.\n\n\n\n\nDay 1\n\nCourse Introduction (20 min)\nGetting Up and Running (60 min) (Lindsay)\nBREAK ☕\nWorking with Code (90 min) (Stuart)\nLUNCH 🍍\nCore Data Structure Concepts (90 min) (Lindsay)\nBREAK 🍩\nGetting Data (90 min) (Lindsay)\n\n\n\nDay 2\n\nData Cleaning (90 min) (Stuart)\nBREAK ☕\nExploring Data with Pandas (90 min) (Stuart)\nLUNCH 🍍\nGraphical Depictions of Data (90 min) (Lindsay)\nBREAK 🍩\nBonus Content: TBD (90 min) (TBD)\n\nFill in this poll to choose our final topic!"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Introduction to Data Science using Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course has been developed and is being given on the traditional territory of the lək̓ʷəŋən speaking peoples, today known as the Esquimalt and Songhees Nations. As this is a data oriented course, we encourage you to check out some of these resources to learn more about these lands, or the lands that you yourself live, work and play on:\n\nFirst Nations in B.C.\nGlobal Territories Map\nMore on the Songhees Nation\n\nParts of the above lesson material are sourced or adapted from Software Carpentry python courses:\n\nPlotting and Programming in Python, Allen Lee (2018)\nProgramming with Python, Azalee Bostroem (2016)\n\nOriginal Work Copyright © Software Carpentry, content modified by the Province of British Columbia.\nThis work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n.\n\n\n\n\nAllen Lee, Sourav Singh, Nathan Moore. 2018. “Software Carpentry: Plotting and Programming in Python.” https://github.com/swcarpentry/python-novice-inflammation.\n\n\nAzalee Bostroem, Valentina Staneva, Trevor Bekolay. 2016. “Software Carpentry: Programming with Python.” Version 2016.06, 10.5281/zenodo.57492. https://github.com/swcarpentry/python-novice-inflammation."
  },
  {
    "objectID": "00_introduction_to_python.html#motivation",
    "href": "00_introduction_to_python.html#motivation",
    "title": "1  Getting Up and Running",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nData has become interwined with the inner workings of nearly every facet of working within the BC Public Service. Whether you have to read an excel spreadsheet, prepare a report based on a survey, comb through csv files to find a specific data source, it is likely that you have worked with a dataset at some point in your career. However, the process of looking at and dealing with data can be messy, error-prone and hard to duplicate. Questions such as ‘wait, how did I get that number again?’ are all too common.\n\n\n\nThe Messy Side of Data\n\n\nThese lessons will teach you how to interact with data in a systematic way using the python ecosystem. By accessing and interpreting data through a set of prescribed methods (developed through the code written), our work with data becomes more accessible, repeatable, and ultimately insightful.\nDuring the course of these lessons, we hope to cover:\n\nPython preliminaries\nExploring and cleaning raw data\nUsing statistical methods\nPlotting results graphically\n\nIf we have time, we may touch on some more advanced python lessons, such as:\n\nPublishing reports\nAccessing the B.C. Data Catalogue\nMachine learning in python"
  },
  {
    "objectID": "00_introduction_to_python.html#before-starting",
    "href": "00_introduction_to_python.html#before-starting",
    "title": "1  Getting Up and Running",
    "section": "1.2 BEFORE STARTING!!",
    "text": "1.2 BEFORE STARTING!!\nSo that we can hit the ground running with this workshop, we are asking that everyone get some basic python tools downloaded and installed before the workshop starts. Tools that we will use include Anaconda (or Miniconda) as well as VSCode. A basic knowledge of the command line/powershell interface will be useful as well, but we will try to keep our use of this to a minimum.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are having issues installing anything we have requested prior to the start of the workshop, please let us know so we can work with you so that we can hit the ground running!\n\n\n\nAnaconda/Miniconda is used to download, install, and organize both python and any packages/libraries we use within python. The actual program doing the organizing is conda, while we will use an anaconda powershell to do the installs and interface with python.\nVSCode is a tool used to write, edit and test code known as an IDE (Integrated Development Environment). It is available for more languages than just python, and its versatility has made it a widespread tool within the BCPS.\n\n\nInstall our Python Tools\nIf you do not have administrative rights on your computer:\nDownload Anaconda and VSCode from the B.C. Government Software Centre:\n\nInstall Anaconda (Anaconda3X64 2022.05 Gen P0)\nInstall VSCode (VSCodeX64 1.55.2 Gen P0)\n\nIf you do have administrative rights on your computer:\n\nIf you have administrative rights on your computer, we suggest downloading the lightweight version of Anaconda called Miniconda.\n\nLink to instructions here!\n\nFind the latest version of VSCode here.\n\n\n\nInstall some Python Packages\nMost of the time, when using python we are not using it by itself, but in conjunction with powerful libraries that have already been built to make our data analysis easier. In order to use these tools, we have to install them as well. Using a package manager such as conda makes our life much easier, as we can safely install tools into local environments where every library installed is checked for compatability with every other library. By utilizing the local conda environments, we maintain clean working spaces that can be easily reproduced between workstations.\nLet’s run through the basic steps of setting up a conda environment, installing python and some packages, and testing that it worked!\n\nOpen an anaconda powershell prompt from your search bar.\n\n\n\n\nPowershell Prompt\n\n\n\nInside the anaconda powershell prompt, create a new local conda environment named ds-env using the following commands (hit Enter or type Y and hit Enter when asked to proceed):\n\n\n\nAnaconda Powershell Prompt\n\n> conda create --name ds-env\n> conda activate ds-env\n\n\n\n\nCreating a Conda Environment\n\n\n\nYou should notice that running this second command switches the name in brackets at the beginning of your prompt from (base) to (ds-env). This means we have successfully created a new, empty environment to work in.\n\nInstall python and some useful datascience packages by typing the following commands into the same powershell prompt window:\n\n\n\nAnaconda Powershell Prompt\n\n> conda install python=3.9\n> conda install notebook jupyterlab ipywidgets matplotlib seaborn numpy scikit-learn pandas openpyxl\n\n\nMake sure that python installed successfully. From the same anaconda powershell prompt, simply type python. If this causes no error, success! Try typing this command in the python environment that started to make sure the packages installed as well:\n\n\nimport pandas\npandas.__version__\n\n\n\n\nTesting the python installation\n\n\n\nIf this all works with no errors, python was successfully installed.\n\n\nSetup our VSCode Environment\nStill with me? Great. Here’s a cute otter as congratulations for making it this far.\n\n\n\nThe cutest.\n\n\n\nWe have just a few more steps to go.\n\nOpen the VSCode program.\nOn the left toolbar, find the extensions tab (It looks like 4 squares). Search for the python extension and install this extension.\nFor those using Windows computers, change your default terminal to the command prompt:\n\nFrom anywhere inside VSCode, hit Ctrl + Shift + P. This will open up the command palette.\nStart typing Terminal: Select Default Profile until this option pops up.\nChoose this option, and then click on Command Prompt\n\n\nThat’s it. We are ready to go!"
  },
  {
    "objectID": "00_introduction_to_python.html#hello-world",
    "href": "00_introduction_to_python.html#hello-world",
    "title": "1  Getting Up and Running",
    "section": "1.3 Hello World",
    "text": "1.3 Hello World\ni.e. the how many different ways can we print Hello World! to our screen? section\n\n\n\nHello World!\n\n\n\nThere are many different ways in which we can interact with python. These include:\n\nFrom the command line\nInside a Jupyter Notebook\nFrom a file (inside VSCode)\n\nIn this next section, we are going to have a brief introduction to all of these methods of interaction.\n\n\n\n\n\n\nTip: Using the command line\n\n\n\nIt’s worth pointing out that the methods that we will focus on in this course will rely on using VSCode or JupyterLab and all of its inner workings. However, if you are comfortable with the command line, we can also access any of these methods directly from there as well, you just need to be able to move to directories before typing commands. If you use the command line, I recommend using an anaconda powershell prompt, as this allows for the easiest use and access to conda commands and only the smallest of headaches.\n\n\n\nStep 0\nIn all cases, we will want to have a folder from which we wish to work out of. Take some time to set up a folder somewhere you won’t lose it. For me, I’ve simply made a folder called Intro to Python on my C: drive that will hold any course materials we use/create here.\nNext, to make any interactions with python, we will want to open VSCode and work from here. When we first open VSCode, you should be prompted to open a folder. We are going to work out of that Intro to Python folder, so open it here. After doing this, we should now have a VSCode screen open that will look something like this:\n\n\n\nVS Code\n\n\n\nWe have 3 main areas that we can utilize:\n\nTo the left (area A): is the current folder and subfolder directory list. We can make new files directly from here.\nTo the right (area B): this is where files we are working on will live. For some file types, preview windows will be available as well.\nTo the bottom (area C): this is where we can open and run commands from the command line (or terminal).\n\nNow remember, we set up a special environment that contains python and our data science packages. We want to make sure we are always using this environment, so in the open terminal, re-type conda activate ds-env and this terminal will now be open in this environment. We also want to check that VSCode itself (and not just the terminal) will use the same environment. We again access the command palette with Ctrl + Shift + P, and begin typing Python: Select Interpreter. Click on this, and choose the ds-env option. We are good to go!\n\n\n\n\n\n\nConda Environments\n\n\n\nAlthough it does add an extra level of set-up whenever we start a python project, having these conda environments ends up being incredibly important for not only reproducibility, but making sure that packages work well together. When in doubt as to if you are using the correct environment, double check that the terminal you are using has (ds-env) in brackets at the start of a line.\n\n\n\n\nFrom the command line/terminal\nLet’s start with an easy one. To start a python session from a terminal, simply type python at the command line, and the terminal will automatically open a python interface. You will know you are inside the python interface if your command lines now start with >>>. Now, let’s do the classic Hello World command for python:\n\nprint('Hello World!')\n\nHello World!\n\n\nTo exit the python interface and return to the regular terminal, you can type exit() and return to the terminal.\n\n\nFrom a file (in VSCode)\nNext up, let’s run an entire python file to tell us hello. Inside our directory, create a new file called hello_world.py. Note that .py extension - this signifies that the content inside will be python code. Inside this file, let’s have two lines of code:\n\n\nhello_world.py\n\nprint(\"Hello World!\")\nprint(\"Otters are the best animal.\")\n\nTo run this code, first save the file, and then simply click the play button (triangle in the top right!). Note that this will display an output in a terminal at the bottom of VSCode. VSCode takes the python file you told it to run, and will run every line of code individually. Thus, we get two lines of output for the two print statements.\nBut wait, there’s more! In VSCode we can run individual lines of code within a file as well. Simply move your cursor to the line you wish to run, and hit Shift+Enter.\nNote the difference here. Instead of running the entire file, VSCode actually opened up a python window inside our terminal, and ran the single line of code, just like we did before.\n\n\n\n\n\n\nChallenge 1\n\n\n\nRun the other line of code, and then add and run a third line of code that prints your favourite TV show.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nPress Shift+Enter while the cursor is on the other line of code.\nAdd a line of code such as:\nprint(\"C'mon son! You know the best show is Psych!\")\nSave the file, and again press Shift+Enter while on this line.\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\nTry clicking the play button again. What happens here? Can you explain why?\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\nAn error will occur in the terminal.\nThis is because VSCode tried to run the entire program again, but from inside an already open python program (which we opened when we ran a single line using the Shift+Enter method).\nTo fix this, we can either exit() the current python execution in the current terminal, or Kill/Delete the open terminal. This button can be found by hovering over the Python symbol at the right hand side of the terminals.\n\n\n\n\n\nFrom JupyterLab\nJupyterLab is an application with a web-based user interface that enables one to work with documents and activities such as Jupyter notebooks, text editors, terminals, and even custom components in a flexible, integrated, and extensible manner.\nTo start JupyterLab, you can use Anaconda Navigator, a GUI that comes packaged with Anaconda if you wish. However it is nearly always easier to access it from the command line. Inside VSCode, navigate to a new terminal.\n\n\n\n\n\n\nConda Reminder!\n\n\n\nMake sure that you double check that this new terminal is opened with the ds-env!\n\n\nFrom the terminal, to launch a new JupyterLab session, simply type:\n\n\nterminal\n\n> jupyter lab\n\nThis should open up a screen that looks something like this:\n\n\n\nJupyterLab\n\n\n\n\n\n\n\n\n\nFun Fact\n\n\n\nYou might have noticed by this point that the author of this section prefers dark mode. So if any of your programs are popping up in a different/lighter colour scheme, that’s okay!\n\n\nWe will be using the JupyterLab interface quite a bit, so let’s get used to the key pieces. Similar to VSCode, we have a few key areas to utilize:\n\nOn the left (area A), we have the sidebar that contains commonly used tabs. These include the file directory system (which defaults to the folder from which the session was launched), a list of open files, running kernels and terminals, a table of contents for any markdown that is written, and possibly a set of extensions.\nOn the right (area B), we have the main work area, where we can open new notebooks, terminals, files, etc. Here, when we have multiple open tabs, we can drag them around the main area to produce smaller panels displaying multiple pieces of work.\n\nGenerally speaking, we will be using Jupyter Notebooks, which have a .ipynb file extension. Like python files, these notebooks can run python code. However, they also support markdown (text that can be added to support source code with explanations) as well as inline viewing of data tables, plots and more. This allows us to mix source code, text and images all in one file that we can quickly use for anything such as:\n\nAnswering ‘how did I get this number in that report?’\nLooking at the number.\nUpdating the number with an updated dataset.\nPlotting the number with other numbers, and then looking at all of those numbers.\n\nLet’s open up a notebook (click the Python 3 icon underneath the Notebook heading). This will open up a new tab called Untitled.ipynb.\n\n\n\n\n\n\nChallenge 3\n\n\n\nRename the notebook to hello_world.ipynb\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nOn either the tab to the top, or the filename that popped up on the left panel, right click the file Untitled.ipynb and click Rename notebook.... Enter your new name here.\n\n\n\nIn this new file, we have a single blue box (this is a single cell). We can type multiple lines of code inside a single cell. Within the blue box, pressing Enter will let you add a new line to the cell. To execute a cell, we press Shift+Enter. This will execute every line of code within that cell. If there is output to display, it will display in the space directly below the cell. Pressing the + at the top of the tab will add a new cell (as will running the bottom-most cell). The ‘active’ cell will always be the one that is highlighted with a blue box.\n\n\n\n\n\n\nChallenge 4\n\n\n\nPrint ‘Hello World’ inside the Jupyter Notebook.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nInside the blue box, write the python code:\nprint('Hello World!')\nWhile still on this cell (highlighted in blue), press Shift+Enter. We should see the output directly below!\n\n\n\nWhen we are done with a JupyterLab session, we must shutdown the server. From the Menu Bar select the File menu and then choose Shut Down at the bottom of the dropdown menu. You will be prompted to confirm that you wish to shutdown the JupyterLab server (don’t forget to save your work!). Click Shut Down to shutdown the JupyterLab server.\n\n\n\n\n\n\nJupyterLab and Us!\n\n\n\nWe will be using JupyterLab for the vast majority of our work throughout this course because of the ease of use in writing and executing code all in one place. Make sure you are comfortable with:\n\nopening JupyterLab to a specific folder\ncreating and renaming notebooks\nadding python code to multiple cells within a notebook\nexecuting entire code blocks\n\nIf you are unsure about any of these pieces, please ask for help!"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#variables-assignment",
    "href": "02_core_data_structure_concepts.html#variables-assignment",
    "title": "2  Core Concepts",
    "section": "2.1 Variables & Assignment",
    "text": "2.1 Variables & Assignment\nWe can assign values to variables in Python that we can use over and over. Variables are always assigned using the format:\n\nvariable_name = 'variable_value'\nfirst_name = 'Loki'\nage = 1054\n\nWhere the name of the variable is always to the left, and whatever value we wish to assign being on the right of =.\nSome rules regarding naming variables:\n\nNames may only contain letters, digits, and underscores\nAre case sensitive\nMust not start with a digit\n\nTypically, variables starting with _ or __ have special meaning, so we will try to stick to starting variables with letters only\n\n\nTo display the value we have previously assigned to a variable, we can use the print function:\n\nprint(first_name, 'is', age, 'Earth years old.')\n\nLoki is 1054 Earth years old.\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nIn the next cell, run the following command:\nprint(last_name)\nWhat happens? Why? How can we fix it?\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nVariables cannot be referenced before they are assigned, so we will run into an error:\n\nprint(last_name)\n\nNameError: name 'last_name' is not defined\n\n\nTo fix this, we simply need to create another cell that assigns this variable, then we can go back and run the print command.\n\nlast_name = 'Odinson'\n\n\nprint(last_name)\n\nOdinson\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\nFill the table below showing the values of the variables in this program after each statement is executed.\n\n\n\nCommand\nValue of x\nValue of y\nValue of swap\n\n\n\n\nx = 1.0\n\n\n\n\n\ny = 3.0\n\n\n\n\n\nswap = x\n\n\n\n\n\nx = y\n\n\n\n\n\ny = swap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\n\n\n\nCommand\nValue of x\nValue of y\nValue of swap\n\n\n\n\nx = 1.0\n1.0\nnot defined\nnot defined\n\n\ny = 3.0\n1.0\n3.0\nnot defined\n\n\nswap = x\n1.0\n3.0\n1.0\n\n\nx = y\n3.0\n3.0\n1.0\n\n\ny = swap\n3.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Notebooks: Order of Execution\n\n\n\nIf you noticed in the last challenge, you could go back to a previous cell above where you assigned a variable, and the print command would work. This is because, in a Jupyter notebook, it is the order of execution of cells that is important, not the order in which they appear. Python will remember all the code that was run previously, including any variables you have defined, irrespective of cell order.\nAfter a long day of work and to prevent confusion, it can be helpful to use the Kernel → Restart & Run All option which clears the interpreter and runs everything from a clean slate going top to bottom."
  },
  {
    "objectID": "02_core_data_structure_concepts.html#lists-indexing",
    "href": "02_core_data_structure_concepts.html#lists-indexing",
    "title": "2  Core Concepts",
    "section": "2.2 Lists & Indexing",
    "text": "2.2 Lists & Indexing\n\n\n\n#sorrynotsorry R\n\n\n\nLists\nAn important aspect of Pythonic programming is the use of indicies to allow us to slice and dice our datasets. We will learn a bit more about indexing here through the introduction of lists. A list is an ordered list of items in Python, where the items can take on any datatype (even another list!). We create a list by putting values inside square brackets and separate items with commas:\n\nmy_list = [1, 'two', 3.0, True]\nprint(my_list)\n\n[1, 'two', 3.0, True]\n\n\n\n\nIndexing\nTo access the elements of a list we use indices, the numbered positions of elements in the list. These positions are numbered starting at 0, so the first element has an index of 0. Python has made it easy to count backwards as well: the last index can be accessed using index -1, the second last with -2 and so on.\n\n\n\n\n\n\n0-Based Indexing!\n\n\n\nIf you have used other coding languages, such as R, you may notice that different programming languages start counting from different numbers. In R, you start your indexing from 1, but in Python it is 0. It’s important to keep this in mind!\n\n\n\nprint('First element:', my_list[0])\nprint('Last element:', my_list[-1])\nprint('Second last element:', my_list[2])\nprint('Also second last element:', my_list[-2])\n\nFirst element: 1\nLast element: True\nSecond last element: 3.0\nAlso second last element: 3.0\n\n\nStrings also have indices, pointing to the character in each string. These work in the same way as lists.\n\nprint(first_name)\nprint(first_name[0])\n\nLoki\nL\n\n\nHowever, there is one important difference between lists and strings: we can change values in a list, but we cannot change individual characters in a string. For example:\n\nprint(my_list)\nmy_list[0] = 'changing the first element!'\nprint(my_list)\n\n[1, 'two', 3.0, True]\n['changing the first element!', 'two', 3.0, True]\n\n\nwill work. However:\n\nprint(first_name)\nfirst_name[0] = 'N'\n\nLoki\n\n\nTypeError: 'str' object does not support item assignment\n\n\nWill throw an error.\n\n\n\n\n\n\nMutable vs Immutable\n\n\n\nData which can be modified in place is called mutable, while data which cannot be modified is called immutable. Strings and numbers are immutable. This does not mean that variables with string or number values are constants, but when we want to change the value of a string or number variable, we can only replace the old value with a completely new value.\nLists and arrays, on the other hand, are mutable: we can modify them after they have been created. We can change individual elements, append new elements, or reorder the whole list. For some operations, like sorting, we can choose whether to use a function that modifies the data in-place or a function that returns a modified copy and leaves the original unchanged.\nBe careful when modifying data in-place. If two variables refer to the same list, and you modify the list value, it will change for both variables!\n\n\nWe can use indicies for more than just accessing single elements from an ordered object such as a list or a string. We can also slice our dataset to give us different portions of the list. We do this using the slice notation [start:stop], where start is the integer index of the first element we want and stop is the integer index of the element just after the last element we want. If either of start or stop is left out, it is assumed that you want to default with either starting from the beginning of the list or ending at the end.\n\nnumber_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint('0:5  --> ', number_list[0:5])\nprint('_:5  --> ', number_list[:5])\nprint('3:7  --> ', number_list[3:5])\nprint('3:_  --> ', number_list[3:])\nprint('3:-1 --> ', number_list[3:-1])\n\n0:5  -->  [0, 1, 2, 3, 4]\n_:5  -->  [0, 1, 2, 3, 4]\n3:7  -->  [3, 4]\n3:_  -->  [3, 4, 5, 6, 7, 8, 9]\n3:-1 -->  [3, 4, 5, 6, 7, 8]\n\n\nWe can also use a step-size to indicate how often we want to pick up an element of the list. By altering the slice notation to [start:stop:step] we will be telling the code to only include those elements at each step after start, ending at the final step that occurs just before running into stop. This allows us to reverse lists as well:\n\nprint('All evens:', number_list[0::2])\nprint('All odds: ', number_list[1::2])\nprint('Just 1 and 4:', number_list[1:5:3])\nprint('Reversed: ', number_list[-1::-1])\n\nAll evens: [0, 2, 4, 6, 8]\nAll odds:  [1, 3, 5, 7, 9]\nJust 1 and 4: [1, 4]\nReversed:  [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\nGiven the following string:\n\nfull_name = 'Peregrin Fool of a Took Pippin'\n\nWhat would these expressions return?\n\nfull_name[2:8]\nfull_name[11:] (without a value after the colon)\nfull_name[:4] (without a value before the colon)\nfull_name[:] (just a colon)\nfull_name[11:-3]\nfull_name[-5:-3]\nWhat happens when you choose a stop value which is out of range? (i.e., try full_name[0:42] or full_name[:103])\n\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\n\n'regrin '\n'ol of a Took Pippin'\n'Pere'\n'Peregrin Fool of a Took Pippin'\n'ol of a Took Pip'\n'ip'\nIf a part of the slice is out of range, the operation does not fail. full_name[:] gives the same result as full_name[0:42], and the same result as full_name[:103].\n\n\n\n\n\n\nDictionaries\nAnother object that exists in Python is the dictionary. It is similar to a list in that it can hold a variety of different types of objects inside of it. However an important difference is in how we access these objects. With a list (or string), we have an ordered arrangement of items that we access with an integer index. However, we access the values in a dictionary with a key, which can be anything we want.\nLet’s build a dictionary, which is denoted in Python with curly {} brackets:\n\nmy_dict = {\n    'first_key': 'some value',\n    'A': ['a', 'differerent', 'type', 'of', 'object'],\n    2: False\n}\n\nprint(my_dict)\n\n{'first_key': 'some value', 'A': ['a', 'differerent', 'type', 'of', 'object'], 2: False}\n\n\nHere we listed three key - value pairs. The key comes before the value, with a colon between. Commas separate different pairs. Now that we have a dictionary, we access it the same way as a list, with square [] brackets:\n\nprint(my_dict['first_key'])\nprint()\nprint(my_dict['A'])\nprint()\nprint(my_dict[2])\n\nsome value\n\n['a', 'differerent', 'type', 'of', 'object']\n\nFalse\n\n\nUnlike a list, dictionaries are unordered, and so we cannot perform integer indexing or slicing of these elements:\n\nmy_dict[0]\n\nKeyError: 0\n\n\n\nmy_dict[0:5]\n\nTypeError: unhashable type: 'slice'\n\n\nDictionaries are an abstract data type that can take a while to get used to! They can be a powerful tool in Python. Common uses for dictionaries include:\n\nCreating searchable parameter lists for models\nSupplying extra arguments to functions\nStoring complex outputs or datasets\n\nWe will not need to use dictionaries frequently in this course. However, they will become useful when we learn more about data tables and aggregation methods later on, and so gaining familiarity now is beneficial!"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#data-types-operations",
    "href": "02_core_data_structure_concepts.html#data-types-operations",
    "title": "2  Core Concepts",
    "section": "2.3 Data Types & Operations",
    "text": "2.3 Data Types & Operations\nEvery value in a program has a specific type. In this course, you will run across four basic types in Python:\n\nInteger (int): positive or negative whole numbers like 42 or 90210\nFloating point numbers (float): real fractional numbers like 3.14159 or -87.6\nCharacter strings (str): text written either in single or double quotes.\nBoolean (bool): the logical values of True and False\n\nIf you are unsure what type anything is, we can use the built in function type. Note that this works on variables as well.\n\nprint(type(42))\nprint(type(3.14))\nprint(type('Otter'))\nprint(type(True))\n\n<class 'int'>\n<class 'float'>\n<class 'str'>\n<class 'bool'>\n\n\n\n\n\n\n\n\nMessy Numbers\n\n\n\nWhen you start to have really long integers, it starts to look really messy (how many thousands are in 1982137092 at a glance?) Luckily, Python allows us to use _ inside our integers to space out our digits. Thus we could write that instead as 1_902_137_092. Isn’t that nicer!\n\n\n\nBasic Arithmetic\nThe type of a variable controls what operations can be performed on it. For example, we can subtract floats and ints, but we cannot subtract strings:\n\nprint(42-12)\nprint(3.14-15)\nprint('hello' - 'h')\n\n30\n-11.86\n\n\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n\n\nHowever, we can add strings together:\n\nmy_sentence = 'Adding' + ' ' + 'strings' + ' ' + 'concatenates them.'\nprint(my_sentence)\n\nAdding strings concatenates them.\n\n\nAs well as multipling a string by an integer to get a repeated string:\n\nrepeated_string = '=+'*10\nprint(repeated_string)\n\n=+=+=+=+=+=+=+=+=+=+\n\n\nAs we saw above, we can mix and match both of the numerical types, however we will get an error if we try to mix a string with a number:\n\nprint(1 + '2')\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\nIn order to have a sensical operation, we need to convert variables to a common type before doing our operation. We can convert variables using the type name as a function.\n\nprint(1 + int('2'))\nprint(str(1) + '2')\n\n3\n12\n\n\n\n\n\n\n\n\nInt of a Float?\n\n\n\nNote that when converting floats to integers, it will always round down! That is, int(3.6) = int(3.1) = 3!\n\n\nOne last bit of math you might come across are the various types of division:\n\n/ performs regular floating-point division\n// performs integer floor division\n% returns the remainder of integer division\n\n\nprint('5 / 3 :', 5 / 3)\nprint('5 // 3:', 5 // 3)\nprint('5 % 3 :', 5 % 3)\n\n5 / 3 : 1.6666666666666667\n5 // 3: 1\n5 % 3 : 2\n\n\n\n\nBuilt in Functions\nPython has multiple pre-built functions that come in handy. We have already made use of the print() command frequently, and learned how to use type() to tell us what type of data our variables are. Here are some other frequently used functions:\n\nlen() : Tells us the length of a list, string, or other ordered object. Does not work on numbers!\nhelp() : Gives help for other functions\nmin() : Gives the mininum value in a list of options.\nmax() : Gives the maximum value in a list of options.\nround(): Rounds a value to a given decimal length.\n\nNote that, similar to the arithmetic operations above, these built in functions must operate on logically consistent datatypes. We can find the min of 2 strings, or 4 numbers, but we cannot compare a string to a float.\nEvery function in python will take 0 or more arguments that are passed to a function. For example, len() takes exactly one argument, and returns the length of that argument:\n\n print(len('this string is how long?'))\n\n24\n\n\nSome functions, such as min() and max() take a variable number of arguments:\n\n print(min(1,2,3,4))\n print(max('a', 'b', 'c'))\n\n1\nc\n\n\nWhile others have default values that do not need to be provided at all.\n\n\n\n\n\n\nChallenge 4\n\n\n\nUse the help() and round() functions to print out the value of 2.71828182845904523536 to 0 and 2 decimal places.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nThe result of help() tells us that round() has a default option:\n\nhelp(round)\n\nHelp on built-in function round in module builtins:\n\nround(number, ndigits=None)\n    Round a number to a given precision in decimal digits.\n    \n    The return value is an integer if ndigits is omitted or None.  Otherwise\n    the return value has the same type as the number.  ndigits may be negative.\n\n\n\nThus, we can use the round() function with and without the default ndigits to get our answer:\n\neulers_number = 2.71828182845904523536\nprint(round(eulers_number))\nprint(round(eulers_number, 2))\n\n3\n2.72\n\n\n\n\n\n\n\n\n\n\n\nHelp in Jupyter Lab!\n\n\n\nIn Jupyter notebooks, we can also get help by starting a line with ?. For example, ?round will display the help information about the round() function.\n\n\n\n\nA Quick Intro to Boolean Logic\nWe can ask Python to take different actions, depending on a condition, with the if statement:\n\nnum = 37\nif num > 100:\n    print('greater')\nelse:\n    print('not greater')\nprint('done')\n\nnot greater\ndone\n\n\nThe if keyword tells Python we want to make a choice. We then use : to end the conditional we would like to consider, and indentation to specify our if block of code that should execute if the condition is met. If the condition is not met, the body of the else block gets executed instead.\nIn either case, ‘done’ will always print as it is in neither indented block.\n\n\n\nFollowing a Logical Flow\n\n\nConditional statements do not need to include an else block. If there is no block and the condition is False, Python simply does nothing:\n\nnum = 37\nif num > 100:\n    print('greater')\nprint('done')\n\ndone\n\n\nWe can also chain several tests together using elif. Python will go through the code line by line, looking for a condition that is met. If no condition is met, it will execute the else block (or nothing if there is no else).\n\nnum = 45\nif num < 42:\n    print('This is not the answer.')\nelif num > 42:\n    print('This is also not the answer.')\nelse:\n    print('This is the answer to life, the universe, and everything.')\n\nThis is also not the answer.\n\n\nThere are multiple different comparisons we can make in Python:\n\n>: greater than\n<: less than\n==: equal to (note the double ‘=’ here!)\n!=: does not equal\n>=: greater than or equal to\n<=: less than or equal to\n\nAnd these can be used in conjunction with each other using the special keywords and, or, and not. and will evaluate to True if both parts are True, while or will evaluate to True if either side is. not will evaluate the condition, and then return the opposite result.\n\ncondition_1 =  1 > 0  # True\ncondition_2 = -1 > 0  # False\n\nprint('Testing and: ')\nif condition_1 and condition_2:\n    print('both parts are true')\nelse:\n    print('at least one part is false')\n\nprint()\nprint('Testing or: ')\nif condition_1 or condition_2:\n    print('at least one part is true')\nelse:\n    print('both parts are false')\n\nprint()\nprint('Testing not: ')\nif not condition_1:\n    print('condition_1 was false')\nelse:\n    print('condition_1 was true')\n\nTesting and: \nat least one part is false\n\nTesting or: \nat least one part is true\n\nTesting not: \ncondition_1 was true\n\n\n\n\n\n\n\n\nMultiple Conditions Cause Confusion\n\n\n\nJust like with arithmetic, you can and should use parentheses whenever there is possible ambiguity. A good general rule is to always use parentheses when mixing and and or in the same condition.\n\n\n\n\n\n\n\n\nChallenge 5\n\n\n\nWhat will be the output of the following block of code?\nnum = 42 \nanimal = 'otter'\n\nif num==42 and animal=='mouse':\n    print('Correct, that is the animal that found the answer')\nelif num==42 and animal!='mouse':\n    print('Almost, the number is correct but not the animal.')\n    animal = 'dolphin'\nelif num!=42 or animal=='mouse':\n    print('Almost, the animal is correct but not the number.')\n    num = 5\nelif (1>3) or (4>3):\n    print('This has nothing to do with it, we just needed an or statement')\nelse:\n    print('Not even close, those pesky mice need to work harder.')\n    num = 19\n    animal = 'kangaroo'\n\nprint('The end result is the number', number, 'and animal', animal)\n\n\n\n\n\n\n\n\nSolution to Challenge 5\n\n\n\n\n\n\n\nAlmost, the number is correct but not the animal.\nThe end result is the number 42 and animal dolphin\n\n\n\n\n\n\n\n\n\n\n\nElement-Wise Logic\n\n\n\nBefore we move on from our foray into boolean logic, let us make a brief mention of the &, |, and ~ symbols. These are similar, but not identical, to and, or and not. Where and is used for boolean logic on scalars, & is used for boolean logic on vectors, and will do an element-by-element comparison. This will be important when we introduce data structures later on!"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#methods-chaining",
    "href": "02_core_data_structure_concepts.html#methods-chaining",
    "title": "2  Core Concepts",
    "section": "2.4 Methods & Chaining",
    "text": "2.4 Methods & Chaining\n\n\n\nObject Oriented Programming\n\n\nSo far we have seen built in functions that can be applied to a variety of different datatypes (as long as the datatype makes sense for that particular function). However, there are some functions that we apply specifically to a particular class of objects - we call these functions methods. Methods have parentheses, just like functions, but they come after the variable to denote that the method belongs to this particular object.\nWe have met classes already: all of our basic datatypes (strings, integers, floats, booleans) are different classes of objects in Python. An individual instance of a class is considered an object of that class. Understanding how to use methods will become useful when we reach the pandas portion of the course, which is our main tool when looking at, cleaning, and summarizing data.\nLet’s consider the string class. Here are a few common methods associated with it:\n\nlower(): coverts all characters in the string to lowercase\nupper(): converts all characters in the string to uppercase\nindex(): returns the position of the first occurrence of a substring in a string\nrjust(): right aligns the string according to the width specified\nisnumeric(): returns True if all characters are numeric\nreplace(): replaces all occurrences of a substring with another substring\n\n\n\n\n\n\n\nGetting Help For Methods\n\n\n\nYou will notice that trying to find help on a method will not work if you only specify the method. Because these are not built in functions, and only belong to instances of a class, you need to specify the object together with the method to use help.\nFor example, help(lower) will result in an error, whereas help(\"any string\".lower) will give you the help you were looking for.\n\n\nLet’s see some of these in action. You’ll notice that when being used, methods don’t always have an argument supplied to them. That is because the first argument is always the object is being applied to. If a method requires secondary arguments, these are subsequently included in the parentheses.\n\nobject.method(a, b, c, ...) ↔︎ method(object, a, b, c, ...)\n\n\nmy_string = 'Peter Piper Picked a Peck of Pickled Peppers'\nprint(my_string.lower())\nprint(my_string.isnumeric())\n\npeter piper picked a peck of pickled peppers\nFalse\n\n\nWe can also chain methods together. Each subsequent method (reading from left to right) acts on the output of the previous method. Chaining can be done in a single line, or over multiple lines (which helps for readability).\n\nprint(my_string.upper().replace('P', 'M'))\n\n# Chaining over multiple lines can be done in 2 ways:\n# 1. Enclose the entire operation in brackets\nchain_1 = (my_string\n    .upper()\n    .replace('P', 'M')\n)\n\n# 2. Use the character \"\\\" to denote an operation is continuing on the next line\nchain_2 = my_string \\\n    .upper() \\\n    .replace('P', 'M')\n\nprint(chain_1)\nprint(chain_2)\n\nMETER MIMER MICKED A MECK OF MICKLED MEMMERS\nMETER MIMER MICKED A MECK OF MICKLED MEMMERS\nMETER MIMER MICKED A MECK OF MICKLED MEMMERS\n\n\n\n\n\n\n\n\nChallenge 6\n\n\n\nWhat happens if we try to run the block of code:\nprint(my_string.isnumeric().upper())\n\n\n\n\n\n\n\n\nSolution to Challenge 6\n\n\n\n\n\nWe will get an error:\n\nprint(my_string.isnumeric().upper())\n\nAttributeError: 'bool' object has no attribute 'upper'\n\n\nThis is because the code is read from left to right. In this case, the output of isnumeric() is a boolean object, which does not have the method upper() anymore:\n\nprint(my_string.isnumeric())\nprint(type(my_string.isnumeric()))\n\nFalse\n<class 'bool'>\n\n\n\n\n\n\n\n\n\n\n\nChallenge 7\n\n\n\nA common data string you see across government are various personal IDs. One example is the Personal Education Number (PEN), which is a nine-digit number assigned to you when you enter the K-12 School System. Oftentimes when looking at such an ID, any leading zeros that are an actual part of the PEN get stripped away, leading to unevenly sized strings of IDs across the dataset.\nWrite a piece of code for PEN IDs that does the following:\n\nChecks to make sure that the ID is entirely numeric. If it is not, print out a warning that this is an invalid PEN.\nIf the PEN is numeric, make sure that it is less than or equal to 9 digits long. If it is longer, print out a warning that this is an invalid PEN.\nIf the PEN is too short, make sure to pad it with the appropriate number of 0’s to the left. Print out the correct PEN.\n\nTry your code out with the following PENs:\npen_1 = '12345678x'\npen_2 = '123456789'\npen_3 = '1234567890'\npen_4 = 123456789\npen_5 = '123456'\n\n\n\n\n\n\n\n\nSolution to Challenge 7\n\n\n\n\n\n\npen_1 = '123456'\n\n# first! make sure we are looking at strings so we can use the string method!\npen = str(pen_1)\n\n# first check for numerical:\nif not pen.isnumeric():\n    print('Warning! This PEN has non-numeric characters.')\n\n# second, check that it isn't too long\nelif len(pen)>9:\n    print('Warning! This PEN is longer than 9 digits.')\n\n# third: make sure that we pad it to the correct length\nelse:\n    pen = pen.rjust(9, '0')\n    print('This PEN is valid:', pen)\n\nThis PEN is valid: 000123456"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#accessing-other-packages",
    "href": "02_core_data_structure_concepts.html#accessing-other-packages",
    "title": "2  Core Concepts",
    "section": "2.5 Accessing Other Packages",
    "text": "2.5 Accessing Other Packages\n\n\n\nImport Packages\n\n\nMost of the power of Python lies in its ability to use libraries, or external packages that are not part of the base Python programming language. These libraries have been written and maintained by other members of the Python community, and will make data cleaning, manipulation, visualization and any other data project much simpler. Throughout this course we will use packages such as:\n\npandas: this is the go-to package for all things data-table.\nmatplotlib: this is the most frequently used plotting package in Python\nseaborn: this is a plotting package built with pandas and data in mind\n\nWhen we set up our Python environment, we already installed many of the packages we will need directly into the conda environment we produced. If you ever need another package, it is simple enough to install again using conda:\n\n\nAnaconda Powershell Prompt\n\n> conda activate ds-env\n> conda install <package>\n\n\n\n\n\n\n\nTo Pip or not to Pip?\n\n\n\nIf you are ever searching for a package you think will aid you in your work, you might come across the pip command. This is a different (yet related) method of installing packages. While it is possible to use pip in tandem with conda commands, it is recommended that you stick to only conda wherever possible.\nAs a rule of thumb, try to conda install package as a first try. If this does not work, search the website for the package for installation instructions. Sometimes it will recommend using a different conda channel (and will provide the code to do so). Sometimes, it is only possible to get the package from pip, in which case using pip inside the conda environment is the only way to go. Just use this as a last resort!\n\n\nOkay great, we have all these awesome libraries that have been built out by others. How do we actually use them? In Python, it is actually fairly simple!\nOption 1: Use import to load an entire library module into a program’s memory. Refer to things from the module as module_name.thing_name\n\n import math\n\n print('pi is', math.pi)\n print('cos(pi) is', math.cos(math.pi))\n\npi is 3.141592653589793\ncos(pi) is -1.0\n\n\nOption 2: If we only need a specific function or tool from the library, use from module import thing\n\nfrom math import cos, pi\n\nprint('cos(pi) is', cos(pi))\n\ncos(pi) is -1.0\n\n\nOption 3: If we really do need the entire library, but we do not want to type the entire long name over and over, create an alias\n\nimport math as m\n\nprint('cos(pi) is', m.cos(m.pi))\n\ncos(pi) is -1.0\n\n\nSome common alias for common libraries include:\n\npandas → pd\nmatplotlib.pyplot → plt\nseaborn → sns\nnumpy → np"
  },
  {
    "objectID": "21_getting_data_with_pandas.html#introduction-to-pandas",
    "href": "21_getting_data_with_pandas.html#introduction-to-pandas",
    "title": "3  Getting Data With Pandas",
    "section": "3.1 Introduction to Pandas",
    "text": "3.1 Introduction to Pandas\nPandas is one of the most widely-used Python libraries for statistics, and excels at processing tabular data. If you have ever had any experience with R, it is modeled extensively on the dataframes used there. In this section we aim to learn about the fundamental objects used in pandas, bring our own data into these objects, and then view basic information about it.\n\n\n\n\n\n\nDid You Know?\n\n\n\nDid you know that the pandas library package is aptly named? It is a portmanteau of the words panel and data. We are literally viewing panels of data!\n\n\nFirst, we need to make sure that we import the library into Python so we can use it:\n\nimport pandas as pd\n\nObjects in pandas are typically two-dimensional tables called dataframes. You can think of this as being similar to a single spreadsheet in excel. Each column (called series) in the dataframe can have a name, as well as an indexing value.\n\n\n\nA pandas dataframe\n\n\nWe can create both single series in pandas or full dataframes. Let’s consider a single series first. We can create this using the pandas Series method, as well as a list of elements we wish to include in our series.\n\nseries_x = pd.Series([1, 2, 3])\nseries_x\n\n0    1\n1    2\n2    3\ndtype: int64\n\n\nWhen this displays in python, we see 2 columns. The left column is always the index, while the right column contains the data values. The series is also given a dtype. This is similar to the datatypes we considered previously, and tells us what type of data is held inside the series. When a series is first created, pandas will try to guess the dtype for the series based on the individual elements inside it. We can make a series made of float numbers:\n\nseries_y = pd.Series([1, 2.0, 3.14])\nseries_y\n\n0    1.00\n1    2.00\n2    3.14\ndtype: float64\n\n\nNotice that here, the first element was actually converted from an int to a float by the program! This will often happen when working with int and float type objects.\nNext, a series of str elements will default to the dtype of object.\n\nseries_z = pd.Series(['settlers', 'of', 'catan'])\nseries_z\n\n0    settlers\n1          of\n2       catan\ndtype: object\n\n\nThe dtype: object is pandas ‘catchall’ Series type, and we want to be cautious when we see this! It is also the Series dtype used when we have mixed data:\n\nseries_mixed = pd.Series([1, 1.0, 'one'])\nseries_mixed\n\n0      1\n1    1.0\n2    one\ndtype: object\n\n\nWhen pandas sees data that cannot be obviously converted into a single datatype, it leaves each individual entry alone, as whatever its original datatype happened to be. Be careful when you see this! Many operations on pandas dataframes apply to the entire column (similar to how a new excel column is often created with a function that applies to a previous column). If this operation is built to only work on a single datatype, we might run into errors! To avoid this, we can utilize the .astype() methods. Possible arguments for astype() include the standard datatypes:\n\nint\nfloat\nstr\nbool\n\nHowever if you need to be more specific, you can be as well. These arguments are all required to be included in quotations, as they refer to aliases for pandas specific datatypes:\n\n'Int8', 'Int32', 'Int64' provide nullable integer types (note the capital I! more on nullable typing below)\n'string' provides access to a series that will be string specific, instead of the general objects type. Pandas recommends using this type for strings whenever possible.\n\n\ndisplay(series_y)\ndisplay(series_y.astype(int))\ndisplay(series_y.astype(bool))\ndisplay(series_y.astype(str))\ndisplay(series_y.astype('string'))\n\n0    1.00\n1    2.00\n2    3.14\ndtype: float64\n\n\n0    1\n1    2\n2    3\ndtype: int64\n\n\n0    True\n1    True\n2    True\ndtype: bool\n\n\n0     1.0\n1     2.0\n2    3.14\ndtype: object\n\n\n0     1.0\n1     2.0\n2    3.14\ndtype: string\n\n\nYou will have noticed that each of these series has an index associated with it. We can access series indicies using the index attribute:\n\nseries_a = pd.Series([2, 4, 6])\ndisplay(series_a)\ndisplay(series_a.index)\n\n0    2\n1    4\n2    6\ndtype: int64\n\n\nRangeIndex(start=0, stop=3, step=1)\n\n\nWhen an index is first assigned to a series, it is automatically assigned as an integer index, with similar properties to a list index (starts at 0, can be sliced, etc.). However we can change this index to be whatever we want by directly modifying the index attribute:\n\nseries_a.index = ['a', 'b', 'c']\ndisplay(series_a.index)\ndisplay(series_a)\n\nIndex(['a', 'b', 'c'], dtype='object')\n\n\na    2\nb    4\nc    6\ndtype: int64\n\n\nA useful feature in pandas is the ability to reindex the dataset. Reindexing a dataset will do two things:\n\nReorder the data according to the order we ask for.\nAdd new rows for indicies that are missing in the original dataset but are included in our new index.\n\nOne popular use for this method is in filling out a timeseries dataset: if there were missing years in a dataset but we do not wish to simply skip over them, we can add the extra years to the index.\n\nseries_a = series_a.reindex(['d', 'c', 'b', 'a'])\ndisplay(series_a.index)\ndisplay(series_a)\n\nIndex(['d', 'c', 'b', 'a'], dtype='object')\n\n\nd    NaN\nc    6.0\nb    4.0\na    2.0\ndtype: float64\n\n\n\n\n\n\n\n\nBeware NaN values!\n\n\n\nSometimes in our datasets, we want to allow a row to contain missing, or null values. The default null value for pandas is NaN. In python, NaN is considered a float value. In the above example, we introduced a missing value for the new ‘d’ index, which defaults to the NaN float value. Because its type is float, this converted our entire series to float as well. If we wish to keep the series as an int, we can coerce it back using the astype() method with one of the nullable integer types. This will introduce a slightly different looking null value that works for integers!\n\nseries_a.astype('Int64')\n\nd    <NA>\nc       6\nb       4\na       2\ndtype: Int64\n\n\n\n\nWe commonly want to make new series out of old ones inside our dataframes. Operations are typically done on an element by element basis. We will see many examples of these in future sessions as we learn to manipulate dataframes, but here is a short example of what we may wish to do:\n\nseries_a = pd.Series([2, 4, 6])\nseries_b = pd.Series([1, 2, 3])\n\ndisplay(series_a + series_b)\ndisplay(series_a > 3)\ndisplay(series_a*5)\n\n0    3\n1    6\n2    9\ndtype: int64\n\n\n0    False\n1     True\n2     True\ndtype: bool\n\n\n0    10\n1    20\n2    30\ndtype: int64"
  },
  {
    "objectID": "21_getting_data_with_pandas.html#bringing-in-our-own-data",
    "href": "21_getting_data_with_pandas.html#bringing-in-our-own-data",
    "title": "3  Getting Data With Pandas",
    "section": "3.2 Bringing in our own data",
    "text": "3.2 Bringing in our own data\nEnough about series. Let’s talk dataframes! This is the main tool in our pandas toolkit. As we showed earlier, it is simply a collection of series all stacked together like an excel spreadsheet. There are many different ways to create a dataframe including:\n\nWithin python itself\nFrom a csv, excel file, or some other local tabular format\nFrom more exotic data sources, such as a parquet file, json, or a website\nFrom a SQL database\n\nFor the purposes of this course, we are going to focus on opening up local datafiles (with the most common type being a csv or excel file), and then utilizing the data once it is in python. To bring in data from a csv or excel file, we utilize the pandas methods read_csv() or read_excel(), with the only required argument being the path to the datafile.\nBut first we need some data! Navigate to this URL, right click on the data, and save the csv as gapfinder.csv in a folder called data inside our project folder. Now that we have a dataset, let’s load it into pandas.\n\n\n\n\n\n\nThe Data Folder\n\n\n\nWhile everyone may organize their projects and folders slightly differently, there are some general principles to adhere to that make project management easier. Arguably the most important of these is to treat input data as read only. If you have an excel spreadsheet, it is tempting to go make changes to the data directly in the spreadsheet: I’ll just tweak a single value here, or add a column there. However, once we start doing this we lose the concept of reproducibility! How did we get certain values if the changes are all hidden in the excel spreadsheet? By keeping data as read only and making all of our changes in the python scripts, the processing is not only reproducible but also transparent to others.\nBecause of this, it is common to have a folder exclusively for raw data. Secondary folders may then be set-up for ‘cleaned’ data, python scripts, analysis outputs and more.\n\n\n\ndf = pd.read_csv('../data/gapfinder.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1699\n      Zimbabwe\n      1987\n      9216418.0\n      Africa\n      62.351\n      706.157306\n    \n    \n      1700\n      Zimbabwe\n      1992\n      10704340.0\n      Africa\n      60.377\n      693.420786\n    \n    \n      1701\n      Zimbabwe\n      1997\n      11404948.0\n      Africa\n      46.809\n      792.449960\n    \n    \n      1702\n      Zimbabwe\n      2002\n      11926563.0\n      Africa\n      39.989\n      672.038623\n    \n    \n      1703\n      Zimbabwe\n      2007\n      12311143.0\n      Africa\n      43.487\n      469.709298\n    \n  \n\n1704 rows × 6 columns\n\n\n\n\n\n\n\n\n\n.. ?\n\n\n\nThe .. used in the above path tells python to look in the directory directly above the one you are currently in - the one where your notebook is saved.\nAs an example, my directory structure looks like:\n/ds_intro_to_python\n│\n└───/introduction_to_python\n│   │   hello_world.ipynb\n│   │   hello_world.py\n│   \n└───/introduction_to_pandas\n│   │   intro_to_pandas.ipynb  <------ (THIS FILE!)\n│\n└───/data\n│   │   gapfinder.csv <--------------- (The file we want access to!)\nThis file is in a different subfolder relative to the csv, so we first ‘back out’ of this folder using .., and then ‘enter’ the data folder using a regular file path. This is called relative pathing and can be useful for accessing data within a single project that will always be in the same spot!\n\n\nThis dataset has 6 columns, 1704 rows, and a mixture of different datatypes. Just like we were able to access the index of a series, we can do the same with a dataframe. Now, we can also access (and change if need be!) the column names as well:\n\ndisplay(df.index)\ndisplay(df.columns)\n\ndf.columns = ['country', 'year', 'pop', 'continent', 'life expectancy', 'gdpPercap']\ndisplay(df.columns)\n\nRangeIndex(start=0, stop=1704, step=1)\n\n\nIndex(['country', 'year', 'pop', 'continent', 'lifeExp', 'gdpPercap'], dtype='object')\n\n\nIndex(['country', 'year', 'pop', 'continent', 'life expectancy', 'gdpPercap'], dtype='object')\n\n\nWe can access each of these series individually if we want. There are two ways to access a series in a dataframe - either with square bracket indexing, or treating the column name as an attribute:\n\ndisplay(df['country'])\ndisplay(df.country)\n\n0       Afghanistan\n1       Afghanistan\n2       Afghanistan\n3       Afghanistan\n4       Afghanistan\n           ...     \n1699       Zimbabwe\n1700       Zimbabwe\n1701       Zimbabwe\n1702       Zimbabwe\n1703       Zimbabwe\nName: country, Length: 1704, dtype: object\n\n\n0       Afghanistan\n1       Afghanistan\n2       Afghanistan\n3       Afghanistan\n4       Afghanistan\n           ...     \n1699       Zimbabwe\n1700       Zimbabwe\n1701       Zimbabwe\n1702       Zimbabwe\n1703       Zimbabwe\nName: country, Length: 1704, dtype: object\n\n\n\n\n\n\n\n\nNo Spaces!\n\n\n\n\n\n\n\n\nAs a general rule of thumb, we never want to include special characters such as spaces, periods, hyphens and so on in our column names, as this will alter pandas capability of calling each of the columns as an attribute. Above, you will notice we reset one of the columns to have a space in the name. If we try to access this series as an attribute now, it will fail:\n\ndf['life expectancy']\n\n0       28.801\n1       30.332\n2       31.997\n3       34.020\n4       36.088\n         ...  \n1699    62.351\n1700    60.377\n1701    46.809\n1702    39.989\n1703    43.487\nName: life expectancy, Length: 1704, dtype: float64\n\n\n\ndf.life expectancy\n\nSyntaxError: invalid syntax (41671440.py, line 1)\n\n\nTry to stick to the same naming conventions for columns as for your python variables: lowercase letters, numbers (but not at the start of the name) and underscores only! (And as a matter of fact, let us change it back now):\n\ndf.columns = ['country', 'year', 'pop', 'continent', 'lifeExp', 'gdpPercap']\n\n\n\nWhen reading data into a dataframe from a csv (or an excel file), there are multiple optional arguments we can use to start the process of data wrangling, which is writing code to shape the data into the format we want it for our analysis. Some important options include:\n\nheader: row number to use as the column names. This allows us to skip past rows in the dataset and start from lower down if need be.\nindex_col: name of the column we might wish to use for the index of the dataframe instead of the default integer list.\nusecols: list of columns we wish to use. If the dataset is large with many columns that we do not care about, we can pull in only those of interest!\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nBring the dataset into pandas again. This time:\n\nuse the country as the index\nonly include the year, continent and population columns.\n\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\n\ndf_challenge = pd.read_csv(\n    '../data/gapfinder.csv',\n    index_col = 'country',\n    usecols = ['country', 'continent', 'year', 'pop']\n)\ndisplay(df_challenge)\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      continent\n    \n    \n      country\n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      1952\n      8425333.0\n      Asia\n    \n    \n      Afghanistan\n      1957\n      9240934.0\n      Asia\n    \n    \n      Afghanistan\n      1962\n      10267083.0\n      Asia\n    \n    \n      Afghanistan\n      1967\n      11537966.0\n      Asia\n    \n    \n      Afghanistan\n      1972\n      13079460.0\n      Asia\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      Zimbabwe\n      1987\n      9216418.0\n      Africa\n    \n    \n      Zimbabwe\n      1992\n      10704340.0\n      Africa\n    \n    \n      Zimbabwe\n      1997\n      11404948.0\n      Africa\n    \n    \n      Zimbabwe\n      2002\n      11926563.0\n      Africa\n    \n    \n      Zimbabwe\n      2007\n      12311143.0\n      Africa\n    \n  \n\n1704 rows × 3 columns\n\n\n\nNotice that, because we specified an index, the index now has a name! You can access the name of the index via df_challenge.index.name:\n\ndf_challenge.index.name\n\n'country'"
  },
  {
    "objectID": "21_getting_data_with_pandas.html#learning-about-our-data",
    "href": "21_getting_data_with_pandas.html#learning-about-our-data",
    "title": "3  Getting Data With Pandas",
    "section": "3.3 Learning about our data",
    "text": "3.3 Learning about our data\nOkay, so now we have a dataset. Great! Now what can we do with it? In the next few sessions, we will explore in detail some of the more in-depth tools that pandas gives us. For now, let’s stick to learning how to view different portions of the data, as well as learning how to describe the overall dataset.\nWhen viewing data, we do not want to be scrolling past multiple lines of individual rows of the data. This might be a shift in mindset if you are used to working with tables of data directly in front of you! An excel spreadsheet just has all the data right there for you to look at! Why not do that here? The simple answer is magnitude. If you only have 10s to 100s of rows of data, seeing it visually is okay. But once you start to deal with thousands, millions or even trillions of rows of data, it’s going to take a while to scroll through the entire thing. At this stage, the important piece of information is how we are treating the data we see, not the actual values.\nTypically, we just want to view a small slice of the data to get an understanding of the types of data we have in our dataset. We have three tools in the toolkit for this:\n\nhead(): This returns the first N rows of data (default N = 5)\ntail(): This returns the last N rows of data (default N = 5)\nsample(): This returns a random sampling of N rows of data (default N = 1)\n\n\nprint('The first 5 rows of data:')\ndisplay(df.head())\n\nprint('The last 3 rows of data:')\ndisplay(df.tail(3))\n\nprint('A random sampling of 7 rows of data:')\ndisplay(df.sample(7))\n\nThe first 5 rows of data:\n\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n  \n\n\n\n\nThe last 3 rows of data:\n\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      1701\n      Zimbabwe\n      1997\n      11404948.0\n      Africa\n      46.809\n      792.449960\n    \n    \n      1702\n      Zimbabwe\n      2002\n      11926563.0\n      Africa\n      39.989\n      672.038623\n    \n    \n      1703\n      Zimbabwe\n      2007\n      12311143.0\n      Africa\n      43.487\n      469.709298\n    \n  \n\n\n\n\nA random sampling of 7 rows of data:\n\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      1470\n      Sweden\n      1982\n      8325260.0\n      Europe\n      76.420\n      20667.381250\n    \n    \n      206\n      Burundi\n      1962\n      2961915.0\n      Africa\n      42.045\n      355.203227\n    \n    \n      565\n      Germany\n      1957\n      71019069.0\n      Europe\n      69.100\n      10187.826650\n    \n    \n      1393\n      Somalia\n      1957\n      2780415.0\n      Africa\n      34.977\n      1258.147413\n    \n    \n      1191\n      Paraguay\n      1967\n      2287985.0\n      Americas\n      64.951\n      2299.376311\n    \n    \n      1620\n      Uruguay\n      1952\n      2252965.0\n      Americas\n      66.071\n      5716.766744\n    \n    \n      1580\n      Turkey\n      1992\n      58179144.0\n      Europe\n      66.146\n      5678.348271\n    \n  \n\n\n\n\nOnce we have looked at the data, and it seems to look normal at first glance, we can ask some basic questions about the dataset. How many columns are there? How many rows? Are there null values in any of our columns? What about some basic statistics??\nLuckily for us, pandas has done all of the hard work here. Two valuable methods built into pandas will give us basic information about the overall dataset: .info() and .describe().\n.info() will gives us basic information about each column: what data type it is storing and how many non-null values are in the column.\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1704 entries, 0 to 1703\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   country    1704 non-null   object \n 1   year       1704 non-null   int64  \n 2   pop        1704 non-null   float64\n 3   continent  1704 non-null   object \n 4   lifeExp    1704 non-null   float64\n 5   gdpPercap  1704 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 80.0+ KB\n\n\n.describe() will give us basic statistical information about every numerical column: mean, standard deviation, quartiles, and counts are all included with a call to a single method!\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      count\n      1704.00000\n      1.704000e+03\n      1704.000000\n      1704.000000\n    \n    \n      mean\n      1979.50000\n      2.960121e+07\n      59.474439\n      7215.327081\n    \n    \n      std\n      17.26533\n      1.061579e+08\n      12.917107\n      9857.454543\n    \n    \n      min\n      1952.00000\n      6.001100e+04\n      23.599000\n      241.165876\n    \n    \n      25%\n      1965.75000\n      2.793664e+06\n      48.198000\n      1202.060309\n    \n    \n      50%\n      1979.50000\n      7.023596e+06\n      60.712500\n      3531.846988\n    \n    \n      75%\n      1993.25000\n      1.958522e+07\n      70.845500\n      9325.462346\n    \n    \n      max\n      2007.00000\n      1.318683e+09\n      82.603000\n      113523.132900\n    \n  \n\n\n\n\nFinally, if we want basic information about the non-numerical columns, we can use the value_counts() method. For a given series (or multiple series), this tells us how freqeuntly a given value appears. We will learn more about what this is doing under the hood when we learning about aggregation methods in a later section, but we can apply it to singular text columns here as a teaser\n\ndf.country.value_counts()\n\nAfghanistan          12\nPakistan             12\nNew Zealand          12\nNicaragua            12\nNiger                12\n                     ..\nEritrea              12\nEquatorial Guinea    12\nEl Salvador          12\nEgypt                12\nZimbabwe             12\nName: country, Length: 142, dtype: int64\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\nWhat will happen if we run the following code:\ndf.sample(42).describe()\n\nDo we expect the results to be the same as df.describe()? Why or why not?\nRun the code again. Are the results the same or different than before? Can you explain?\n\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\n\nThis should output a different result from df.describe(). We can break this down into two portions:\n\n\nWe create a new dataframe that holds the sampled dataframe via df.sample(42)\n\n\ndf_sample = df.sample(42)\n\n\nWe are now sending the results of this sampling to the describe method. Because the sampled dataset has only 42 rows that were randomly chosen from the original 1704, it would be an impressive coincidence if all the outputs were identical!\n\n\ndf_sample.describe()\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      count\n      42.000000\n      4.200000e+01\n      42.000000\n      42.000000\n    \n    \n      mean\n      1978.428571\n      1.335323e+07\n      63.269738\n      7061.108762\n    \n    \n      std\n      16.939120\n      1.908197e+07\n      12.209600\n      6160.436001\n    \n    \n      min\n      1952.000000\n      1.651100e+05\n      32.500000\n      299.850319\n    \n    \n      25%\n      1963.250000\n      1.659377e+06\n      53.718250\n      3278.926028\n    \n    \n      50%\n      1977.000000\n      4.417112e+06\n      67.653000\n      5463.159976\n    \n    \n      75%\n      1990.750000\n      1.471103e+07\n      72.412000\n      9066.273260\n    \n    \n      max\n      2007.000000\n      7.771830e+07\n      77.570000\n      24639.185660\n    \n  \n\n\n\n\n\nThe results should be different from the previous call! This is because sample() outputs a random sampling of the dataframe. Everytime we sample the dataset, we get a different subset! Each subset should end up with slightly different statistics if it is small enough relative to the entire dataset!\n\n\ndf.sample(42).describe()\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      count\n      42.000000\n      4.200000e+01\n      42.000000\n      42.000000\n    \n    \n      mean\n      1979.142857\n      2.383454e+07\n      59.864071\n      9425.050259\n    \n    \n      std\n      16.310361\n      4.870969e+07\n      12.555492\n      14248.305987\n    \n    \n      min\n      1952.000000\n      1.204470e+05\n      31.997000\n      388.000000\n    \n    \n      25%\n      1967.000000\n      2.907377e+06\n      49.643500\n      1515.316072\n    \n    \n      50%\n      1977.000000\n      8.040808e+06\n      62.024500\n      4189.473451\n    \n    \n      75%\n      1992.000000\n      2.029212e+07\n      71.588500\n      9785.682121\n    \n    \n      max\n      2007.000000\n      2.568942e+08\n      80.040000\n      80894.883260"
  },
  {
    "objectID": "21_getting_data_with_pandas.html#saving-our-data",
    "href": "21_getting_data_with_pandas.html#saving-our-data",
    "title": "3  Getting Data With Pandas",
    "section": "3.4 Saving our data",
    "text": "3.4 Saving our data\n\n\n\n\n\n\n\n\n\nYes, you can even save to a pickle.\n\n\n\n\n\nWhat do we do once we have cleaned up our data or produced some analysis? It is very likely that we will want to save that clean dataset or analysis to a new file. Pandas to the rescue! As simple as it is to read in data via read_csv() or read_excel(), we can export it back out. While I’ve shown the entire list of to_file() options available in pandas (it’s extensive!), we will focus on to_csv(). Required arguments to this method are:\n\npath_or_buf - full path/filename where you wish to save this file\n\nThat’s it! However, there are some useful optional arguments as well:\n\nindex: True or False. Whether we wish to include the index in our output (default is True). We will often want to set this to False, as the index is just a set of integers labeling the row numbers.\ncolumns: list of columns to keep in the output\n\n\n\n\n\n\n\npd.method() or df.method()?\n\n\n\nSometimes, in order to access a function, we directly access it via the library (pd), or we access it as a method of the dataframe we are using (df). It can be hard to keep track of which functions live where. As a general rule of thumb, if the method is being used to do something to a specific dataframe, it probably belongs to the dataframe object (which is, let’s be honest, most of the functions we might use). Don’t be afraid to use all of your resources to keep it straight! (Yes, google counts as a valid resource). Using the help function is also a quick and easy way to check where a method lives.\nFor example, to find the to_csv() function, we can see that it belongs to the dataframe by checking help(df.to_csv). However, trying to use pd.to_csv will throw an error - a good hint that it was a dataframe method all along!\n\nhelp(pd.to_csv)\n\nAttributeError: module 'pandas' has no attribute 'to_csv'\n\n\nInversely, the read_csv function belongs directly to pandas, and so trying something like this will break:\n\nhelp(df.read_csv)\n\nAttributeError: 'DataFrame' object has no attribute 'read_csv'\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\nSave a copy of all of the summary statistics for the gapfinder dataset. Only include the statistics for the pop and lifeExp columns. What happens when we include or exclude the index?\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nFirst, we want to make sure we have a folder to save our outputs to. I have made a folder called outputs that lives at the same level as the data folder. Next, we can invoke the to_csv method on a copy of our descriptive statistics:\n\ndf_descriptive = df.describe()\ndf_descriptive.to_csv('../outputs/challenge_output.csv', columns=['pop', 'lifeExp'])\n\nIn this case, if we exclude the index, we will actually lose information about what each row represents. This is one case when keeping the index will retain valuable information!\n\ndf.describe().to_csv(\n    '../outputs/challenge_output_no_index.csv', \n    columns=['pop', 'lifeExp'],\n    index=False\n    )\n\n\n\n\nUp next, we will learn how to clean our data."
  },
  {
    "objectID": "31_exploring_data_structures.html#exploring-and-understanding-data",
    "href": "31_exploring_data_structures.html#exploring-and-understanding-data",
    "title": "4  Exploring Data Structures",
    "section": "4.1 Exploring and Understanding Data",
    "text": "4.1 Exploring and Understanding Data\nGetting a high level summary of the data is important but data is particularly valuable when refined. Your analysis will start to come alive when we start to do some slicing and dicing and grouping of data or even creating additional variables. In an Excel world, this is like when you use filter options for columns, or create pivot tables, or when you create a formula in a new column to create a new variable. In data science parlance, this is the kind of thing that is referred to as data wrangling - getting the data you want in the form you want it in.\nIn the world of python, this usually means working with a library or package you have already been introduced to called pandas. It lets you do so much!\nThe first dataset we’ll look at is one that looks at data for countries around the world and shows population counts, life expectancy and GDP per capita over a number of years. Let’s import the pandas library and then use the .read_csv() function to get some population by country data. We will read the data into an object that will call country_metrics, so that it is easier for us to remember what it consists of.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/bcgov/\"\\\n    \"ds-intro-to-python/main/data/gapfinder.csv\"\ncountry_metrics = pd.read_csv(url)\n\nIt’s usually a good idea right away to take a quick look at the data to make sure what we have read in makes sense. The .info() method prints the number of columns, column labels, column data types, memory usage, range index, and the number of cells in each column (non-null values).\n\n\n\n\n\n\nReminder: Naming variables!\n\n\n\nWhen you create a variable or object, you can name it pretty much whatever you want. You will see widespread use of the name “df” on the internet, which stands for “dataframe”. However, as it is good practice to name your data something that relates to the contents. Naming it something intuitive will help reduce chance for confusion as you develop code. Objects you create should be some noun that is at the same time intuitive but not overly verbose. Also remember the form is important too. Remember not to not leave spaces in your names, and to always be consistent in the naming conventions you use (e.g. camelCase, underscore_case, etc.).\n\n\n\ncountry_metrics.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1704 entries, 0 to 1703\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   country    1704 non-null   object \n 1   year       1704 non-null   int64  \n 2   pop        1704 non-null   float64\n 3   continent  1704 non-null   object \n 4   lifeExp    1704 non-null   float64\n 5   gdpPercap  1704 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 80.0+ KB\n\n\nSo this data is looking good so far. We can see that we have created a pandas dataframe within our python environment. There are 1204 rows of data and six columns or variables. You can see there are only non-null values… so happily there is no missing data to worry about.\nThe .head() pandas method lets us look at actual data in a somewhat similar way to how one does in an Excel spreadsheet.\n\ncountry_metrics.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n  \n\n\n\n\nWhile we don’t learn too too much from this look, but we do see some sample data to see and start getting familiar with visually."
  },
  {
    "objectID": "31_exploring_data_structures.html#selecting-columns-variables",
    "href": "31_exploring_data_structures.html#selecting-columns-variables",
    "title": "4  Exploring Data Structures",
    "section": "4.2 Selecting columns (variables)",
    "text": "4.2 Selecting columns (variables)\nIn the “real world” of data it is not uncommon to see hundreds or even thousands of columns in a single dataframe, so knowing how to pare down our dataset so it is not overly bloated is important.\n\n\n\nSelecting Columns\n\n\n\nFirst things first. If we want to look the contents of a single column, we could do it like this, specifying the column name after the . without parentheses:\n\ncountry_metrics.country\n\n0       Afghanistan\n1       Afghanistan\n2       Afghanistan\n3       Afghanistan\n4       Afghanistan\n           ...     \n1699       Zimbabwe\n1700       Zimbabwe\n1701       Zimbabwe\n1702       Zimbabwe\n1703       Zimbabwe\nName: country, Length: 1704, dtype: object\n\n\nWhen we want to select more than one column to include in our dataframe, we use the convention df[['col1', 'col2']] to select the columns we want.\nIn our example below, we will create a new dataframe called narrow_country_metrics. We are primarily interested in getting the population metric from the original dataset, and since we want to be able to analyze it by year, country, and continent, we will add those into our new dataframe also. So the dataset will be more narrow than the one it was created from.\nThere are two ways to approach this. The first way to approach is to identify all the columns you want to keep.\n\nnarrow_country_metrics = country_metrics[\n    ['country', 'year', 'pop', 'continent']\n    ]\nnarrow_country_metrics.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n    \n  \n\n\n\n\nThe second way is use the pandas .drop() method to eliminate the columns from the original object that you do not want to keep. Thus in the end we can achieve the same result as the example above by simply dropping two variables instead of naming four. With this method, we also need to add the specification axis = 1, which indicates that it is columns (and not rows) being referenced for being dropped.\n\nnarrow_country_metrics = country_metrics.drop(\n    ['lifeExp', 'gdpPercap'], axis=1\n    )\nnarrow_country_metrics.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n    \n  \n\n\n\n\nEither way we have a more manageable dataset to work with called narrow_country_metrics. The original dataframe country_metrics is still there, and it still the same shape as before, we have not changed it.\nHowever, it is important to realize that in python it is very common to change an object by referring to that object on both sides of the assignment operator. In the code below, we narrow the narrow_country_metrics dataset down even further by removing the column pop as well.\n\nnarrow_country_metrics = narrow_country_metrics.drop(['pop'], axis=1)\nnarrow_country_metrics.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      continent\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      Asia\n    \n    \n      1\n      Afghanistan\n      1957\n      Asia\n    \n  \n\n\n\n\nIf we want to have the pop variable back in the narrow_country_metrics object, we will need to recreate it from the source it came from, where pop was still intact.\n\n\n\n\n\n\nReminder on Assignment!\n\n\n\nWhen you create a variable or dataframe object in python, to the left of the = sign you always put the name of the object you want to make or modify. To the right, that’s where you put the content that shows how the existing object is to be modified. It may feel counterintuitive but don’t be concerned, it is the standard way that the code is structured to be understood.\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nLet’s say you want to narrow down the dataset to include only the country and the year. How would you do it? Don’t forget to run your code so it also shows a view of the result so you can confirm the code worked as you wanted it to.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nFirst you would like to create a new object to hold this narrowed down version of the dataset, then use the .head() to see some of the resulting data.\n\nchallenge1_df = country_metrics[['country', 'year']]\nchallenge1_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n    \n    \n      1\n      Afghanistan\n      1957"
  },
  {
    "objectID": "31_exploring_data_structures.html#selecting-rows",
    "href": "31_exploring_data_structures.html#selecting-rows",
    "title": "4  Exploring Data Structures",
    "section": "4.3 Selecting rows",
    "text": "4.3 Selecting rows\nOf course, in data analysis we are usually interested in looking at just some rows of data, not all rows all the time.\n\n\n\nSelecting Rows\n\n\n\nRelational operators\nWhen we want to look at just selected rows (i.e. select rows that have certain values within a given column) we can supply a condition that must be met for a given row in that column. To do this, we must use one of the following comparison operators, which are also called relational operators:\n\nequal to ==\nnot equal to !=\nless than <\ngreater than >\nless than or equal to <=\ngreater than or equal to >=\n\nLet’s try an example. The most commonly used relational operator is likely equal to (==). In the example below we have a statement that pandas evaluates line by line in the dataframe as to whether it corresponds to a boolean value of True or False.\n\ncountry_metrics['year'] == 1972\n\n0       False\n1       False\n2       False\n3       False\n4        True\n        ...  \n1699    False\n1700    False\n1701    False\n1702    False\n1703    False\nName: year, Length: 1704, dtype: bool\n\n\nWhen we create an object from this evaluation in pandas, it returns to that object only the rows that evaluate as True:\n\nfiltered_country_metrics = country_metrics[country_metrics['year'] == 1972]\nfiltered_country_metrics.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n    \n      16\n      Albania\n      1972\n      2263554.0\n      Europe\n      67.690\n      3313.422188\n    \n  \n\n\n\n\n Logical operators\nIn python, it is common to use the logical operators and, or, and not to evaluate expressions. For example, as in the code below, python evaluates the and to see whether BOTH operands are true:\n\nx = 5\nprint(x > 3 and x < 10)\n\nTrue\n\n\nHowever, when we ask pandas to evaluate whether a set of logical relations exist within a pandas series object, we must use pandas bitwise logical operators, whose syntax is different:\n\nand &\nor |\nnot ~\n\nWhen combining multiple conditional statements in a pandas series object, each condition must be surrounded by parentheses () within the square brackets []. In the example below, we use an & to indicate that both conditions must be true for a row to be returned:\n\nfiltered_country_metrics = country_metrics[\n    (country_metrics['year'] == 1972) &   \n    (country_metrics['country'] == 'Albania')\n    ]\nfiltered_country_metrics.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      16\n      Albania\n      1972\n      2263554.0\n      Europe\n      67.69\n      3313.422188\n    \n  \n\n\n\n\nIn the example above, there is only one row of data that matches the condition.\nWe can use the | which indicates that if either or both of the conditions are true for a given row, that row is returned into the object:\n\nfiltered_country_metrics = country_metrics[\n    (country_metrics['year'] == 1972) | \n    (country_metrics['country'] == 'Albania')\n    ]\nfiltered_country_metrics.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n    \n      12\n      Albania\n      1952\n      1282697.0\n      Europe\n      55.230\n      1601.056136\n    \n    \n      13\n      Albania\n      1957\n      1476505.0\n      Europe\n      59.280\n      1942.284244\n    \n    \n      14\n      Albania\n      1962\n      1728137.0\n      Europe\n      64.820\n      2312.888958\n    \n    \n      15\n      Albania\n      1967\n      1984060.0\n      Europe\n      66.220\n      2760.196931\n    \n  \n\n\n\n\nIn the example above, there are many rows that satisfy one or the other condition. As an aside, were we to want to know how many rows fulfill this | situation, we could have called the .info() method instead of .head().\n\n\n\n\n\n\nChallenge 2\n\n\n\nYour director has come to you and asked if you know what the life expectancy has been in Canada since 1992. How would you use pandas code to get the data you need? And after having run the code, what’s the answer?\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\nFirst you would like to create a new object to hold this narrowed down version of the dataset, then use some method like .print() or .head() to see some of the resulting data.\n\nchallenge2_df = country_metrics[\n    (country_metrics['year'] >= 1992) & \n    (country_metrics['country'] == 'Canada')\n    ]    \nchallenge2_df.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      248\n      Canada\n      1992\n      28523502.0\n      Americas\n      77.950\n      26342.88426\n    \n    \n      249\n      Canada\n      1997\n      30305843.0\n      Americas\n      78.610\n      28954.92589\n    \n    \n      250\n      Canada\n      2002\n      31902268.0\n      Americas\n      79.770\n      33328.96507\n    \n    \n      251\n      Canada\n      2007\n      33390141.0\n      Americas\n      80.653\n      36319.23501\n    \n  \n\n\n\n\n\n\n\nYou will find that it is very common within python/pandas to see the .iloc() function used, so it is worthwhile to give that a brief introduction also.\nThis function enables the selection of data from a particular row or column, according to the integer based position index as shown in the image below:\n\n\n\nRow and column values iloc() refers to\n\n\n\nBy referring to these locations, we can use iloc() to retrieve the exact cells we want to see. For example, if we just want the row of data corresponding to the index integer value 5, we would run the following code:\n\ncountry_metrics.iloc[5]\n\ncountry      Afghanistan\nyear                1977\npop           14880372.0\ncontinent           Asia\nlifeExp           38.438\ngdpPercap      786.11336\nName: 5, dtype: object\n\n\nWe can also modify the above call to look at column(s)! For example, we specify the row(s) of interest to the left of a comma in the square brackets, and the column(s) of interest to the right of it. In the example below, the code will retrieve just the content found at the intersection of row 5 and column 3:\n\ncountry_metrics.iloc[5,3]\n\n'Asia'\n\n\nWe can also retrieve ranges of rows and columns respectively by employing a colon along with the starting and ending rows or columns we want. In the example below, we want to get rows “0” through “2” and the columns “0” through “3”:\n\ncountry_metrics.iloc[0:2,0:3]\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n    \n  \n\n\n\n\nIt is a flexible tool that can be very helpful as you build and review your code."
  },
  {
    "objectID": "31_exploring_data_structures.html#sorting-rows",
    "href": "31_exploring_data_structures.html#sorting-rows",
    "title": "4  Exploring Data Structures",
    "section": "4.4 Sorting rows",
    "text": "4.4 Sorting rows\nSo now you’ve mastered how to select rows and columns you want, congratulations! Instead of hunting and pecking for insights, one way to quickly make some sense of the data is to sort it - something you probably do in Excel all the time.\nLet’s say we wanted to know more about Asian countries since 2002. First we should go ahead and create a dataframe that consists of just the data we are wanting to look at.\n\ncountries_2000s = country_metrics[\n    (country_metrics['year'] >= 2002) & \n    (country_metrics['continent'] == 'Asia')\n    ]    \ncountries_2000s.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      10\n      Afghanistan\n      2002\n      25268405.0\n      Asia\n      42.129\n      726.734055\n    \n    \n      11\n      Afghanistan\n      2007\n      31889923.0\n      Asia\n      43.828\n      974.580338\n    \n    \n      94\n      Bahrain\n      2002\n      656397.0\n      Asia\n      74.795\n      23403.559270\n    \n    \n      95\n      Bahrain\n      2007\n      708573.0\n      Asia\n      75.635\n      29796.048340\n    \n    \n      106\n      Bangladesh\n      2002\n      135656790.0\n      Asia\n      62.013\n      1136.390430\n    \n  \n\n\n\n\nThis is a nice start. But to see some meaningful insights it is helpful to sort the metric we are interested in something other than the default way the data appear positioned in the dataframe.\nThe .sort_values() method let’s us specify a column (or multiple columns) we want to sort and enter an argument that indicates in which direction we would like it to be sorted, from low to high or vice-versa. We want life expectancy from high to low, so the ascending parameter should be set to False.\n\ncountries_2000s = countries_2000s.sort_values('lifeExp', ascending=False)\ncountries_2000s.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      803\n      Japan\n      2007\n      127467972.0\n      Asia\n      82.603\n      31656.06806\n    \n    \n      671\n      Hong Kong China\n      2007\n      6980412.0\n      Asia\n      82.208\n      39724.97867\n    \n    \n      802\n      Japan\n      2002\n      127065841.0\n      Asia\n      82.000\n      28604.59190\n    \n    \n      670\n      Hong Kong China\n      2002\n      6762476.0\n      Asia\n      81.495\n      30209.01516\n    \n    \n      767\n      Israel\n      2007\n      6426679.0\n      Asia\n      80.745\n      25523.27710\n    \n  \n\n\n\n\nThat was pretty straightforward and it helps us get more insights. Looks like Japan, in 2007, was the Asian country with the highest life expectancy.\nOften one will want to sort by more than one column. To do that, instead of passing in a single column, we pass in a list of the columns we want to sort on. We can also control whether we would like each column to be sorted in ascending or descending order by passing in a respective list of boolean values in the ascending= parameter. In the example below, the code indicates that we want to sort first by ‘country’ in reverse alphabetical order, then by life expectancy from high to low.\n\ncountries_2000s = countries_2000s.sort_values(\n    ['country', 'lifeExp'], ascending=[False, False]\n    )\ncountries_2000s.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      1679\n      Yemen Rep.\n      2007\n      22211743.0\n      Asia\n      62.698\n      2280.769906\n    \n    \n      1678\n      Yemen Rep.\n      2002\n      18701257.0\n      Asia\n      60.308\n      2234.820827\n    \n    \n      1667\n      West Bank and Gaza\n      2007\n      4018332.0\n      Asia\n      73.422\n      3025.349798\n    \n    \n      1666\n      West Bank and Gaza\n      2002\n      3389578.0\n      Asia\n      72.370\n      4515.487575\n    \n    \n      1655\n      Vietnam\n      2007\n      85262356.0\n      Asia\n      74.249\n      2441.576404\n    \n  \n\n\n\n\nWe see that when we created and sorted this new object, the index values are those that were associated with the original country_metrics dataframe.\nDepending on what we want to do with countries_2000s going forward, we might want to create a new “default” index for it instead of the existing one. That’s where the .reset_index() function comes into play.\nIn the code below, we reset the index.\n\ncountries_2000s = countries_2000s.reset_index(drop=False)\ncountries_2000s.head()\n\n\n\n\n\n  \n    \n      \n      index\n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      1679\n      Yemen Rep.\n      2007\n      22211743.0\n      Asia\n      62.698\n      2280.769906\n    \n    \n      1\n      1678\n      Yemen Rep.\n      2002\n      18701257.0\n      Asia\n      60.308\n      2234.820827\n    \n    \n      2\n      1667\n      West Bank and Gaza\n      2007\n      4018332.0\n      Asia\n      73.422\n      3025.349798\n    \n    \n      3\n      1666\n      West Bank and Gaza\n      2002\n      3389578.0\n      Asia\n      72.370\n      4515.487575\n    \n    \n      4\n      1655\n      Vietnam\n      2007\n      85262356.0\n      Asia\n      74.249\n      2441.576404\n    \n  \n\n\n\n\nNotice how the original index now is turned into a column itself called “index”. Depending on the use case, we might want to keep this column in the new dataframe. For example, if we ultimately wanted to join this dataframe back with the original it could act as a key. If we know, however, that it is of no value anymore to us, we can and probably should delete it. To do this, we would have set the drop parameter above to True.\nLet’s use the iloc() method to look at the positional content of this countries_2000s object and confirm that Yeman shows up in row 0 as we expect.\n\ncountries_2000s.iloc[0:4,0:5]\n\n\n\n\n\n  \n    \n      \n      index\n      country\n      year\n      pop\n      continent\n    \n  \n  \n    \n      0\n      1679\n      Yemen Rep.\n      2007\n      22211743.0\n      Asia\n    \n    \n      1\n      1678\n      Yemen Rep.\n      2002\n      18701257.0\n      Asia\n    \n    \n      2\n      1667\n      West Bank and Gaza\n      2007\n      4018332.0\n      Asia\n    \n    \n      3\n      1666\n      West Bank and Gaza\n      2002\n      3389578.0\n      Asia"
  },
  {
    "objectID": "31_exploring_data_structures.html#putting-multiple-methods-together",
    "href": "31_exploring_data_structures.html#putting-multiple-methods-together",
    "title": "4  Exploring Data Structures",
    "section": "4.5 Putting multiple methods together",
    "text": "4.5 Putting multiple methods together\nThe next step is usually to bring several of these commands together to get a nicely refined look at the data. Essentially you will need to invoke a number of calls sequentially to some object, with each one in turn performing some action on it.\nA common approach is to put the object name on each sucessive line of code along with the = operator as well as the modifying code to the right. Imagine we were given the following task:\n\nSelect only the rows where the country is equal to Ireland AND where the year is 1992 or greater\nTake these rows and sort them by year, going from high to low values, top to bottom\n\nThe example below shows how we would employ the line-by-line approach:\n\nchained2_df = country_metrics[(country_metrics['country'] == 'Ireland')]\nchained2_df = chained2_df[(chained2_df['year'] >= 1992)]\nchained2_df = chained2_df.sort_values('year', ascending=False)\nchained2_df.head(3)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      755\n      Ireland\n      2007\n      4109086.0\n      Europe\n      78.885\n      40675.99635\n    \n    \n      754\n      Ireland\n      2002\n      3879155.0\n      Europe\n      77.783\n      34077.04939\n    \n    \n      753\n      Ireland\n      1997\n      3667233.0\n      Europe\n      76.122\n      24521.94713\n    \n  \n\n\n\n\nYou can read fairly clearly what is happening in each line.\nYou can also put multiple methods in the same line of code, as long as you separate each of the elements. In pandas, this is sometimes referred to as joining or chaining, as you are essentially creating a joining/chaining actions together.\nThe code below accomplishes what the more verbose code above does but in a more efficient way:\n\nchained_df = country_metrics[\n    (country_metrics['country'] == 'Ireland') & \n    (country_metrics['year'] >= 1992)\n    ].sort_values('year', ascending = False)\nchained_df.head(3)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      755\n      Ireland\n      2007\n      4109086.0\n      Europe\n      78.885\n      40675.99635\n    \n    \n      754\n      Ireland\n      2002\n      3879155.0\n      Europe\n      77.783\n      34077.04939\n    \n    \n      753\n      Ireland\n      1997\n      3667233.0\n      Europe\n      76.122\n      24521.94713\n    \n  \n\n\n\n\nEach line is executed in sequence, so be careful when constructing code that the order in which you modify your dataframe is what you intend. Take the example below, in which we want to find European values for GDP per capita. In it, we first have a line of code that selects the rows that contain “Europe” in the first line, then looks at the column values for “year”, “country”, and “gdpPercap”.\n\nyear_country_gdp = country_metrics[\n    (country_metrics['continent'] == 'Europe')\n    ]\nyear_country_gdp = year_country_gdp[['year', 'country', 'gdpPercap']]\nyear_country_gdp.head(3)\n\n\n\n\n\n  \n    \n      \n      year\n      country\n      gdpPercap\n    \n  \n  \n    \n      12\n      1952\n      Albania\n      1601.056136\n    \n    \n      13\n      1957\n      Albania\n      1942.284244\n    \n    \n      14\n      1962\n      Albania\n      2312.888958\n    \n  \n\n\n\n\nHad we reversed the lines of code, and selected the “year”, “country”, and “gdpPercap” columns first to put into our new object, we would not have been able to sort for “Europe” in the “continent” column, as the “continent” would have been essentially removed from the dataframe in the step above. So an error would have been thrown.\n\n\n\n\n\n\nChallenge 3\n\n\n\nYour director has come back to you and wondered about whether the life expectancy of people changed during the 1920s in Cambodia. Use what you know about selecting rows and sorting data to get the data you need to answer the question, sorted from most recent at the top.\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nUsing code to select the country Cambodia, then call sort_values() method and chain them together and sorting by year in descending fashion. Looking at the data we see that in the 1920s there was a sharp decline in life expectancy in Cambodia. We also see, thankfully, that it has recovered strongly since then.\n\nchallenge3_df = country_metrics[\n    (country_metrics['country'] == 'Cambodia')\n    ].sort_values('year', ascending = False)\nchallenge3_df.head(10)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      227\n      Cambodia\n      2007\n      14131858.0\n      Asia\n      59.723\n      1713.778686\n    \n    \n      226\n      Cambodia\n      2002\n      12926707.0\n      Asia\n      56.752\n      896.226015\n    \n    \n      225\n      Cambodia\n      1997\n      11782962.0\n      Asia\n      56.534\n      734.285170\n    \n    \n      224\n      Cambodia\n      1992\n      10150094.0\n      Asia\n      55.803\n      682.303175\n    \n    \n      223\n      Cambodia\n      1987\n      8371791.0\n      Asia\n      53.914\n      683.895573\n    \n    \n      222\n      Cambodia\n      1982\n      7272485.0\n      Asia\n      50.957\n      624.475478\n    \n    \n      221\n      Cambodia\n      1977\n      6978607.0\n      Asia\n      31.220\n      524.972183\n    \n    \n      220\n      Cambodia\n      1972\n      7450606.0\n      Asia\n      40.317\n      421.624026\n    \n    \n      219\n      Cambodia\n      1967\n      6960067.0\n      Asia\n      45.415\n      523.432314\n    \n    \n      218\n      Cambodia\n      1962\n      6083619.0\n      Asia\n      43.415\n      496.913648"
  },
  {
    "objectID": "31_exploring_data_structures.html#creating-new-columns-of-data",
    "href": "31_exploring_data_structures.html#creating-new-columns-of-data",
    "title": "4  Exploring Data Structures",
    "section": "4.6 Creating new columns of data",
    "text": "4.6 Creating new columns of data\nVery often we have some data in our dataset that we want to transform to give us additional information. In Excel this is something that is done all the time by creating a formula in a cell that refers to other columns and applies some sort of logic or mathematical expression to it.\nThere are different ways you can go about this in pandas. The most straightforward way is to define a new column on the left of the = and then reference the existing column and whatever additional conditions you would like on the right. In the example below, the existing population variable “pop” is converted to a value that shows population in millions.\n\nnew_cols_df = country_metrics\nnew_cols_df['pop_millions'] = new_cols_df['pop']/1000000\nnew_cols_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n    \n  \n\n\n\n\n Another way is to call the pandas function apply() in which other, more complicated functions can be passed for a given column. This could be a complicated mathematical formula, a function acting on strings or dates, or any other function that cannot be represented by a simple multiplication or division. In the example below, the len() function is applied to each row in the “country” column and a new column with the number of characters for that row is returned:\n\nnew_cols_df['country_name_chars'] = new_cols_df.country.apply(len)\nnew_cols_df.sample(5)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n      country_name_chars\n    \n  \n  \n    \n      791\n      Jamaica\n      2007\n      2780132.0\n      Americas\n      72.567\n      7320.880262\n      2.780132\n      7\n    \n    \n      406\n      Czech Republic\n      2002\n      10256295.0\n      Europe\n      75.510\n      17596.210220\n      10.256295\n      14\n    \n    \n      878\n      Lesotho\n      1962\n      893143.0\n      Africa\n      47.747\n      411.800627\n      0.893143\n      7\n    \n    \n      650\n      Honduras\n      1962\n      2090162.0\n      Americas\n      48.041\n      2291.156835\n      2.090162\n      8\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n      13.079460\n      11\n    \n  \n\n\n\n\n There is also a special python function called lambda. It is known as an anonymous function, appears in a single line of code, and is not given a name other than lambda. It can take any number of arguments in an expression.\nThe .assign() method looks at this expression with lambda in it and returns the value it is asked to do and assigns it to the variable name given it. This is how it comes all together to give us actual GDP for each row in our dataframe:\n\nGDP_df = new_cols_df.assign(GDP=lambda x: x['pop'] * x['gdpPercap'])\nGDP_df.head(5)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n      country_name_chars\n      GDP\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n      11\n      6.567086e+09\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n      11\n      7.585449e+09\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n      10.267083\n      11\n      8.758856e+09\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n      11.537966\n      11\n      9.648014e+09\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n      13.079460\n      11\n      9.678553e+09\n    \n  \n\n\n\n\nSo now we can see the newly created column “GDP” and see the value for each country in our new object. Notice that the GDP data is in scientific notation (i.e. the decimal number times x number of zeros), so it’s a bit hard to read. If we wanted readers to consume that data we would go ahead and change the data type for it. But for current purposes we’ll leave that alone."
  },
  {
    "objectID": "31_exploring_data_structures.html#joining-datasets-together",
    "href": "31_exploring_data_structures.html#joining-datasets-together",
    "title": "4  Exploring Data Structures",
    "section": "4.7 Joining datasets together",
    "text": "4.7 Joining datasets together\nOne of the most important tasks in data analysis is to be able to join multiple datasets together. With pandas, there are functions called .merge() and .join() that are similar to each other. As .merge() is perhaps the more used, intuitive, and powerful of the two, we will introduce that method in this tutorial in some depth. There is also a cool function called .concat() that will be introduced below as well.\nBut first, let’s get a second file that gives us country size in square kilometers by country. We will use this data to put together with our country_metrics dataframe.\n\ncountry_size_url = \"https://raw.githubusercontent.com/bcgov/\"\\\n    \"ds-intro-to-python/main/data/countrysize.csv\"\ncountry_size = pd.read_csv(country_size_url, encoding= 'unicode_escape')\ncountry_size.info()\ncountry_size.sample(2)\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 235 entries, 0 to 234\nData columns (total 2 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   nation           235 non-null    object\n 1   area_square_kms  235 non-null    int64 \ndtypes: int64(1), object(1)\nmemory usage: 3.8+ KB\n\n\n\n\n\n\n  \n    \n      \n      nation\n      area_square_kms\n    \n  \n  \n    \n      212\n      Saint Pierre and Miquelon\n      242\n    \n    \n      208\n      Maldives\n      300\n    \n  \n\n\n\n\nOk, this country_size dataset looks like we expect - a list of countries alongside the land size of that country in square kilometers. There are some omissions in this list - not all countries in our country_metrics dataset are present in this country_size object. But for our present purposes this is ok. We just want to get that square kilometers data into a combined dataset and it is sufficient for that.\nEssentially what we want to do is a classic “left-join” operation of the sort in the diagram below. Conceptually, the country_metrics dataset is like the purple table and the country_size dataset is like the red one.\n\n\n\nTypes of Joins\n\n\n\nThe .merge() method (with reference material available here) works similarly to how table joining works in SQL or how the VLOOKUP function works in Excel. One needs to specify both dataframes, the key variables on which to join them, and the kind of join desired.\nSo let’s look at the example below to see how it all comes together in code.\n\ncombined_df = country_metrics.merge(\n    country_size, \n    left_on='country', \n    right_on='nation', \n    how='left'\n    )\ncombined_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n      country_name_chars\n      nation\n      area_square_kms\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n      11\n      Afghanistan\n      652230.0\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n      11\n      Afghanistan\n      652230.0\n    \n  \n\n\n\n\n When you run the code above you will notice that both the nation key column and the area_square_kms column have been joined together in the new combined_df object. One can keep that nation column in there for control purposes, or it can be removed by using the .drop() method we used earlier:\n\ncombined_df = combined_df.drop('nation', axis=1) \ncombined_df.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n      country_name_chars\n      area_square_kms\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n      11\n      652230.0\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n      11\n      652230.0\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n      10.267083\n      11\n      652230.0\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n      11.537966\n      11\n      652230.0\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n      13.079460\n      11\n      652230.0\n    \n  \n\n\n\n\nAnother innovative way to put “merge” data together in pandas is with the .concat() function. Conceptually you can think if it like “stacking” two data objects on top of each other or side-by-side as shown in the diagram below.\n\n\n\nWays to stack data\n\n\n\nTo illustrate, let’s fetch two simple dataframes. Each contains the average scores for three subjects by year for two separate schools.\n\nschool1_url  = \"https://raw.githubusercontent.com/bcgov/\"\\\n    \"ds-intro-to-python/main/data/school1.csv\"\nschool2_url  = \"https://raw.githubusercontent.com/bcgov/\"\\\n    \"ds-intro-to-python/main/data/school2.csv\"\nschool1_df = pd.read_csv(school1_url)\nschool2_df = pd.read_csv(school2_url)\n\nLet’s take a quick look at the two dataframes.\n\nschool1_df\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      67\n      88\n      89\n    \n    \n      1\n      English\n      86\n      67\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      75\n    \n  \n\n\n\n\n\nschool2_df\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      32\n      88\n      90\n    \n    \n      1\n      English\n      88\n      44\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      53\n    \n  \n\n\n\n\nWe call .concat() and pass in the objects that we want to stack vertically. This is a similiar operation to union in SQL.\n\nvertical_stack_df = pd.concat([school1_df, school2_df])\nvertical_stack_df\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      67\n      88\n      89\n    \n    \n      1\n      English\n      86\n      67\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      75\n    \n    \n      0\n      Science\n      32\n      88\n      90\n    \n    \n      1\n      English\n      88\n      44\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      53\n    \n  \n\n\n\n\nIt is also possible to stack the data horizontally. Here it is necessary to specify the columnar axis (axis=1) as the default setting is for rows (axis=0).\n\nhorizontal_stack = pd.concat([school1_df, school2_df], axis=1)\nhorizontal_stack\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      67\n      88\n      89\n      Science\n      32\n      88\n      90\n    \n    \n      1\n      English\n      86\n      67\n      77\n      English\n      88\n      44\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      75\n      Math\n      84\n      73\n      53\n    \n  \n\n\n\n\nThis introduction only scratches the surface of how to leverage this way of joining datasets together. But it can be a powerful tool in the toolkit for the right use case. More detail found here."
  },
  {
    "objectID": "31_exploring_data_structures.html#grouping-and-summarizing",
    "href": "31_exploring_data_structures.html#grouping-and-summarizing",
    "title": "4  Exploring Data Structures",
    "section": "4.8 Grouping and summarizing",
    "text": "4.8 Grouping and summarizing\nSometimes of course one would prefer to group rows together for the purpose of summarizing them in various ways.\nIn pandas, we can accomplish this using the .groupby() method. A .groupby() operation involves some combination of splitting the object, applying a function, and combining the results. It is used together with one or more aggregation functions:\n\ncount(): Total number of items\nfirst(), last(): First and last item\nmean(), median(): Mean and median\nmin(), max(): Minimum and maximum\nstd(), var(): Standard deviation and variance\nmad(): Mean absolute deviation\nprod(): Product of all items\nsum(): Sum of all items\n\nFirst let us look at a simple example where we want to get the mean life expectancy for each continent in the data. To do this, we would use the groupby() function to call the appropriate segment (i.e. continent), metric (i.e. lifeExp), and type of aggregation (i.e. mean):\n\nsimple_mean = country_metrics.groupby('continent').lifeExp.mean()\nsimple_mean\n\ncontinent\nAfrica      48.865330\nAmericas    64.658737\nAsia        60.064903\nEurope      71.903686\nOceania     74.326208\nName: lifeExp, dtype: float64\n\n\nFortunately, if we want to look at several aggregations at once, we can do that too. To extend our example above, we would specify the “continent” column in the .groupby() function, then pass additional aggregation functions (in our case, we will add “mean”, “min”, and “max”) as a dictionary within the .agg() function.\n\n\n\n\n\n\nRecall: Dictionaries in Python\n\n\n\nRemember that a dictionary in Python language is a particular type of data structure that contains a collection of key: value pairs. It is analogous to a regular word dictionary you are familiar with which is a collection of words to their meanings. Python dictionaries allow us to associate a value to a unique key, and then to quickly access this value. They are generally created within curly braces {} and have a specified key name and value (e.g. basic form for a python dictionary {\"key1\": \"value1\"})\n\n\n This dictionary takes the column that we are aggregating - in this case life expectancy - as a key and a list of aggregation functions as its value. We also add a line of code that gives each of the columns a new name with the .columns() function.\n\ngrouped_single = country_metrics.groupby('continent') \\\n    .agg({'lifeExp': ['mean', 'min', 'max']})\ngrouped_single.columns = ['lifeExp_mean', 'lifeExp_min', 'lifeExp_max']\ngrouped_single\n\n\n\n\n\n  \n    \n      \n      lifeExp_mean\n      lifeExp_min\n      lifeExp_max\n    \n    \n      continent\n      \n      \n      \n    \n  \n  \n    \n      Africa\n      48.865330\n      23.599\n      76.442\n    \n    \n      Americas\n      64.658737\n      37.579\n      80.653\n    \n    \n      Asia\n      60.064903\n      28.801\n      82.603\n    \n    \n      Europe\n      71.903686\n      43.585\n      81.757\n    \n    \n      Oceania\n      74.326208\n      69.120\n      81.235\n    \n  \n\n\n\n\nIf you look closely at grouped_single you see that the variable “continent” is on a different line than are the columns that are aggregated. That is because continent in this instance is actually an index and not a column.\nWe can nest additional “groups within groups” by creating a list of column names and passing them to the .groupby() function instead of passing a single value.\nThe example below adds more granularity with the introduction of ‘country’ and the creation of a list to hold both ‘continent’ and ‘country’.\n\ngrouped_multiple = country_metrics.groupby(\n    ['continent', 'country']).agg({'pop': ['mean', 'min', 'max']}\n    )\ngrouped_multiple.columns = ['pop_mean', 'pop_min', 'pop_max']\ngrouped_multiple.head(10)\n\n\n\n\n\n  \n    \n      \n      \n      pop_mean\n      pop_min\n      pop_max\n    \n    \n      continent\n      country\n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      1.987541e+07\n      9279525.0\n      33333216.0\n    \n    \n      Angola\n      7.309390e+06\n      4232095.0\n      12420476.0\n    \n    \n      Benin\n      4.017497e+06\n      1738315.0\n      8078314.0\n    \n    \n      Botswana\n      9.711862e+05\n      442308.0\n      1639131.0\n    \n    \n      Burkina Faso\n      7.548677e+06\n      4469979.0\n      14326203.0\n    \n    \n      Burundi\n      4.651608e+06\n      2445618.0\n      8390505.0\n    \n    \n      Cameroon\n      9.816648e+06\n      5009067.0\n      17696293.0\n    \n    \n      Central African Republic\n      2.560963e+06\n      1291695.0\n      4369038.0\n    \n    \n      Chad\n      5.329256e+06\n      2682462.0\n      10238807.0\n    \n    \n      Comoros\n      3.616839e+05\n      153936.0\n      710960.0\n    \n  \n\n\n\n\nAgain will notice that this dataframe looks a little different than some others we have seen in that there are blank spaces below each grouped value of continent in that index. This object is a multi-indexed dataframe. It presents the data nicely to consume visually, but it is not ideal to work with if you want to do further manipulation.\nThis is likely again a good time to use the reset_index() function. This is a step that resets the index to an integer based index and re-creates a non-indexed pandas dataframe.\nThe code block below is identical to the one we just ran except for the line of code that resets the index:\n\ngrouped_multiple = country_metrics.groupby(\n    ['continent', 'country']).agg({'pop': ['mean', 'min', 'max']}\n    )\ngrouped_multiple.columns = ['pop_mean', 'pop_min', 'pop_max']\ngrouped_multiple = grouped_multiple.reset_index()  # resets the index\ngrouped_multiple.head(10)\n\n\n\n\n\n  \n    \n      \n      continent\n      country\n      pop_mean\n      pop_min\n      pop_max\n    \n  \n  \n    \n      0\n      Africa\n      Algeria\n      1.987541e+07\n      9279525.0\n      33333216.0\n    \n    \n      1\n      Africa\n      Angola\n      7.309390e+06\n      4232095.0\n      12420476.0\n    \n    \n      2\n      Africa\n      Benin\n      4.017497e+06\n      1738315.0\n      8078314.0\n    \n    \n      3\n      Africa\n      Botswana\n      9.711862e+05\n      442308.0\n      1639131.0\n    \n    \n      4\n      Africa\n      Burkina Faso\n      7.548677e+06\n      4469979.0\n      14326203.0\n    \n    \n      5\n      Africa\n      Burundi\n      4.651608e+06\n      2445618.0\n      8390505.0\n    \n    \n      6\n      Africa\n      Cameroon\n      9.816648e+06\n      5009067.0\n      17696293.0\n    \n    \n      7\n      Africa\n      Central African Republic\n      2.560963e+06\n      1291695.0\n      4369038.0\n    \n    \n      8\n      Africa\n      Chad\n      5.329256e+06\n      2682462.0\n      10238807.0\n    \n    \n      9\n      Africa\n      Comoros\n      3.616839e+05\n      153936.0\n      710960.0\n    \n  \n\n\n\n\nThis looks more like the structure of the dataframes we know already and will be easier to manipulate further.\n\n\n\n\n\n\nChallenge 4\n\n\n\nYou would like to summarize population as well as life expectancy by year, grouped by continent. Pick some aggregations that would make sense to look at for this task. Don’t worry about re-setting the index or about creating new labels for the result.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nUsing what we learned about how to select rows, we should limit the dataframe to rows where the year is greater than or equal to 1992. Next we should create a multi-column call to the .groupby() function. Finally we should select some aggregations such as mean and max among others could make sense here.\n\nchallenge4_df = country_metrics.groupby(\n    ['continent', 'year']).agg({'pop' : ['mean', 'max'], 'lifeExp' : ['mean', 'max']}\n    )\nchallenge4_df\n\n\n\n\n\n  \n    \n      \n      \n      pop\n      lifeExp\n    \n    \n      \n      \n      mean\n      max\n      mean\n      max\n    \n    \n      continent\n      year\n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      1952\n      4.570010e+06\n      3.311910e+07\n      39.135500\n      52.724\n    \n    \n      1957\n      5.093033e+06\n      3.717334e+07\n      41.266346\n      58.089\n    \n    \n      1962\n      5.702247e+06\n      4.187135e+07\n      43.319442\n      60.246\n    \n    \n      1967\n      6.447875e+06\n      4.728775e+07\n      45.334538\n      61.557\n    \n    \n      1972\n      7.305376e+06\n      5.374008e+07\n      47.450942\n      64.274\n    \n    \n      1977\n      8.328097e+06\n      6.220917e+07\n      49.580423\n      67.064\n    \n    \n      1982\n      9.602857e+06\n      7.303938e+07\n      51.592865\n      69.885\n    \n    \n      1987\n      1.105450e+07\n      8.155152e+07\n      53.344788\n      71.913\n    \n    \n      1992\n      1.267464e+07\n      9.336424e+07\n      53.629577\n      73.615\n    \n    \n      1997\n      1.430448e+07\n      1.062078e+08\n      53.598269\n      74.772\n    \n    \n      2002\n      1.603315e+07\n      1.199013e+08\n      53.325231\n      75.744\n    \n    \n      2007\n      1.787576e+07\n      1.350312e+08\n      54.806038\n      76.442\n    \n    \n      Americas\n      1952\n      1.380610e+07\n      1.575530e+08\n      53.279840\n      68.750\n    \n    \n      1957\n      1.547816e+07\n      1.719840e+08\n      55.960280\n      69.960\n    \n    \n      1962\n      1.733081e+07\n      1.865380e+08\n      58.398760\n      71.300\n    \n    \n      1967\n      1.922986e+07\n      1.987120e+08\n      60.410920\n      72.130\n    \n    \n      1972\n      2.117537e+07\n      2.098960e+08\n      62.394920\n      72.880\n    \n    \n      1977\n      2.312271e+07\n      2.202390e+08\n      64.391560\n      74.210\n    \n    \n      1982\n      2.521164e+07\n      2.321878e+08\n      66.228840\n      75.760\n    \n    \n      1987\n      2.731016e+07\n      2.428035e+08\n      68.090720\n      76.860\n    \n    \n      1992\n      2.957096e+07\n      2.568942e+08\n      69.568360\n      77.950\n    \n    \n      1997\n      3.187602e+07\n      2.729118e+08\n      71.150480\n      78.610\n    \n    \n      2002\n      3.399091e+07\n      2.876755e+08\n      72.422040\n      79.770\n    \n    \n      2007\n      3.595485e+07\n      3.011399e+08\n      73.608120\n      80.653\n    \n    \n      Asia\n      1952\n      4.228356e+07\n      5.562635e+08\n      46.314394\n      65.390\n    \n    \n      1957\n      4.735699e+07\n      6.374080e+08\n      49.318544\n      67.840\n    \n    \n      1962\n      5.140476e+07\n      6.657700e+08\n      51.563223\n      69.390\n    \n    \n      1967\n      5.774736e+07\n      7.545500e+08\n      54.663640\n      71.430\n    \n    \n      1972\n      6.518098e+07\n      8.620300e+08\n      57.319269\n      73.420\n    \n    \n      1977\n      7.225799e+07\n      9.434550e+08\n      59.610556\n      75.380\n    \n    \n      1982\n      7.909502e+07\n      1.000281e+09\n      62.617939\n      77.110\n    \n    \n      1987\n      8.700669e+07\n      1.084035e+09\n      64.851182\n      78.670\n    \n    \n      1992\n      9.494825e+07\n      1.164970e+09\n      66.537212\n      79.360\n    \n    \n      1997\n      1.025238e+08\n      1.230075e+09\n      68.020515\n      80.690\n    \n    \n      2002\n      1.091455e+08\n      1.280400e+09\n      69.233879\n      82.000\n    \n    \n      2007\n      1.155138e+08\n      1.318683e+09\n      70.728485\n      82.603\n    \n    \n      Europe\n      1952\n      1.393736e+07\n      6.914595e+07\n      64.408500\n      72.670\n    \n    \n      1957\n      1.459635e+07\n      7.101907e+07\n      66.703067\n      73.470\n    \n    \n      1962\n      1.534517e+07\n      7.373912e+07\n      68.539233\n      73.680\n    \n    \n      1967\n      1.603930e+07\n      7.636845e+07\n      69.737600\n      74.160\n    \n    \n      1972\n      1.668784e+07\n      7.871709e+07\n      70.775033\n      74.720\n    \n    \n      1977\n      1.723882e+07\n      7.816077e+07\n      71.937767\n      76.110\n    \n    \n      1982\n      1.770890e+07\n      7.833527e+07\n      72.806400\n      76.990\n    \n    \n      1987\n      1.810314e+07\n      7.771830e+07\n      73.642167\n      77.410\n    \n    \n      1992\n      1.860476e+07\n      8.059776e+07\n      74.440100\n      78.770\n    \n    \n      1997\n      1.896480e+07\n      8.201107e+07\n      75.505167\n      79.390\n    \n    \n      2002\n      1.927413e+07\n      8.235067e+07\n      76.700600\n      80.620\n    \n    \n      2007\n      1.953662e+07\n      8.240100e+07\n      77.648600\n      81.757\n    \n    \n      Oceania\n      1952\n      5.343003e+06\n      8.691212e+06\n      69.255000\n      69.390\n    \n    \n      1957\n      5.970988e+06\n      9.712569e+06\n      70.295000\n      70.330\n    \n    \n      1962\n      6.641759e+06\n      1.079497e+07\n      71.085000\n      71.240\n    \n    \n      1967\n      7.300207e+06\n      1.187226e+07\n      71.310000\n      71.520\n    \n    \n      1972\n      8.053050e+06\n      1.317700e+07\n      71.910000\n      71.930\n    \n    \n      1977\n      8.619500e+06\n      1.407410e+07\n      72.855000\n      73.490\n    \n    \n      1982\n      9.197425e+06\n      1.518420e+07\n      74.290000\n      74.740\n    \n    \n      1987\n      9.787208e+06\n      1.625725e+07\n      75.320000\n      76.320\n    \n    \n      1992\n      1.045983e+07\n      1.748198e+07\n      76.945000\n      77.560\n    \n    \n      1997\n      1.112072e+07\n      1.856524e+07\n      78.190000\n      78.830\n    \n    \n      2002\n      1.172741e+07\n      1.954679e+07\n      79.740000\n      80.370\n    \n    \n      2007\n      1.227497e+07\n      2.043418e+07\n      80.719500\n      81.235\n    \n  \n\n\n\n\n\n\n\n\nBy now you should be able to select the rows and columns, join together dataframes, and create some basic summarizations of data. The next section will look at some of the more sophisticated and elegant tools for understanding and presenting your data. But the building blocks you have just learned about are the meat and potatoes of data analysis in the python world."
  },
  {
    "objectID": "32_graphical_depictions_of_data.html#introduction-to-matplotlib",
    "href": "32_graphical_depictions_of_data.html#introduction-to-matplotlib",
    "title": "5  Graphical Depictions of Data",
    "section": "5.1 Introduction to matplotlib",
    "text": "5.1 Introduction to matplotlib\nmatplotlib is the most widely used scientific plotting library in Python. For many data related purposes, the sub-library called matplotlib.pyplot is all that is necessary to use. A fantastic feature of the Jupyter environments is that anytime we create a plot, we can view it directly inline with our code, allowing us to make adjustments quickly and easily as we go along.\nWe will typically import this library with the name plt:\n\nimport matplotlib.pyplot as plt\n\nSimple plots are then straightforward to create:\n\nx = [1, 2, 3, 4, 5]\ny = [1, 8, 27, 64, 125]\n\nplt.plot(x, y)\n\n\n\n\n\n\n\n\n\n\nplt.show()\n\n\n\nIn Jupyter environments, running the cell that produces a plot will generate the figure directly below the code, and then the figure is saved with the notebook document for future viewing. However, other Python environments, such as an interactive Python session started from a terminal, or a Python script executed at the command line or within VSCode, require an additional command to display the figure. To do this, we use the basic call:\nplt.show()\nDirectly after creating our plot using plt.plot(...).\nThis command can also be used within a Notebook - for instance to display multiple figures if they are created within a single cell:\n\n# create some lists of data\nx = [1, 2, 3, 4, 5]\ny1 = [2, 4, 6, 8, 10] \ny2 = [1, 4, 9, 16, 25]\n\n# plot and display x vs y1\nplt.plot(x, y1)\nplt.show()\n\n# plot and display x vs y2\nplt.plot(x, y2)\nplt.show()\n\n\n\n\n\n\n\nWe recommend as accepted practice to always include a call to plt.show() if you ever intend to show a plot, in a notebook or otherwise.\n\n\nPlots typically require some set of values to supply to the x value, and an equal length set of values to supply to the y value. If we supply mismatched data, we will get an error:\n\nx = [1, 2, 3, 4, 5]\ny = [1, 8, 27, 64]\nplt.plot(x, y)\n\nValueError: x and y must have same first dimension, but have shapes (5,) and (4,)\n\n\n\n\n\nIn the above examples, we used lists of numbers to supply for our dataset. However, we can also use values directly from pandas dataframes. So let’s work on a new dataset in a pandas dataframe. We will load this dataset from the Seaborn package. Seaborn is another plotting library which we will learn how to use in subsequent sections - but it also has a great built-in dataset that we are going to use to demonstrate matplotlib now, and seaborn later. This dataset looks at species of penguins, and compares various anatomical body part sizes to their species, location and sex.\n\n\n\nload_dataset('penguins')\n\n\n\nfrom seaborn import load_dataset\n\npenguins = load_dataset('penguins')\npenguins.sample(10)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      29\n      Adelie\n      Biscoe\n      40.5\n      18.9\n      180.0\n      3950.0\n      Male\n    \n    \n      135\n      Adelie\n      Dream\n      41.1\n      17.5\n      190.0\n      3900.0\n      Male\n    \n    \n      38\n      Adelie\n      Dream\n      37.6\n      19.3\n      181.0\n      3300.0\n      Female\n    \n    \n      233\n      Gentoo\n      Biscoe\n      48.4\n      14.6\n      213.0\n      5850.0\n      Male\n    \n    \n      152\n      Chinstrap\n      Dream\n      46.5\n      17.9\n      192.0\n      3500.0\n      Female\n    \n    \n      43\n      Adelie\n      Dream\n      44.1\n      19.7\n      196.0\n      4400.0\n      Male\n    \n    \n      265\n      Gentoo\n      Biscoe\n      43.6\n      13.9\n      217.0\n      4900.0\n      Female\n    \n    \n      116\n      Adelie\n      Torgersen\n      38.6\n      17.0\n      188.0\n      2900.0\n      Female\n    \n    \n      114\n      Adelie\n      Biscoe\n      39.6\n      20.7\n      191.0\n      3900.0\n      Female\n    \n    \n      22\n      Adelie\n      Biscoe\n      35.9\n      19.2\n      189.0\n      3800.0\n      Female\n    \n  \n\n\n\n\nLet’s try comparing the length of the penguin bill to the depth of the bill.\n\nplt.plot(penguins['bill_length_mm'], penguins['bill_depth_mm'])\nplt.show()\n\n\n\n\nOkay, that doesn’t look great. That is because the default behaviour of plot is to draw a line plot, which connects all the data points in the order in which they are given. For this type of comparison, a different matplotlib plot may work better. Some of the basic plots include:\n\nplt.plot(x, y) - produces a line plot of x versus y\nplt.scatter(x ,y) - produces a scatter plot of x versus y\nplt.bar(x, height) - produces a bar plot with bars of height ‘height’ positioned at x. Typically reserved for aggregated data!\nplt.hist(x) - produces a simple box histogram for a single column of data\n\nLet’s try to look at the bills again, but this time with a more appropriate comparison plot:\n\nplt.scatter(penguins['bill_length_mm'], penguins['bill_depth_mm'])\nplt.show()\n\n\n\n\nThat’s looking better. But we still need to clean up our plot a little bit. Matplotlib includes methods for adding axis labels, titles, legends and so on. The trick is in figuring out how to apply these methods to the right plot…\n\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.title('Penguin Beaks')\nplt.show()\n\n\n\n\nWhat happened here! Matplotlib drew an axis for us, but neglected to include the data we had provided above. The key to this is in the way that matplotlib interprets the current figure. Anytime we begin a plot method with plt.method()..., matplotlib recognizes this plot instruction to be part of the current figure. Matplotlib will then go through all of the plot instructions one-by-one, adding each individual piece to the current figure. This continues until we hit the plt.show() line, which tells the program we are done adding to this plot, and any new commands should belong to a new plot.\n\n\n\n\n\n\nJupyter vs. Python\n\n\n\nThere is a slight distinction between how Jupyter will handle figures vs. how a Python script wil handle figures. When Jupyter hits the end of a cell or codeblock, it will automatically show the figure at the end of the output, regardless of whether plt.show() was called or not. This is equivalent to calling plt.show() however - a new codeblock will not recognize the code used to create this plot in a new cell.\n\n\nLet’s finally get this all together on one plot:\n\nplt.scatter(penguins['bill_length_mm'], penguins['bill_depth_mm'])\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.title('Penguin Beaks')\nplt.show()\n\n\n\n\nNote that because of this matplotlib behaviour (include every instruction to plt up until it reaches a plt.show() or end of Jupyter cell), we can take advantage to draw multiple plots on a single axis! Some further behaviour includes:\n\nIf a second plot of the same type as the first is created within the same figure, matplotlib will automatically assign it a new colour\nTo distinguish between plots, the argument label = 'plot name' can be provided during creation of the plot. A final call to plt.legend() after creating all plots will produce a legend that consists of the given labels.\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nUsing what we have learned so far about pandas filtering and matplotlib functionality to produce a plot of bill lengths vs. depths, but colours the three penguin species (Adelie, Chinstrap, and Gentoo) different colours. Include a legend that identifies which species is which colour!\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nWe first split up our data into three separate pieces, each filtered to a single species of penguin. Next we plot all three species without displaying the figure. Finally, we add some tidy up code to add the legend and axis titles, and we are good to go!\nOne thing to note is that it is often good practice to use more than just colour to distinguish between different groups of data. Because of this, adding a variation on the marker style is a good idea as well. We have included that in the solution to this challenge. Be sure to explore all the other style settings available within each matplotlib plot!\n\nadelie = penguins[penguins.species == 'Adelie']\nchinstrap = penguins[penguins.species == 'Chinstrap']\ngentoo = penguins[penguins.species == 'Gentoo']\n\n# start creating our plot, individualizing each plot as necessary\nplt.scatter(adelie.bill_length_mm, adelie.bill_depth_mm, marker='o', label='Adelie')\nplt.scatter(chinstrap.bill_length_mm, chinstrap.bill_depth_mm, marker='x', label='Chinstrap')\nplt.scatter(gentoo.bill_length_mm, gentoo.bill_depth_mm, marker='^', label='Gentoo')\n\n# apply overall plot commands after\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.title('Penguin Beaks')\nplt.legend()\n\n# show our plot\nplt.show()"
  },
  {
    "objectID": "32_graphical_depictions_of_data.html#plotting-directly-from-pandas",
    "href": "32_graphical_depictions_of_data.html#plotting-directly-from-pandas",
    "title": "5  Graphical Depictions of Data",
    "section": "5.2 Plotting Directly From Pandas",
    "text": "5.2 Plotting Directly From Pandas\nThat was a lot of work to make a relatively straightforward plot. While matplotlib is extremely versatile, this same versatility comes at the expense of having to tweak the underlying code frequently to make the plots look the way that we want. Luckily, there exist other libraries that have been built on top of matplotlib that take care of much of the abstraction, so we can get directly to looking at our results as fast as possible!\nOne extremely straightforward way to plot is directly from pandas, which implicitly uses matplotlib.pyplot in the background.\n\npenguins.plot.scatter(x = 'bill_length_mm', y = 'bill_depth_mm')\nplt.show()\n\n\n\n\nFor some plot types, we are also given the ability to provide a grouping category.\n\npenguins.plot.hist(column='flipper_length_mm', by='sex')\nplt.show()\n\n\n\n\nWhile pandas provides a straightforward way to quickly look at the data, it is fairly limited, and we will still end up needing to default back to matplotlib methods to get high-quality plots. The next package will help with this!"
  },
  {
    "objectID": "32_graphical_depictions_of_data.html#introduction-to-seaborn",
    "href": "32_graphical_depictions_of_data.html#introduction-to-seaborn",
    "title": "5  Graphical Depictions of Data",
    "section": "5.3 Introduction to Seaborn",
    "text": "5.3 Introduction to Seaborn\nSeaborn is a library for making statistical graphics in Python. It builds on top of matplotlib but is uniquely built to integrate closely with pandas data structures. Seaborn is dataset-oriented, and was built specifically to let you focus on what the different elements of a plot mean, rather than on the details of how to draw it. Leveraging seaborn allows us to build high quality graphics in a minimal amount of code. This not only allows us to spend more time analysing outputs and results, but also makes the code more transparent and easier for others to follow and understand.\nSeaborn is usually imported with the shorthand sns. On top of this, it is often nice to reset the default matplotlib colour theme and layout to the default seaborn layout, which has more subdued colours and tones. This can be done globally via the .set_theme() method. Changing the theme will apply to any other plots created in a notebook/script, even if they are created specifically by matplotlib. More options for the theme or style (which can also be set via .set_style()) can be found in the Seaborn aesthetics tutorial.\n\nimport seaborn as sns\nsns.set_theme()\n\nWithin seaborn, we will focus on three main plot types:\n\nRelational plots\nDistributions\nCategorical plots\n\nAll three of these modules have a higher figure-level interface (relplot, displot, catplot) that have options to produce the different subvarieties. They also have an axes-level interface (for example, histplot inside the displot category) that allows for more control over the matplotlib backend being used to produce the plot. For our purposes, we will stick to the high level interface as it is capable of producing beautiful plots with a few simple commands.\n\n\n\nSeaborn plot categories.\n\n\n\n\n\n\n\n\nAdvanced Tip - Figure vs. Axes Level?\n\n\n\n\n\n\n\n\nMatplotlib Figures\n\n\nYou will notice that I mentioned figure vs. axes level interfaces for seaborn. This is a small but subtle distinction that is worth explaining in more detail. When we introduced matplotlib, we explained that you can create multiple different plots all on the same figure. This is because matplotlib plots are individually drawn onto a common axis, but each individual plot does not own the axis, titles, legends, and so on. Matplotlib produces a base axis, then draws each individual plot on top, and then adds the extra pieces as requested. All the accompanying fluff (title, legend) that surrounds the axis is part of the overall figure, which we set with commands such as plt.xlabel(), plt.title() and so on.\nSo a figure can be thought of as the overall container holding each plot and accompanying axis labels. In seaborn, the figure-level interface we will be using is just this - figure-level. It is a ‘finished product.’ Producing a plot via the relplot, displot, and catplot methods will produce an entire figure, complete with titles, legends, and anything else we might specify, such as the figure size. However, this entire figure is no longer easily accessible like a matplotlib axis is (we cannot add a plot like this to a different set of matplotlib plots, for example).\nThis can be useful as it will simplify the amount of code needed to create a report-ready figure. However, the drawback is that it reduces the customizability we have to work with. So if we wish to work with customizable axes that we can quickly drop into more complex matplotlib plots, we should use the axes-level interface (eg. using sns.scatterplot() instead of sns.relplot()). If we want to produce standard statistical plots, the figure-level interface is the recommended tool as they produce cleaner plots.\nCheck out the Seaborn tutorial for more details on the differences between these methods!\n\n\n\nBefore we dive into the particulars, here’s a wonderful one line command to show just how powerful seaborn can be!\n\nsns.pairplot(penguins, hue='species', height=1.75)\nplt.show()\n\n\n\n\n\nRelational Plots\n\n \n\nCorrelation \\(\\neq\\) Causation\n\n\n\nRelational plots let us quickly look for patterns in our dataset between two different features. The most common approach is to use scatter or lineplots, depending on the type of data we have. A timeseries, for example, will often be displayed with a lineplot, while any two numerical features can be plotted against each other with a scatterplot.\nWhen we use the high-level relplot method, we need to supply our pandas dataframe, as well as which columns we are interested in plotting. The default plot type is to use a scatterplot:\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm')\nplt.show()\n\n\n\n\nHowever, we can force the line plot by including the optional kind argument:\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', kind='line')\nplt.show()\n\n\n\n\nWhen we utilize the lineplot option, Seaborn will default to automatically aggregating data along the x-axis, displaying uncertainty in the y-axis with confidence intervals. Note that it also doesn’t care about the initial order of the data. Seaborn automatically assumes that we wanted a nicely flowing line from left to right, and will sort the x values accordingly.\n\n\n\n\n\n\nChallenge 2\n\n\n\nUsing the Seaborn load_dataset method, import a dataset called ‘dowjones’. Answer the following questions:\n\nWhat does this dataset include/represent?\nOn what date does the data represented here reach its highest price?\nCreate a line plot to depict the data. Does the plot agree with the answer you came up with in part 2?\n\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\n\nLet’s get our dataset first and take a quick peek\n\n\ndowjones = load_dataset('dowjones')\ndowjones.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n    \n  \n  \n    \n      0\n      1914-12-01\n      55.00\n    \n    \n      1\n      1915-01-01\n      56.55\n    \n    \n      2\n      1915-02-01\n      56.00\n    \n    \n      3\n      1915-03-01\n      58.30\n    \n    \n      4\n      1915-04-01\n      66.45\n    \n  \n\n\n\n\nIt looks like this is a list of dates and prices. Context clues from the name of the dataset hint that this is probably the Dow Jones Industrial Average. From the looks of the Date column, it is probably a monthly average price!\n\nWe could do this one of two ways. We could sort the dataframe by price and then look at the first data in the sorted dataframe. Alternatively, we can use aggregation and filtering. Note that because we do not want to get statistics per varying group, we do not need to do any groupby() before looking for the max value.\n\n\n# option 1: sort and grab first row\ndate_of_max = dowjones.sort_values(by='Price', ascending=False).head(1)\ndisplay(date_of_max)\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n    \n  \n  \n    \n      613\n      1966-01-01\n      985.93\n    \n  \n\n\n\n\n\n# option 2: aggregate and filter \n# determine the max price in the dataset\nmax_price = dowjones['Price'].max()\n\n# use this max price to filter to the date it occured on\ndate_of_max = dowjones[dowjones['Price']==max_price]\n\n# check out the result \ndisplay(date_of_max)\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n    \n  \n  \n    \n      613\n      1966-01-01\n      985.93\n    \n  \n\n\n\n\n\nWe will use Seaborn to build our lineplot.\n\n\nimport datetime as dt\nsns.relplot(\n    data=dowjones,\n    x='Date',\n    y='Price',\n    kind='line'\n)\nplt.show()\n\n\n\n\nIt does indeed look like the plot is peaking around 1966, which matches what we found earlier! It also looks like you can see the stock market crash that started the Great Depression…\n\n\n\nAlthough relational plots are two dimensional in their presentation, we can add a third dimension to the data by applying the hue, size, and style arguments to secondary columns.\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', hue='species')\nplt.show()\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', style='sex')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWhen used on a non-categorical column, the hue and size arguments will provide sequential coloring and sizing:\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', hue='bill_length_mm')\nplt.show()\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', size='bill_length_mm')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can also mix and match multiple different styling choices. Note that here, we want to be cautious about creating our plots using too many different styling choices. While informative, they can be tricky to interpret if we use too many all at once.\n\nsns.relplot(\n    data=penguins, x='body_mass_g', y='flipper_length_mm', \n    hue='species', style='species'\n    )\nplt.show()\n\nsns.relplot(\n    data=penguins, x='body_mass_g', y='flipper_length_mm', \n    hue='sex', size='bill_length_mm'\n    )\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReminder: Use Multiple Lines!\n\n\n\nIn that last example you’ll notice that I broke the code up for the plot onto multiple lines. When you start to have many arguments being supplied to a function, it is often a good idea to put different arguments on different lines for readability. As long as argument stays with the brackets, you can use as many lines or indentations as you want!\n\n\nIf we want to avoid using too many different styling choices, but we would still like to understand how our features vary across different categories, we can make multiple plots using the built in row and col arguments of the relplot objects. By supplying a category to row (col), Seaborn is told to split the dataset into all the different values in the category. Then each subset of the data is plotted on a different row (column) of the data. The end result is a grid of plots, with each plot having a unique subset of the category represented.\n\ng = sns.relplot(\n    data=penguins, x='body_mass_g', y='flipper_length_mm',\n    row='island', col='species', hue='sex',\n    height=2.5\n)\ng.set_titles(size=8)  # include this line because the titles will overlap otherwise!\nplt.show()\n\n\n\n\nFinally, a frequent goal of creating a scatter plot is to identify if there is a trend in the data. Seaborn offers an extra method, lmplot, to allow for quick viewing of best fit lines (including confidence interval estimates) in our datasets:\n\nsns.lmplot(data=penguins, x='body_mass_g', y='flipper_length_mm', hue='sex')\nplt.show()\n\n\n\n\nNote that because we included a third categorical dimension (hue='sex'), we have actually displayed two different best fit lines: the best relationship between body mass and flipper length for male penguins, and the best relationship for female penguins as well.\nThis method is capable of doing more than the default linear regression! Some such arguments available include:\n\nlogistic: boolean. If True, will estimate a logistic regression.\nrobust: boolean. If True, will de-weight far outliers in performing the regression.\norder: integer. If supplied, will fit a polynomial regression with the given order.\n\n\nSummary\nThat is a lot to take in. To summarize the relational plots available in Seaborn:\n\nrelplot is the go-to tool to create a statistical comparison plot between two numerical columns\nThese plots default to scatter, but can be made into lineplots with kind='line'\nstyle, hue, and size can all be adjusted to provide insight into both numerical and categorical features\nrow and col will split our plot out into multiple facets, according to the categories found in the datasets\nlmplot can be used to provide basic regression fits to the data\n\n\n\n\n\n\n\nChallenge 3\n\n\n\nChoose any two numerical features in the penguins dataset. Produce a scatter plot that highlights the differences between species. Separate the results into two plots - one for male and one for female penguins.\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nYou might produce code that looks similar to this. col and row could be interchanged, and maybe you chose to use just one of style or hue. Explore your options!\n\ng = sns.relplot(\n    data=penguins, x='flipper_length_mm', y='bill_length_mm',\n    hue='species', style='species', row='sex'\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDistribution Plots\n\n\n\nSpoooky\n\n\nAnother early step that we often take when analyzing our data is to understand how the features are distributed. We can quickly answer questions such as:\n\nWhat is the range of observations in the dataset?\nIs the data skewed?\nDo we have outliers?\nHow do different subsets compare?\n\nUsing the displot method, we can access multiple different styles of plots to answer these questions. While Seaborn can (and does) get into multivariate distributions, let us stick to univariate distributions: histograms and kernel density estimation (KDE).\nThe default displot option will produce a histogram of whichever column we choose in our dataset:\n\nsns.displot(data=penguins, x='flipper_length_mm')\nplt.show()\n\n\n\n\nSeaborn automatically chooses what it feels is a reasonable number of bins for the dataset, but this is of course customizable via either the binwidth or the bins arguments, which will force the width or number of bins, respectively.\n\nsns.displot(data=penguins, x='flipper_length_mm', binwidth=1)\nplt.show()\n\nsns.displot(data=penguins, x='flipper_length_mm', bins=5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nJust like with our relational plots, we can also condition our histograms on other features, colouring each member of the category separately.\n\nsns.displot(data=penguins, x='flipper_length_mm', hue='sex')\nplt.show()\n\n\n\n\nThe default here is to create an overlapping histogram, but we could create stacked histograms, dodged (side-by-side) histograms, or even use the col or row options to produce multiple plots!\n\nsns.displot(data=penguins, x='flipper_length_mm', hue='sex', multiple='stack')\nplt.show()\n\nsns.displot(data=penguins, x='flipper_length_mm', hue='sex', multiple='dodge')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nsns.displot(data=penguins, x='flipper_length_mm', col='sex', height=3.5)\nplt.show()\n\n\n\n\nWhile the histogram option provides the exact count of the underlying values contained in our dataset, sometimes we may wish to approximate the distribution of data. This is done using kernel density estimation, which plots a smooth, continuous density estimate. Just like with the relational plots, we can access this new plot type using the kind='kde' option.\n\nsns.displot(data=penguins, x='flipper_length_mm', kind='kde')\nplt.show()\n\n\n\n\nJust like we were able to adjust the bin sizes for the histogram, we can adjust the ‘bandwidth’ of our estimation. This will vary the amount of smoothing that is applied in the end distribution:\n\nsns.displot(data=penguins, x='flipper_length_mm', kind='kde', bw_adjust=0.25)\nplt.show()\n\nsns.displot(data=penguins, x='flipper_length_mm', kind='kde', bw_adjust=4)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFinally, we can have both types of plots in action at the same time by including the optional kde argument:\n\nsns.displot(data=penguins, x='flipper_length_mm', kde=True, hue='species')\nplt.show()\n\n\n\n\n\nSummary\nTo summarize distributions available in Seaborn:\n\ndisplot is the go-to tool to create a statistical comparison plot between two numerical columns\nThese plots default to histograms, but other options are available:\n\nkde will produce an estimation of the underlying density\necdf will produce a cumulative distribution function\n\nA shortcut to include both a histogram and a KDE can be used by setting the argument kde=True\nhue can be adjusted to provide insight into extra categorical features\nrow and col will split our plot out into multiple facets, according to the categories found in the datasets\n\n\n\n\n\n\n\nChallenge 4\n\n\n\nCreate a KDE plot of body mass. Does it appear to be bi-modal (2 peaks)? If so, create another plot (or more) to identify what may be the cause. Are male penguins heavier? Is it a specific species? Does the island matter?\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nFirst let’s just look at the body mass variable alone.\n\nsns.displot(\n    data=penguins, x='body_mass_g', kind='kde'\n    )\nplt.show()\n\n\n\n\nHmm.. it’s not immediately obvious that there might be two peaks here, but it does seem to skew a little bit to lower values. Let’s see if looking at the different sexes gives us any more information:\n\nsns.displot(\n    data=penguins, x='body_mass_g', kind='kde', hue='sex',\n    fill=True  # Use fill to fill the KDE plot!\n    )\nplt.show()\n\n\n\n\nOkay, weird. Splitting by sex does start to show two peaks, but it shows up in both male and female! So maybe it’s the species where the peak is showing up?\n\nsns.displot(\n    data=penguins, x='body_mass_g', kind='kde', hue='species', col='sex',\n    fill=True \n    )\nplt.show()\n\n\n\n\nAha - looks like we found a key difference here! While males tend to be heavier than the females on average, there are distinctly different distributions for the three species, with the Gentoo penguins being heavier than both the Adelie and Chinstrap. Doing this sort of exploration via plot is always a useful tool to learn more about our data!\n\n\n\n\n\n\nCategorical Plots\n\n\n\nSuspicious…\n\n\nRelational plots let us quickly view relationships between two sets of numerical features. Categorical plots will allow us to do the same where one (or both) of the features is categorical (divided into discrete groups).\nThe catplot method will provide us a unified high-level interface to a variety of different plots. The default behaviour of catplot is to produce a non-aggregated view of the categories in a strip plot.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm')\nplt.show()\n\n\n\n\nThis is a categorical version of a scatter plot, in which all of the data for a single category is plotted at the same horizontal position, but with a small amount of horizontal ‘jitter’ added so that data points with identical numerical values do not completely overlap. This gives us a general idea of how the data is distributed within a category: do most of the penguins have similar flipper lengths within a species, or do they spread out to cover a wide range?\nThe most commonly used categorical plot however, is the bar plot. Bar plots are something that we are likely all familiar with, as human beings living in a society of.. people. However, typically when you see a bar plot, it is representing some sort of aggregation of our data. What is the average flipper length of the various penguin species? How many penguins live on each island? As the data stands, we do not have this information directly - it is non-aggregated, row level data. We could use our skills with Pandas to group the data according to the categories of interest and apply some statistic measures to the numerical columns of interest, and then use a matplotlib or Seaborn plot to display the results of that aggregation. However, Seaborn has developed tools that allow us to skip the grouping steps ourselves and allow the plotting package to do the grouping and statistical analysis behind the scenes. This has two advantages:\n\nThe code is shorter and more precise, leading to easier understanding for others. This helps with code transparency.\nSeaborn also provides confidence intervals to include in its aggregated bar plots, which would be an extra level of complexity added to a manual grouping of the data.\n\nIn Seaborn, a barplot operates on the full dataset to obtain an estimate of some aggregation function (which is the mean by default). It will also default to providing an estimation of the confidence interval on its estimates. Just like with all of the other Seaborn plots, we can add an additional feature for free via the hue styling choice.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex')\nplt.show()\n\n\n\n\nIf we wish to summarize a different aggregate statistic, there are built in options: estimator='mean', 'median','min', and 'max' will all work.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex', estimator='max')\nplt.show()\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex', estimator='min')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Tip - Callable Estimators!\n\n\n\n\n\nIn addition to the built in estimators, we can also use estimators for external functions. The function is required to take as an input a vector of values, and output a single value that summarizes that vector. The Python library numpy is the go-to resource for mathematical functions in Python. If we use numpy, we can send any sort of estimator to the barplot! Common statistical functions in numpy include sum, prod, mean, std, and var.\n\nimport numpy as np\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex', estimator=np.sum)\nplt.show()\n\n\n\n\n\n\n\nOne last unique piece that you might be interested in is the orientation of the plot. In the above examples, we gave our categorical category to the x feature, and y held the numerical values. If we wish to have a horizontal set of bars instead, we can simply swap these:\n\nsns.catplot(\n    data=penguins, x='flipper_length_mm', y='species',\n    kind='bar', hue='sex')\nplt.show()\n\n\n\n\n\nSummary\nTo summarize the categorical plots available in Seaborn:\n\ncatplot is the go-to tool to create a statistical comparison plot between a numerical and categorical column\nThese plots default to categorical swarmplots (no, I do not know why bar is not the default) but other options are available. These options can be organized into three broad subcategories.\n\nCategorical scatterplots:\n\nswarm\nstrip\n\nCategorical distribution plots:\n\nbox\nviolin\nboxen\n\nCategorical estimate plots:\n\npoint\nbar\ncount\n\n\nhue can be adjusted to provide insight into extra categorical features\nrow and col will split our plot out into multiple facets, according to the categories found in the datasets\nBy swapping the x and y variables, we can choose which orientation the plot will display in\n\n\n\n\n\n\n\nChallenge 5\n\n\n\nWhich penguin species was most frequently included in this dataset? Display your result visually!\n\n\n\n\n\n\n\n\nSolution to Challenge 5\n\n\n\n\n\nTo get the count for each of the species, we need to make a countplot:\n\nsns.catplot(data=penguins, x='species', kind='count')\nplt.show()\n\n\n\n\nDone, in one line of code!\nAs a bonus, if we want to plot the data in descending order, we can pass a list of strings containing the categories to the catplot method:\n\nsns.catplot(\n    data=penguins, x='species', \n    kind='count', \n    order=['Adelie', 'Gentoo', 'Chinstrap']\n    )\nplt.show()"
  },
  {
    "objectID": "32_graphical_depictions_of_data.html#saving-your-plots",
    "href": "32_graphical_depictions_of_data.html#saving-your-plots",
    "title": "5  Graphical Depictions of Data",
    "section": "5.4 Saving your Plots",
    "text": "5.4 Saving your Plots\nWe’ve now done all this work to make amazing incredible charts that produce stunning visuals for our datasets. Chances are, however that you do not want these images to live solely in your Jupyter notebook. You might want it for a presentation or a report. So, we need to save these images so we can access them outside of our Python environment. To do this we use the plt.savefig() method. This method will save the currently open figure. This is the figure that has not yet been shown on screen! Any call to save a plot must come before we display it via plt.show(), as this actually closes the figure/plotting device so we can start our next one!\n\n\n\n\n\n\nNo Show and Tell!\n\n\n\nThis point is important enough that I am repeating it here. Always save your figure before showing it!\n\n\nLet’s create a plot, view it, and save it. We will save it to the same outputs folder we used in a prior section.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex'\n    )\nplt.savefig('../outputs/penguins.png', bbox_inches='tight')\nplt.show()\n\n\n\n\nYou will notice that I used an optional argument here called bbox_inches. This is entirely optional, but when used it will cut out any empty whitespace on the sides of the figure before saving. I find it makes for cleaner looking images at the end, so I will often use this argument. And that’s it, we now have our image saved and ready for use elsewhere!\n\n\n\n\n\n\nAdvanced Tip - Changing the Figure Size\n\n\n\n\n\nWe might want to vary the shape or size of the plot we are producing. There are two ways to do this, depending on the type of figure we have created.\n\nAxes-Level (Matplotlib Style) Figures\nIf we are working directly with matplotlib or the axes-level seaborn functions, this is done by initializing the figure with a figsize command before creating any plot elements:\nplt.figure(figsize=(10, 8))\nplt.plot(...)\nsns.scatterplot(...)\nplt.savefig()\nplt.show()\nThe figsize argument inside the plt.figure method specifies the required width and height in inches.\n\n\nFigure-Level (Seaborn Style) Figures\nIf we are working with the figure-level Seaborn interface, we cannot access the figure attributes externally. However, Seaborn has given us arguments we can pass directly to the plot method itself to set the figure size via the height and aspect arguments.\nsns.relplot(data=..., height=5, aspect=0.5)\nplt.savefig()\nplt.show()\nThe height argument indicates the required height of each subplot in inches, while the aspect argument supplies the aspect ratio such that the width = aspect * height. Note that height applies to every individual subplot in the plot, not the overall image!"
  },
  {
    "objectID": "32_graphical_depictions_of_data.html#online-galleries",
    "href": "32_graphical_depictions_of_data.html#online-galleries",
    "title": "5  Graphical Depictions of Data",
    "section": "5.5 Online Galleries",
    "text": "5.5 Online Galleries\nAll of the tools described above are a great way to start visualizing your data, but there are many other libraries and tools available. If you ever need inspiration for a new visual, or simply need a reminder on what code you would need to create a specific chart, the Python Graph Gallery is a fantastic resource. It contains examples as well as code for over 40 different types of charts using Python code."
  },
  {
    "objectID": "30_bcdata.html#installing",
    "href": "30_bcdata.html#installing",
    "title": "6  Introduction to bcdata",
    "section": "6.1 Installing",
    "text": "6.1 Installing\nUnlike the rest of the packages we have downloaded (using conda commands), the bcdata package is not available for download via conda (this is often the case for smaller packages that are not widely distributed globally). This is one of those instances where we need to mix our use of pip and conda. However, the installation process proceeds in the same fashion as before. These commands can be done from an anaconda prompt, or from the command prompt (cmd) utility inside VS Code.\n\n\nAnaconda Prompt\n\n> conda activate ds-env\n> pip install bcdata\n\nAs before, make sure that you are installing this package in the ds-env environment! To make sure that this has successfully downloaded, we can try importing it into our Python session.\n\nimport bcdata\nbcdata.__version__\n\n'0.7.4'"
  },
  {
    "objectID": "30_bcdata.html#basic-usage",
    "href": "30_bcdata.html#basic-usage",
    "title": "6  Introduction to bcdata",
    "section": "6.2 Basic Usage",
    "text": "6.2 Basic Usage\n\nIntroduction to the Catalogue\nTo use the bcdata package, we will need to familiarize ourselves with the B.C. Data Catalogue. If we enter a search term in the catalogue, it will bring up a variety of data sets that might meet our needs. However, the python bcdata package is only for geospatial datsets. These will typically be data types with a resource storage format (one of the filter options) that matches:\n\narcgis_rest\nkml\ngeojson\nmultiple\nwms\n\nIf we filter to only include these options for the format, we will find some geospatial datasets. Let’s try looking for a list of the locations of all B.C. hospitals.\n\n\n\nB.C. Hospitals\n\n\nThe first search result here, BC Health Care Facilities (Hospital), looks like it might be what we are looking for. If we click on it, we see that there are options for geographic downloads on the right hand side of the page. This is good - it means that we have found a geographic dataset!\n\n\n\nGeographic Download Options\n\n\nIf we now click on the View button beside the BC Geographic Warehouse Custom Download resource, we will get some more information about the dataset. This includes details such as how frequently the dataset is refreshed, when it was last refreshed, the type of geometry used, the details of the columns included in the data and more.\nAt this point in a ‘pre-programming’ world, we would have to request to access/download the dataset, wait for the file to arrive, store it locally in a folder we hopefully don’t misplace, and then find some tool capable of viewing the data. While each of these steps individually is not terribly difficult, putting them altogther, and documenting the entire process can be prohibitive and mistake prone. What if we forget where we stored the file? Or forget which file we were using when we need to refresh the dataset? When datasets are acquired manually like this, having a reproducible workflow becomes so much more difficult!\nIn a python world, all of these steps can be done directly within a single script! This essentially guarantees the reproducibility and transparency of the workflow.\n\n\nGetting the Data into Python\nOkay, we have a geospatial dataset in mind, now what? First, we will call on the bcdata package to import our chosen dataset into python. To do this, we need to use the get_data() method, and use 2 arguments:\n\ndataset: this is the name of the dataset we wish to pull from the catalog. This can be either the id or Object Name:\n\nid: last portion of the URL - in this case ‘bc-health-care-facilities-hospital’\nObject Name: under Object Description - in this case ‘WHSE_IMAGERY_AND_BASE_MAPS.GSR_HOSPITALS_SVW’\n\nas_gdf: a boolean argument that specifies to load the data into a geopandas DataFrame (a specialized pandas dataframe capable of holding geospatial data)\n\nLet’s go ahead and use these arguments to pull the dataset into Python!\n\nhospitals = bcdata.get_data(\n    dataset='bc-health-care-facilities-hospital', \n    as_gdf=True\n    )\nhospitals.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      CUSTODIAN_ORG_DESCRIPTION\n      BUSINESS_CATEGORY_CLASS\n      BUSINESS_CATEGORY_DESCRIPTION\n      OCCUPANT_TYPE_DESCRIPTION\n      SOURCE_DATA_ID\n      SUPPLIED_SOURCE_ID_IND\n      OCCUPANT_NAME\n      DESCRIPTION\n      PHYSICAL_ADDRESS\n      ...\n      SITE_GEOCODED_IND\n      GEOCODING_METHOD_DESCRIPTION\n      HEALTH_AUTHORITY_CODE\n      HEALTH_AUTHORITY_NAME\n      HEALTH_SERVICE_DLVR_AREA_CODE\n      HEALTH_SERVICE_DLVR_AREA_NAME\n      LOCAL_HEALTH_AREA_CODE\n      LOCAL_HEALTH_AREA_NAME\n      SEQUENCE_ID\n      SE_ANNO_CAD_DATA\n    \n  \n  \n    \n      0\n      POINT (-120.81634 56.25604)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      701\n      Y\n      Fort St. John General Hospital & Peace Villa\n      Hospital\n      8407 112 Avenue , Fort St. John, BC, Peace Riv...\n      ...\n      None\n      Air Photo\n      5\n      Northern Health Authority\n      53\n      Northeast\n      60\n      Fort St. John\n      45\n      None\n    \n    \n      1\n      POINT (-121.42390 49.37725)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      606\n      Y\n      Fraser Canyon Hospital\n      Hospital\n      1275 7th Avenue, Hope, BC, Hope, BC\n      ...\n      None\n      Air Photo\n      2\n      Fraser Health Authority\n      21\n      Fraser East\n      32\n      Hope\n      47\n      None\n    \n    \n      2\n      POINT (-122.49954 52.98134)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      705\n      Y\n      G.R. Baker Memorial Hospital\n      Hospital\n      543 Front Street, Quesnel, BC, Quesnel, BC\n      ...\n      None\n      Air Photo\n      5\n      Northern Health Authority\n      52\n      Northern Interior\n      28\n      Quesnel\n      49\n      None\n    \n    \n      3\n      POINT (-116.96655 51.29723)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      409\n      Y\n      Golden and District General Hospital\n      Hospital\n      835 - 9th Avenue South, Golden, BC, Golden, BC\n      ...\n      None\n      Air Photo\n      1\n      Interior Health Authority\n      11\n      East Kootenay\n      18\n      Golden\n      51\n      None\n    \n    \n      4\n      POINT (-132.07058 53.25481)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      907\n      Y\n      Haida Gwaii Hospital and Health Centre - Xaayd...\n      Hospital\n      3209 Oceanview Drive, Queen Charlotte, BC, Que...\n      ...\n      None\n      Air Photo\n      5\n      Northern Health Authority\n      51\n      Northwest\n      50\n      Queen Charlotte\n      53\n      None\n    \n  \n\n5 rows × 33 columns\n\n\n\n\nSuccess! We have our data. Notice, in particular that we have a column called geometry. This is a special column for spatial datasets that will hold different types of geography depending on what the dataset is. In this case, we have specific locations of hospitals, so the geometry is a list of point objects that contain latitude/longitude coordinates.\nLet’s try a different dataset that will give us area geometries. Area geometries will show up as ‘(multi)polygons’ in the datset, meaning that they are a list of points that define the boundary of some area. This could be school district boundaries, health authority areas, or any other region we might be interested in. For fun, let’s look at the B.C. Wildfire Fire Zones.\n\n\n\n\n\n\nChallenge 1\n\n\n\nBring the B.C. Wildfire Fire Zones dataset into python.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\n\nfires = bcdata.get_data(\n    dataset='bc-wildfire-fire-zones',\n    as_gdf=True\n)\nfires.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      MOF_FIRE_ZONE_ID\n      MOF_FIRE_CENTRE_NAME\n      MOF_FIRE_ZONE_NAME\n      HEADQUARTERS_CITY_NAME\n      OBJECTID\n      SE_ANNO_CAD_DATA\n      FEATURE_AREA_SQM\n      FEATURE_LENGTH_M\n    \n  \n  \n    \n      0\n      POLYGON ((-118.37713 49.91304, -118.37772 49.9...\n      401\n      Southeast Fire Centre\n      Boundary Fire Zone\n      Grand Forks\n      452\n      None\n      6.590044e+09\n      4.570555e+05\n    \n    \n      1\n      MULTIPOLYGON (((-131.15968 54.00000, -131.6926...\n      402\n      Coastal Fire Centre\n      Fraser Fire Zone\n      Abbotsford\n      453\n      None\n      5.844112e+10\n      2.029064e+06\n    \n    \n      2\n      POLYGON ((-119.73636 50.20392, -119.75076 50.2...\n      403\n      Kamloops Fire Centre\n      Penticton Fire Zone\n      Penticton\n      454\n      None\n      9.302813e+09\n      7.757480e+05\n    \n    \n      3\n      POLYGON ((-121.27589 50.51785, -121.27600 50.5...\n      404\n      Kamloops Fire Centre\n      Merritt Fire Zone\n      Merritt\n      455\n      None\n      1.118309e+10\n      9.195318e+05\n    \n    \n      4\n      POLYGON ((-123.73771 50.84113, -123.73840 50.8...\n      405\n      Coastal Fire Centre\n      Pemberton Fire Zone\n      Pemberton\n      456\n      None\n      1.098514e+10\n      1.024656e+06\n    \n  \n\n\n\n\n\n\n\n\n\nPre-Filtering the Data\nIt might be the case that the data we wish to access is only a subset of the overall dataset. While we could filter the data after it comes into python, it might be faster, more efficient, or simply be less of a memory hog if we filter it before it comes into our session. In this case, we can add the optional query argument when we fetch the data. We can supply a string to this argument that will act like a simple SQL ‘where’ clause. As an example, let’s filter the hospital dataset to include only those in the Vancouver Island Health Authority:\n\nhospital_filtered = bcdata.get_data(\n    dataset='bc-health-care-facilities-hospital', \n    as_gdf=True,\n    query=\"HEALTH_AUTHORITY_NAME='Vancouver Island Health Authority'\"\n)\nhospital_filtered.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      CUSTODIAN_ORG_DESCRIPTION\n      BUSINESS_CATEGORY_CLASS\n      BUSINESS_CATEGORY_DESCRIPTION\n      OCCUPANT_TYPE_DESCRIPTION\n      SOURCE_DATA_ID\n      SUPPLIED_SOURCE_ID_IND\n      OCCUPANT_NAME\n      DESCRIPTION\n      PHYSICAL_ADDRESS\n      ...\n      SITE_GEOCODED_IND\n      GEOCODING_METHOD_DESCRIPTION\n      HEALTH_AUTHORITY_CODE\n      HEALTH_AUTHORITY_NAME\n      HEALTH_SERVICE_DLVR_AREA_CODE\n      HEALTH_SERVICE_DLVR_AREA_NAME\n      LOCAL_HEALTH_AREA_CODE\n      LOCAL_HEALTH_AREA_NAME\n      SEQUENCE_ID\n      SE_ANNO_CAD_DATA\n    \n  \n  \n    \n      0\n      POINT (-123.50867 48.86185)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      206\n      Y\n      Lady Minto Gulf Islands Hospital\n      Hospital\n      135 Crofton Road, Saltspring Island, BC, Gulf ...\n      ...\n      None\n      Air Photo\n      4\n      Vancouver Island Health Authority\n      41\n      South Vancouver Island\n      64\n      Salt Spring Island\n      65\n      None\n    \n    \n      1\n      POINT (-123.96995 49.18515)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      501\n      Y\n      Nanaimo Regional General Hospital\n      Hospital\n      1200 Dufferin Crescent, Nanaimo, BC, Nanaimo, BC\n      ...\n      None\n      Air Photo\n      4\n      Vancouver Island Health Authority\n      42\n      Central Vancouver Island\n      68\n      Nanaimo\n      83\n      None\n    \n    \n      2\n      POINT (-125.24263 50.00876)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      508\n      Y\n      North Island Hospital, Campbell River & District\n      Hospital\n      375 2nd Avenue, Campbell River, BC, Campbell R...\n      ...\n      None\n      Air Photo\n      4\n      Vancouver Island Health Authority\n      43\n      North Vancouver Island\n      72\n      Campbell River\n      87\n      None\n    \n    \n      3\n      POINT (-125.90858 49.15142)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      854\n      Y\n      Tofino General Hospital\n      Hospital\n      261 Neill Street, Tofino, BC, Alberni, BC\n      ...\n      None\n      Air Photo\n      4\n      Vancouver Island Health Authority\n      42\n      Central Vancouver Island\n      70\n      Tofino\n      139\n      None\n    \n    \n      4\n      POINT (-123.43261 48.46672)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      202\n      Y\n      Victoria General Hospital\n      Hospital\n      1 Hospital Way, Victoria, BC, Greater Victoria...\n      ...\n      None\n      Air Photo\n      4\n      Vancouver Island Health Authority\n      41\n      South Vancouver Island\n      61\n      Victoria\n      149\n      None\n    \n  \n\n5 rows × 33 columns\n\n\n\n\n\n\n\n\n\nSQL Syntax\n\n\n\nIn the query above, note that the column in the query is NOT put in an extra set of quotations, whereas the column value is for character columns. This is in line with standard SQL syntax, but if you are not familiar with SQL then it might look a little confusing at first! General rule of thumb for the query is that it will be something of the form:\n\"COLUMN_A = 'some string value' OR COLUMN_B = 42\""
  },
  {
    "objectID": "30_bcdata.html#geospatial-plots",
    "href": "30_bcdata.html#geospatial-plots",
    "title": "6  Introduction to bcdata",
    "section": "6.3 Geospatial Plots",
    "text": "6.3 Geospatial Plots\nAlright, we have some geospatial data in Python now. That’s great. I really love\nPOLYGON ((-118.37713213 49.91303648, -118.37772348 49.91308665, -118.37819007 49.91293438, -118.37864102 49.91273297, -118.37885065 49.91240661, -118.37894791 49.91201733, -118.3791819 49.91159333, -118.37960692 49.91128489, -118.3800468 49.91108959, -118.38056154 49.91089751, -118.38113866 49.91078258, -118.38179033 49.91067428, -118.38237742 49.91061745, -118.38289869 49.91056691, -118.38332771 49.91048198, -118.38381897 49.91036744, -118.38447835 49.91018407, -118.38517458 49.91007327, -118.38585128 49.91001193, -118.38650275 49.90998635, -118.38735549 49.90993369, -118.38790802 49.90991568, -118.38842369 49.90990548, -118.38892747 49.90988955, -118.38972597 49.90985294, -118.39035971 49.90986504, -118.39098773 49.9099297, -118.39161337 49.90996627, -118.39214558 49.91004415, -118.39273105 49.91006478, -118.3932995 49.90999734, -118.39412967 49.90993219, -118.39464161 49.90987966, -118.39535483 49.90980076, -118.39601118 49.90972691, …)\nthis time of year, don’t you?\n… no? No clue where that is or what it might look like? Me neither. Luckily for us, we can checkout these datasets with some geospatial plots! The geopandas dataframe that the data was loaded into can automatically display a map with points, borders, or coloured regions.\nLet’s look at the basic plot that is displayed for a ‘point’ style geometry vs. a ‘polygon’ style geometry. Don’t forget to import matplotlib to display the plots!\n\nimport matplotlib.pyplot as plt\n\nhospitals.plot()\nplt.show()\n\nfires.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWay better! With polygon areas, we also have a few more options for how we wish to colour the regions.\nWe can colour based on a categorical variable:\n\nfires.plot(column='MOF_FIRE_CENTRE_NAME')\nplt.show()\n\n\n\n\nOr on a numerical variable:\n\nfires.plot(column='FEATURE_AREA_SQM', legend=True)\nplt.show()\n\n\n\n\nOr if we only want to display the boundaries of our regions, we can do that too:\n\nfires.boundary.plot()\nplt.show()\n\n\n\n\nYou’ll notice that we can easily see the shape of B.C. in these region plots, but it is less obvious for the hospitals dataset. Because the hospitals dataset only contains ‘point’ objects, it will just place these points on an axis, but with no region boundaries to reference, it might just look like a scattering of points on a plot.\nLuckily, we can combine our region plot and point plot to give context to point locations. Because the underlying plotting tools used by these dataframes is matplotlib, we can plot both plots to the same axes before showing the overall figure!\n\nax = fires.boundary.plot()  # assign the output of this first plot to a variable \n\nhospitals.plot(\n    ax=ax, # use the output of the first plot as the basis for the next \n    marker='.',  # apply some styling options \n    color='red'\n    )  \n\n# use the axis again later to turn off the annoying axis labels\n# we don't always want/need those for geospatial plots! \nax.set_axis_off()\n\n# display your work!\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAdvanced Tip: Managing Projections\n\n\n\n\n\nWe were able to plot these two spatial datasets together because they were imported into python using the same Coordinate Reference System (CRS). The CRS tells the polygon or point shapes how to relate to actual physical locations on Earth. On the B.C. Catalogue Reference Page for both datasets, the spatial reference system is given as EPSG:3005 - NAD83/BC Albers.\nWe can check what the CRS is for a given dataset in Python by looking at the geometry series:\n\ndisplay(hospitals['geometry'].crs)\ndisplay(fires['geometry'].crs)\n\n<Geographic 2D CRS: +init=epsg:4326 +type=crs>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- lon[east]: Longitude (degree)\n- lat[north]: Latitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n<Geographic 2D CRS: +init=epsg:4326 +type=crs>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- lon[east]: Longitude (degree)\n- lat[north]: Latitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nIf for some reason, the two spatial datasets do not have the same reference system, the geopandas dataset gives an option for converting one CRS to another:\nmy_geoseries = my_geoseries.to_crs(\"EPSG:4326\")\nmy_geoseries = my_geoseries.to_crs(epsg=4326)\n\n\n\nWell there you have it. You can find more information on making geopandas plots here, as well as more information on the bcdata package here. We know that this is a much more technical topic than some of the previous sections, but hopefully it will get you excited about all the new ways of exploring data that python allows us to explore!"
  },
  {
    "objectID": "references.html#references",
    "href": "references.html#references",
    "title": "References",
    "section": "References",
    "text": "References\n\n\nAllen Lee, Sourav Singh, Nathan Moore. 2018. “Software Carpentry:\nPlotting and Programming in Python.” https://github.com/swcarpentry/python-novice-inflammation.\n\n\nAzalee Bostroem, Valentina Staneva, Trevor Bekolay. 2016.\n“Software Carpentry: Programming with Python.” Version\n2016.06, 10.5281/zenodo.57492. https://github.com/swcarpentry/python-novice-inflammation.\n\n\nHoltz, Yan. 2018. “The Python Graph Gallery.” https://www.python-graph-gallery.com/.\n\n\nNorris, Simon. 2022. “bcdata Python\nPackage.” https://pypi.org/project/bcdata/#description.\n\n\nWaskom, Michael. 2012. “Seaborn Tutorials.” https://seaborn.pydata.org/tutorial.html."
  },
  {
    "objectID": "references.html#cheat-sheets",
    "href": "references.html#cheat-sheets",
    "title": "References",
    "section": "Cheat Sheets",
    "text": "Cheat Sheets\nThere are many useful cheatsheets that will aggregate commonly used functions and commands into one location. datacamp hosts many of these cheatsheets for Python as well as other languages and programs. The cheatsheet itself is typically a 1-2 page PDF that can be easily referenced for standard functions. Some of the cheatsheets that reference materials in this course include:\n\nGetting started with Python\nPython for Data Science\nPandas: Data Wrangling\nMatplotlib\nSeaborn"
  }
]