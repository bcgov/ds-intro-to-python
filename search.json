[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science using Python",
    "section": "",
    "text": "Preface\nThis is a Quarto book.\nTo learn more about Quarto books visit https://quarto.org/docs/books.\n\n\n\nThe Messy Side of Data\n\n\n\n\n\nThe Messy Side of Data\n\n\nThis is a test for the gh action… please work."
  },
  {
    "objectID": "00_introduction_to_python.html#motivation",
    "href": "00_introduction_to_python.html#motivation",
    "title": "1  Getting Up and Running",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nData has become interwined with the inner workings of nearly every facet of working within the BC Public Service. Whether you have to read an excel spreadsheet, prepare a report based on a survey, comb through csv files to find a specific data source, it is likely that you have worked with a dataset at some point in your career. However, the process of looking at and dealing with data can be messy, error-prone and hard to duplicate. Questions such as ‘wait, how did I get that number again?’ are all too common.\n\n\n\nThe Messy Side of Data\n\n\nThese lessons will teach you how to interact with data in a systematic way using the python ecosystem. By accessing and interpreting data through a set of prescribed methods (developed through the code written), our work with data becomes more accessible, repeatable, and ultimately insightful.\nDuring the course of these lessons, we hope to cover:\n\nPython preliminaries\nExploring and cleaning raw data\nUsing statistical methods\nPlotting results graphically\n\nIf we have time, we may touch on some more advanced python lessons, such as:\n\nPublishing reports\nAccessing the B.C. Data Catalog\nMachine learning in python"
  },
  {
    "objectID": "00_introduction_to_python.html#before-starting",
    "href": "00_introduction_to_python.html#before-starting",
    "title": "1  Getting Up and Running",
    "section": "1.2 BEFORE STARTING!!",
    "text": "1.2 BEFORE STARTING!!\nSo that we can hit the ground running with this workshop, we are asking that everyone get some basic python tools downloaded and installed before the workshop starts. Tools that we will use include Anaconda (or Miniconda) as well as VSCode. A basic knowledge of the command line/powershell interface will be useful as well, but we will try to keep our use of this to a minimum.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are having issues installing anything we have requested prior to the start of the workshop, please let us know so we can work with you so that we can hit the ground running!\n\n\n\nAnaconda/Miniconda is used to download, install, and organize both python and any packages/libraries we use within python. The actual program doing the organizing is conda, while we will use an anaconda powershell to do the installs and interface with python.\nVSCode is a tool used to write, edit and test code known as an IDE (Integrated Development Environment). It is available for more languages than just python, and its versatility has made it a widespread tool within the BCPS.\n\n\nInstall our Python Tools\nIf you do not have administrative rights on your computer:\nDownload Anaconda and VSCode from the B.C. Government Software Centre:\n\nInstall Anaconda (Anaconda3X64 2022.05 Gen P0)\nInstall VSCode (VSCodeX64 1.55.2 Gen P0)\n\nIf you do have administrative rights on your computer:\n\nIf you have administrative rights on your computer, we suggest downloading the lightweight version of Anaconda called Miniconda.\n\nLink to instructions here!\n\nFind the latest version of VSCode here.\n\n\n\nInstall some Python Packages\nMost of the time, when using python we are not using it by itself, but in conjunction with powerful libraries that have already been built to make our data analysis easier. In order to use these tools, we have to install them as well. Using a package manager such as conda makes our life much easier, as we can safely install tools into local environments where every library installed is checked for compatability with every other library. By utilizing the local conda environments, we maintain clean working spaces that can be easily reproduced between workstations.\nLet’s run through the basic steps of setting up a conda environment, installing python and some packages, and testing that it worked!\n\nOpen an anaconda powershell prompt from your search bar.\n\n\n\n\nPowershell Prompt\n\n\n\nInside the anaconda powershell prompt, create a new local conda environment named ds-env using the following commands (hit Enter or type Y and hit Enter when asked to proceed):\n\n\n\nAnaconda Powershell Prompt\n\n> conda create --name ds-env\n> conda activate ds-env\n\n\n\n\nCreating a Conda Environment\n\n\n\nYou should notice that running this second command switches the name in brackets at the beginning of your prompt from (base) to (ds-env). This means we have successfully created a new, empty environment to work in.\n\nInstall python and some useful datascience packages by typing the following commands into the same powershell prompt window:\n\n\n\nAnaconda Powershell Prompt\n\n> conda install python=3.9\n> conda install notebook jupyterlab ipywidgets matplotlib seaborn numpy scikit-learn pandas openpyxl\n\n\nMake sure that python installed successfully. From the same anaconda powershell prompt, simply type python. If this causes no error, success! Try typing this command in the python environment that started to make sure the packages installed as well:\n\n\nimport pandas\npandas.__version__\n\n\n\n\nTesting the python installation\n\n\n\nIf this all works with no errors, python was successfully installed.\n\n\nSetup our VSCode Environment\nStill with me? Great. Here’s a cute otter as congratulations for making it this far.\n\n\n\nThe cutest.\n\n\n\nWe have just a few more steps to go.\n\nOpen the VSCode program.\nOn the left toolbar, find the extensions tab (It looks like 4 squares). Search for the python extension and install this extension.\nFor those using Windows computers, change your default terminal to the command prompt:\n\nFrom anywhere inside VSCode, hit Ctrl + Shift + P. This will open up the command palette.\nStart typing Terminal: Select Default Profile until this option pops up.\nChoose this option, and then click on Command Prompt\n\n\nThat’s it. We are ready to go!"
  },
  {
    "objectID": "00_introduction_to_python.html#hello-world",
    "href": "00_introduction_to_python.html#hello-world",
    "title": "1  Getting Up and Running",
    "section": "1.3 Hello World",
    "text": "1.3 Hello World\ni.e. the how many different ways can we print Hello World! to our screen? section\n\n\n\nHello World!\n\n\n\nThere are many different ways in which we can interact with python. These include:\n\nFrom the command line\nInside a Jupyter Notebook\nFrom a file (inside VSCode)\n\nIn this next section, we are going to have a brief introduction to all of these methods of interaction.\n\n\n\n\n\n\nTip: Using the command line\n\n\n\nIt’s worth pointing out that the methods that we will focus on in this course will rely on using VSCode or JupyterLab and all of its inner workings. However, if you are comfortable with the command line, we can also access any of these methods directly from there as well, you just need to be able to move to directories before typing commands. If you use the command line, I recommend using an anaconda powershell prompt, as this allows for the easiest use and access to conda commands and only the smallest of headaches.\n\n\n\nStep 0\nIn all cases, we will want to have a folder from which we wish to work out of. Take some time to set up a folder somewhere you won’t lose it. For me, I’ve simply made a folder called Intro to Python on my C: drive that will hold any course materials we use/create here.\nNext, to make any interactions with python, we will want to open VSCode and work from here. When we first open VSCode, you should be prompted to open a folder. We are going to work out of that Intro to Python folder, so open it here. After doing this, we should now have a VSCode screen open that will look something like this:\n\n\n\nVS Code\n\n\n\nWe have 3 main areas that we can utilize:\n\nTo the left (area A): is the current folder and subfolder directory list. We can make new files directly from here.\nTo the right (area B): this is where files we are working on will live. For some file types, preview windows will be available as well.\nTo the bottom (area C): this is where we can open and run commands from the command line (or terminal).\n\nNow remember, we set up a special environment that contains python and our data science packages. We want to make sure we are always using this environment, so in the open terminal, re-type conda activate ds-env and this terminal will now be open in this environment. We also want to check that VSCode itself (and not just the terminal) will use the same environment. We again access the command palette with Ctrl + Shift + P, and begin typing Python: Select Interpreter. Click on this, and choose the ds-env option. We are good to go!\n\n\n\n\n\n\nConda Environments\n\n\n\nAlthough it does add an extra level of set-up whenever we start a python project, having these conda environments ends up being incredibly important for not only reproducibility, but making sure that packages work well together. When in doubt as to if you are using the correct environment, double check that the terminal you are using has (ds-env) in brackets at the start of a line.\n\n\n\n\nFrom the command line/terminal\nLet’s start with an easy one. To start a python session from a terminal, simply type python at the command line, and the terminal will automatically open a python interface. You will know you are inside the python interface if your command lines now start with >>>. Now, let’s do the classic Hello World command for python:\n\nprint('Hello World!')\n\nHello World!\n\n\nTo exit the python interface and return to the regular terminal, you can type exit() and return to the terminal.\n\n\nFrom a file (in VSCode)\nNext up, let’s run an entire python file to tell us hello. Inside our directory, create a new file called hello_world.py. Note that .py extension - this signifies that the content inside will be python code. Inside this file, let’s have two lines of code:\n\n\nhello_world.py\n\nprint(\"Hello World!\")\nprint(\"Otters are the best animal.\")\n\nTo run this code, first save the file, and then simply click the play button (triangle in the top right!). Note that this will display an output in a terminal at the bottom of VSCode. VSCode takes the python file you told it to run, and will run every line of code individually. Thus, we get two lines of output for the two print statements.\nBut wait, there’s more! In VSCode we can run individual lines of code within a file as well. Simply move your cursor to the line you wish to run, and hit Shift+Enter.\nNote the difference here. Instead of running the entire file, VSCode actually opened up a python window inside our terminal, and ran the single line of code, just like we did before.\n\n\n\n\n\n\nChallenge 1\n\n\n\nRun the other line of code, and then add and run a third line of code that prints your favourite TV show.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nPress Shift+Enter while the cursor is on the other line of code.\nAdd a line of code such as:\nprint(\"C'mon son! You know the best show is Psych!\")\nSave the file, and again press Shift+Enter while on this line.\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\nTry clicking the play button again. What happens here? Can you explain why?\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\nAn error will occur in the terminal.\nThis is because VSCode tried to run the entire program again, but from inside an already open python program (which we opened when we ran a single line using the Shift+Enter method).\nTo fix this, we can either exit() the current python execution in the current terminal, or Kill/Delete the open terminal. This button can be found by hovering over the Python symbol at the right hand side of the terminals.\n\n\n\n\n\nFrom JupyterLab\nJupyterLab is an application with a web-based user interface that enables one to work with documents and activities such as Jupyter notebooks, text editors, terminals, and even custom components in a flexible, integrated, and extensible manner.\nTo start JupyterLab, you can use Anaconda Navigator, a GUI that comes packaged with Anaconda if you wish. However it is nearly always easier to access it from the command line. Inside VSCode, navigate to a new terminal.\n\n\n\n\n\n\nConda Reminder!\n\n\n\nMake sure that you double check that this new terminal is opened with the ds-env!\n\n\nFrom the terminal, to launch a new JupyterLab session, simply type:\n\n\nterminal\n\n> jupyter lab\n\nThis should open up a screen that looks something like this:\n\n\n\nJupyterLab\n\n\n\n\n\n\n\n\n\nFun Fact\n\n\n\nYou might have noticed by this point that the author of this section prefers dark mode. So if any of your programs are popping up in a different/lighter colour scheme, that’s okay!\n\n\nWe will be using the JupyterLab interface quite a bit, so let’s get used to the key pieces. Similar to VSCode, we have a few key areas to utilize:\n\nOn the left (area A), we have the sidebar that contains commonly used tabs. These include the file directory system (which defaults to the folder from which the session was launched), a list of open files, running kernels and terminals, a table of contents for any markdown that is written, and possibly a set of extensions.\nOn the right (area B), we have the main work area, where we can open new notebooks, terminals, files, etc. Here, when we have multiple open tabs, we can drag them around the main area to produce smaller panels displaying multiple pieces of work.\n\nGenerally speaking, we will be using Jupyter Notebooks, which have a .ipynb file extension. Like python files, these notebooks can run python code. However, they also support markdown (text that can be added to support source code with explanations) as well as inline viewing of data tables, plots and more. This allows us to mix source code, text and images all in one file that we can quickly use for anything such as:\n\nAnswering ‘how did I get this number in that report?’\nLooking at the number.\nUpdating the number with an updated dataset.\nPlotting the number with other numbers, and then looking at all of those numbers.\n\nLet’s open up a notebook (click the Python 3 icon underneath the Notebook heading). This will open up a new tab called Untitled.ipynb.\n\n\n\n\n\n\nChallenge 3\n\n\n\nRename the notebook to hello_world.ipynb\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nOn either the tab to the top, or the filename that popped up on the left panel, right click the file Untitled.ipynb and click Rename notebook.... Enter your new name here.\n\n\n\nIn this new file, we have a single blue box (this is a single cell). We can type multiple lines of code inside a single cell. Within the blue box, pressing Enter will let you add a new line to the cell. To execute a cell, we press Shift+Enter. This will execute every line of code within that cell. If there is output to display, it will display in the space directly below the cell. Pressing the + at the top of the tab will add a new cell (as will running the bottom-most cell). The ‘active’ cell will always be the one that is highlighted with a blue box.\n\n\n\n\n\n\nChallenge 4\n\n\n\nPrint ‘Hello World’ inside the Jupyter Notebook.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nInside the blue box, write the python code:\nprint('Hello World!')\nWhile still on this cell (highlighted in blue), press Shift+Enter. We should see the output directly below!\n\n\n\nWhen we are done with a JupyterLab session, we must shutdown the server. From the Menu Bar select the File menu and then choose Shut Down at the bottom of the dropdown menu. You will be prompted to confirm that you wish to shutdown the JupyterLab server (don’t forget to save your work!). Click Shut Down to shutdown the JupyterLab server.\n\n\n\n\n\n\nJupyterLab and Us!\n\n\n\nWe will be using JupyterLab for the vast majority of our work throughout this course because of the ease of use in writing and executing code all in one place. Make sure you are comfortable with:\n\nopening JupyterLab to a specific folder\ncreating and renaming notebooks\nadding python code to multiple cells within a notebook\nexecuting entire code blocks\n\nIf you are unsure about any of these pieces, please ask for help!"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#variables-assignment",
    "href": "02_core_data_structure_concepts.html#variables-assignment",
    "title": "2  Core Concepts",
    "section": "2.1 Variables & Assignment",
    "text": "2.1 Variables & Assignment\nWe can assign values to variables in Python that we can use over and over. Variables are always assigned using the format:\n\nvariable_name = 'variable_value'\nfirst_name = 'Loki'\nage = 1054\n\nWhere the name of the variable is always to the left, and whatever value we wish to assign being on the right of =.\nSome rules regarding naming variables:\n\nNames may only contain letters, digits, and underscores\nAre case sensitive\nMust not start with a digit\n\nTypically, variables starting with _ or __ have special meaning, so we will try to stick to starting variables with letters only\n\n\nTo display the value we have previously assigned to a variable, we can use the print function:\n\nprint(first_name, 'is', age, 'Earth years old.')\n\nLoki is 1054 Earth years old.\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nIn the next cell, run the following command:\nprint(last_name)\nWhat happens? Why? How can we fix it?\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nVariables cannot be referenced before they are assigned, so we will run into an error:\n\nprint(last_name)\n\nNameError: name 'last_name' is not defined\n\n\nTo fix this, we simply need to create another cell that assigns this variable, then we can go back and run the print command.\n\nlast_name = 'Odinson'\n\n\nprint(last_name)\n\nOdinson\n\n\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\nFill the table below showing the values of the variables in this program after each statement is executed.\n\n\n\nCommand\nValue of x\nValue of y\nValue of swap\n\n\n\n\nx = 1.0\n\n\n\n\n\ny = 3.0\n\n\n\n\n\nswap = x\n\n\n\n\n\nx = y\n\n\n\n\n\ny = swap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\n\n\n\nCommand\nValue of x\nValue of y\nValue of swap\n\n\n\n\nx = 1.0\n1.0\nnot defined\nnot defined\n\n\ny = 3.0\n1.0\n3.0\nnot defined\n\n\nswap = x\n1.0\n3.0\n1.0\n\n\nx = y\n3.0\n3.0\n1.0\n\n\ny = swap\n3.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Notebooks: Order of Execution\n\n\n\nIf you noticed in the last challenge, you could go back to a previous cell above where you assigned a variable, and the print command would work. This is because, in a Jupyter notebook, it is the order of execution of cells that is important, not the order in which they appear. Python will remember all the code that was run previously, including any variables you have defined, irrespective of cell order.\nAfter a long day of work and to prevent confusion, it can be helpful to use the Kernel → Restart & Run All option which clears the interpreter and runs everything from a clean slate going top to bottom."
  },
  {
    "objectID": "02_core_data_structure_concepts.html#lists-indexing",
    "href": "02_core_data_structure_concepts.html#lists-indexing",
    "title": "2  Core Concepts",
    "section": "2.2 Lists & Indexing",
    "text": "2.2 Lists & Indexing\n\n\n\n#sorrynotsorry R\n\n\n\nLists\nAn important aspect of Pythonic programming is the use of indicies to allow us to slice and dice our datasets. We will learn a bit more about indexing here through the introduction of lists. A list is an ordered list of items in Python, where the items can take on any datatype (even another list!). We create a list by putting values inside square brackets and separate items with commas:\n\nmy_list = [1, 'two', 3.0, True]\nprint(my_list)\n\n[1, 'two', 3.0, True]\n\n\n\n\nIndexing\nTo access the elements of a list we use indices, the numbered positions of elements in the list. These positions are numbered starting at 0, so the first element has an index of 0. Python has made it easy to count backwards as well: the last index can be accessed using index -1, the second last with -2 and so on.\n\n\n\n\n\n\n0-Based Indexing!\n\n\n\nIf you have used other coding languages, such as R, you may notice that different programming languages start counting from different numbers. In R, you start your indexing from 1, but in Python it is 0. It’s important to keep this in mind!\n\n\n\nprint('First element:', my_list[0])\nprint('Last element:', my_list[-1])\nprint('Second last element:', my_list[2])\nprint('Also second last element:', my_list[-2])\n\nFirst element: 1\nLast element: True\nSecond last element: 3.0\nAlso second last element: 3.0\n\n\nStrings also have indices, pointing to the character in each string. These work in the same way as lists.\n\nprint(first_name)\nprint(first_name[0])\n\nLoki\nL\n\n\nHowever, there is one important difference between lists and strings: we can change values in a list, but we cannot change individual characters in a string. For example:\n\nprint(my_list)\nmy_list[0] = 'changing the first element!'\nprint(my_list)\n\n[1, 'two', 3.0, True]\n['changing the first element!', 'two', 3.0, True]\n\n\nwill work. However:\n\nprint(first_name)\nfirst_name[0] = 'N'\n\nLoki\n\n\nTypeError: 'str' object does not support item assignment\n\n\nWill throw an error.\n\n\n\n\n\n\nMutable vs Immutable\n\n\n\nData which can be modified in place is called mutable, while data which cannot be modified is called immutable. Strings and numbers are immutable. This does not mean that variables with string or number values are constants, but when we want to change the value of a string or number variable, we can only replace the old value with a completely new value.\nLists and arrays, on the other hand, are mutable: we can modify them after they have been created. We can change individual elements, append new elements, or reorder the whole list. For some operations, like sorting, we can choose whether to use a function that modifies the data in-place or a function that returns a modified copy and leaves the original unchanged.\nBe careful when modifying data in-place. If two variables refer to the same list, and you modify the list value, it will change for both variables!\n\n\nWe can use indicies for more than just accessing single elements from an ordered object such as a list or a string. We can also slice our dataset to give us different portions of the list. We do this using the slice notation [start:stop], where start is the integer index of the first element we want and stop is the integer index of the element just after the last element we want. If either of start or stop is left out, it is assumed that you want to default with either starting from the beginning of the list or ending at the end.\n\nnumber_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint('0:5  --> ', number_list[0:5])\nprint('_:5  --> ', number_list[:5])\nprint('3:7  --> ', number_list[3:5])\nprint('3:_  --> ', number_list[3:])\nprint('3:-1 --> ', number_list[3:-1])\n\n0:5  -->  [0, 1, 2, 3, 4]\n_:5  -->  [0, 1, 2, 3, 4]\n3:7  -->  [3, 4]\n3:_  -->  [3, 4, 5, 6, 7, 8, 9]\n3:-1 -->  [3, 4, 5, 6, 7, 8]\n\n\nWe can also use a step-size to indicate how often we want to pick up an element of the list. By altering the slice notation to [start:stop:step] we will be telling the code to only include those elements at each step after start, ending at the final step that occurs just before running into stop. This allows us to reverse lists as well:\n\nprint('All evens:', number_list[0::2])\nprint('All odds: ', number_list[1::2])\nprint('Just 1 and 4:', number_list[1:5:3])\nprint('Reversed: ', number_list[-1::-1])\n\nAll evens: [0, 2, 4, 6, 8]\nAll odds:  [1, 3, 5, 7, 9]\nJust 1 and 4: [1, 4]\nReversed:  [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\nGiven the following string:\n\nfull_name = 'Peregrin Fool of a Took Pippin'\n\nWhat would these expressions return?\n\nfull_name[2:8]\nfull_name[11:] (without a value after the colon)\nfull_name[:4] (without a value before the colon)\nfull_name[:] (just a colon)\nfull_name[11:-3]\nfull_name[-5:-3]\nWhat happens when you choose a stop value which is out of range? (i.e., try full_name[0:42] or full_name[:103])\n\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\n\n'regrin '\n'ol of a Took Pippin'\n'Pere'\n'Peregrin Fool of a Took Pippin'\n'ol of a Took Pip'\n'ip'\nIf a part of the slice is out of range, the operation does not fail. full_name[:] gives the same result as full_name[0:42], and the same result as full_name[:103].\n\n\n\n\n\n\nDictionaries\nAnother object that exists in Python is the dictionary. It is similar to a list in that it can hold a variety of different types of objects inside of it. However an important difference is in how we access these objects. With a list (or string), we have an ordered arrangement of items that we access with an integer index. However, we access the values in a dictionary with a key, which can be anything we want.\nLet’s build a dictionary, which is denoted in Python with curly {} brackets:\n\nmy_dict = {\n    'first_key': 'some value',\n    'A': ['a', 'differerent', 'type', 'of', 'object'],\n    2: False\n}\n\nprint(my_dict)\n\n{'first_key': 'some value', 'A': ['a', 'differerent', 'type', 'of', 'object'], 2: False}\n\n\nHere we listed three key - value pairs. The key comes before the value, with a colon between. Commas separate different pairs. Now that we have a dictionary, we access it the same way as a list, with square [] brackets:\n\nprint(my_dict['first_key'])\nprint()\nprint(my_dict['A'])\nprint()\nprint(my_dict[2])\n\nsome value\n\n['a', 'differerent', 'type', 'of', 'object']\n\nFalse\n\n\nUnlike a list, dictionaries are unordered, and so we cannot perform integer indexing or slicing of these elements:\n\nmy_dict[0]\n\nKeyError: 0\n\n\n\nmy_dict[0:5]\n\nTypeError: unhashable type: 'slice'\n\n\nDictionaries are an abstract data type that can take a while to get used to! They can be a powerful tool in Python. Common uses for dictionaries include:\n\nCreating searchable parameter lists for models\nSupplying extra arguments to functions\nStoring complex outputs or datasets\n\nWe will not need to use dictionaries frequently in this course. However, they will become useful when we learn more about data tables and aggregation methods later on, and so gaining familiarity now is beneficial!"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#data-types-operations",
    "href": "02_core_data_structure_concepts.html#data-types-operations",
    "title": "2  Core Concepts",
    "section": "2.3 Data Types & Operations",
    "text": "2.3 Data Types & Operations\nEvery value in a program has a specific type. In this course, you will run across four basic types in Python:\n\nInteger (int): positive or negative whole numbers like 42 or 90210\nFloating point numbers (float): real fractional numbers like 3.14159 or -87.6\nCharacter strings (str): text written either in single or double quotes.\nBoolean (bool): the logical values of True and False\n\nIf you are unsure what type anything is, we can use the built in function type. Note that this works on variables as well.\n\nprint(type(42))\nprint(type(3.14))\nprint(type('Otter'))\nprint(type(True))\n\n<class 'int'>\n<class 'float'>\n<class 'str'>\n<class 'bool'>\n\n\n\n\n\n\n\n\nMessy Numbers\n\n\n\nWhen you start to have really long integers, it starts to look really messy (how many thousands are in 1982137092 at a glance?) Luckily, Python allows us to use _ inside our integers to space out our digits. Thus we could write that instead as 1_902_137_092. Isn’t that nicer!\n\n\n\nBasic Arithmetic\nThe type of a variable controls what operations can be performed on it. For example, we can subtract floats and ints, but we cannot subtract strings:\n\nprint(42-12)\nprint(3.14-15)\nprint('hello' - 'h')\n\n30\n-11.86\n\n\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n\n\nHowever, we can add strings together:\n\nmy_sentence = 'Adding' + ' ' + 'strings' + ' ' + 'concatenates them.'\nprint(my_sentence)\n\nAdding strings concatenates them.\n\n\nAs well as multipling a string by an integer to get a repeated string:\n\nrepeated_string = '=+'*10\nprint(repeated_string)\n\n=+=+=+=+=+=+=+=+=+=+\n\n\nAs we saw above, we can mix and match both of the numerical types, however we will get an error if we try to mix a string with a number:\n\nprint(1 + '2')\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\nIn order to have a sensical operation, we need to convert variables to a common type before doing our operation. We can convert variables using the type name as a function.\n\nprint(1 + int('2'))\nprint(str(1) + '2')\n\n3\n12\n\n\n\n\n\n\n\n\nInt of a Float?\n\n\n\nNote that when converting floats to integers, it will always round down! That is, int(3.6) = int(3.1) = 3!\n\n\nOne last bit of math you might come across are the various types of division:\n\n/ performs regular floating-point division\n// performs integer floor division\n% returns the remainder of integer division\n\n\nprint('5 / 3 :', 5 / 3)\nprint('5 // 3:', 5 // 3)\nprint('5 % 3 :', 5 % 3)\n\n5 / 3 : 1.6666666666666667\n5 // 3: 1\n5 % 3 : 2\n\n\n\n\nBuilt in Functions\nPython has multiple pre-built functions that come in handy. We have already made use of the print() command frequently, and learned how to use type() to tell us what type of data our variables are. Here are some other frequently used functions:\n\nlen() : Tells us the length of a list, string, or other ordered object. Does not work on numbers!\nhelp() : Gives help for other functions\nmin() : Gives the mininum value in a list of options.\nmax() : Gives the maximum value in a list of options.\nround(): Rounds a value to a given decimal length.\n\nNote that, similar to the arithmetic operations above, these built in functions must operate on logically consistent datatypes. We can find the min of 2 strings, or 4 numbers, but we cannot compare a string to a float.\nEvery function in python will take 0 or more arguments that are passed to a function. For example, len() takes exactly one argument, and returns the length of that argument:\n\n print(len('this string is how long?'))\n\n24\n\n\nSome functions, such as min() and max() take a variable number of arguments:\n\n print(min(1,2,3,4))\n print(max('a', 'b', 'c'))\n\n1\nc\n\n\nWhile others have default values that do not need to be provided at all.\n\n\n\n\n\n\nChallenge 4\n\n\n\nUse the help() and round() functions to print out the value of 2.71828182845904523536 to 0 and 2 decimal places.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nThe result of help() tells us that round() has a default option:\n\nhelp(round)\n\nHelp on built-in function round in module builtins:\n\nround(number, ndigits=None)\n    Round a number to a given precision in decimal digits.\n    \n    The return value is an integer if ndigits is omitted or None.  Otherwise\n    the return value has the same type as the number.  ndigits may be negative.\n\n\n\nThus, we can use the round() function with and without the default ndigits to get our answer:\n\neulers_number = 2.71828182845904523536\nprint(round(eulers_number))\nprint(round(eulers_number, 2))\n\n3\n2.72\n\n\n\n\n\n\n\n\n\n\n\nHelp in Jupyter Lab!\n\n\n\nIn Jupyter notebooks, we can also get help by starting a line with ?. For example, ?round will display the help information about the round() function.\n\n\n\n\nA Quick Intro to Boolean Logic\nWe can ask Python to take different actions, depending on a condition, with the if statement:\n\nnum = 37\nif num > 100:\n    print('greater')\nelse:\n    print('not greater')\nprint('done')\n\nnot greater\ndone\n\n\nThe if keyword tells Python we want to make a choice. We then use : to end the conditional we would like to consider, and indentation to specify our if block of code that should execute if the condition is met. If the condition is not met, the body of the else block gets executed instead.\nIn either case, ‘done’ will always print as it is in neither indented block.\n\n\n\nFollowing a Logical Flow\n\n\nConditional statements do not need to include an else block. If there is no block and the condition is False, Python simply does nothing:\n\nnum = 37\nif num > 100:\n    print('greater')\nprint('done')\n\ndone\n\n\nWe can also chain several tests together using elif. Python will go through the code line by line, looking for a condition that is met. If no condition is met, it will execute the else block (or nothing if there is no else).\n\nnum = 45\nif num < 42:\n    print('This is not the answer.')\nelif num > 42:\n    print('This is also not the answer.')\nelse:\n    print('This is the answer to life, the universe, and everything.')\n\nThis is also not the answer.\n\n\nThere are multiple different comparisons we can make in Python:\n\n>: greater than\n<: less than\n==: equal to (note the double ‘=’ here!)\n!=: does not equal\n>=: greater than or equal to\n<=: less than or equal to\n\nAnd these can be used in conjunction with each other using the special keywords and, or, and not. and will evaluate to True if both parts are True, while or will evaluate to True if either side is. not will evaluate the condition, and then return the opposite result.\n\ncondition_1 =  1 > 0  # True\ncondition_2 = -1 > 0  # False\n\nprint('Testing and: ')\nif condition_1 and condition_2:\n    print('both parts are true')\nelse:\n    print('at least one part is false')\n\nprint()\nprint('Testing or: ')\nif condition_1 or condition_2:\n    print('at least one part is true')\nelse:\n    print('both parts are false')\n\nprint()\nprint('Testing not: ')\nif not condition_1:\n    print('condition_1 was false')\nelse:\n    print('condition_1 was true')\n\nTesting and: \nat least one part is false\n\nTesting or: \nat least one part is true\n\nTesting not: \ncondition_1 was true\n\n\n\n\n\n\n\n\nMultiple Conditions Cause Confusion\n\n\n\nJust like with arithmetic, you can and should use parentheses whenever there is possible ambiguity. A good general rule is to always use parentheses when mixing and and or in the same condition.\n\n\n\n\n\n\n\n\nChallenge 5\n\n\n\nWhat will be the output of the following block of code?\nnum = 42 \nanimal = 'otter'\n\nif num==42 and animal=='mouse':\n    print('Correct, that is the animal that found the answer')\nelif num==42 and animal!='mouse':\n    print('Almost, the number is correct but not the animal.')\n    animal = 'dolphin'\nelif num!=42 or animal=='mouse':\n    print('Almost, the animal is correct but not the number.')\n    num = 5\nelif (1>3) or (4>3):\n    print('This has nothing to do with it, we just needed an or statement')\nelse:\n    print('Not even close, those pesky mice need to work harder.')\n    num = 19\n    animal = 'kangaroo'\n\nprint('The end result is the number', number, 'and animal', animal)\n\n\n\n\n\n\n\n\nSolution to Challenge 5\n\n\n\n\n\n\n\nAlmost, the number is correct but not the animal.\nThe end result is the number 42 and animal dolphin\n\n\n\n\n\n\n\n\n\n\n\nElement-Wise Logic\n\n\n\nBefore we move on from our foray into boolean logic, let us make a brief mention of the &, |, and ~ symbols. These are similar, but not identical, to and, or and not. Where and is used for boolean logic on scalars, & is used for boolean logic on vectors, and will do an element-by-element comparison. This will be important when we introduce data structures later on!"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#methods-chaining",
    "href": "02_core_data_structure_concepts.html#methods-chaining",
    "title": "2  Core Concepts",
    "section": "2.4 Methods & Chaining",
    "text": "2.4 Methods & Chaining\n\n\n\nObject Oriented Programming\n\n\nSo far we have seen built in functions that can be applied to a variety of different datatypes (as long as the datatype makes sense for that particular function). However, there are some functions that we apply specifically to a particular class of objects - we call these functions methods. Methods have parentheses, just like functions, but they come after the variable to denote that the method belongs to this particular object.\nWe have met classes already: all of our basic datatypes (strings, integers, floats, booleans) are different classes of objects in Python. An individual instance of a class is considered an object of that class. Understanding how to use methods will become useful when we reach the pandas portion of the course, which is our main tool when looking at, cleaning, and summarizing data.\nLet’s consider the string class. Here are a few common methods associated with it:\n\nlower(): coverts all characters in the string to lowercase\nupper(): converts all characters in the string to uppercase\nindex(): returns the position of the first occurrence of a substring in a string\nrjust(): right aligns the string according to the width specified\nisnumeric(): returns True if all characters are numeric\nreplace(): replaces all occurrences of a substring with another substring\n\n\n\n\n\n\n\nGetting Help For Methods\n\n\n\nYou will notice that trying to find help on a method will not work if you only specify the method. Because these are not built in functions, and only belong to instances of a class, you need to specify the object together with the method to use help.\nFor example, help(lower) will result in an error, whereas help(\"any string\".lower) will give you the help you were looking for.\n\n\nLet’s see some of these in action. You’ll notice that when being used, methods don’t always have an argument supplied to them. That is because the first argument is always the object is being applied to. If a method requires secondary arguments, these are subsequently included in the parentheses.\n\nobject.method(a, b, c, ...) ↔︎ method(object, a, b, c, ...)\n\n\nmy_string = 'Peter Piper Picked a Peck of Pickled Peppers'\nprint(my_string.lower())\nprint(my_string.isnumeric())\n\npeter piper picked a peck of pickled peppers\nFalse\n\n\nWe can also chain methods together. Each subsequent method (reading from left to right) acts on the output of the previous method. Chaining can be done in a single line, or over multiple lines (which helps for readability).\n\nprint(my_string.upper().replace('P', 'M'))\n\n# Chaining over multiple lines can be done in 2 ways:\n# 1. Enclose the entire operation in brackets\nchain_1 = (my_string\n    .upper()\n    .replace('P', 'M')\n)\n\n# 2. Use the character \"\\\" to denote an operation is continuing on the next line\nchain_2 = my_string \\\n    .upper() \\\n    .replace('P', 'M')\n\nprint(chain_1)\nprint(chain_2)\n\nMETER MIMER MICKED A MECK OF MICKLED MEMMERS\nMETER MIMER MICKED A MECK OF MICKLED MEMMERS\nMETER MIMER MICKED A MECK OF MICKLED MEMMERS\n\n\n\n\n\n\n\n\nChallenge 6\n\n\n\nWhat happens if we try to run the block of code:\nprint(my_string.isnumeric().upper())\n\n\n\n\n\n\n\n\nSolution to Challenge 6\n\n\n\n\n\nWe will get an error:\n\nprint(my_string.isnumeric().upper())\n\nAttributeError: 'bool' object has no attribute 'upper'\n\n\nThis is because the code is read from left to right. In this case, the output of isnumeric() is a boolean object, which does not have the method upper() anymore:\n\nprint(my_string.isnumeric())\nprint(type(my_string.isnumeric()))\n\nFalse\n<class 'bool'>\n\n\n\n\n\n\n\n\n\n\n\nChallenge 7\n\n\n\nA common data string you see across government are various personal IDs. One example is the Personal Education Number (PEN), which is a nine-digit number assigned to you when you enter the K-12 School System. Oftentimes when looking at such an ID, any leading zeros that are an actual part of the PEN get stripped away, leading to unevenly sized strings of IDs across the dataset.\nWrite a piece of code for PEN IDs that does the following:\n\nChecks to make sure that the ID is entirely numeric. If it is not, print out a warning that this is an invalid PEN.\nIf the PEN is numeric, make sure that it is less than or equal to 9 digits long. If it is longer, print out a warning that this is an invalid PEN.\nIf the PEN is too short, make sure to pad it with the appropriate number of 0’s to the left. Print out the correct PEN.\n\nTry your code out with the following PENs:\npen_1 = '12345678x'\npen_2 = '123456789'\npen_3 = '1234567890'\npen_4 = 123456789\npen_5 = '123456'\n\n\n\n\n\n\n\n\nSolution to Challenge 7\n\n\n\n\n\n\npen_1 = '123456'\n\n# first! make sure we are looking at strings so we can use the string method!\npen = str(pen_1)\n\n# first check for numerical:\nif not pen.isnumeric():\n    print('Warning! This PEN has non-numeric characters.')\n\n# second, check that it isn't too long\nelif len(pen)>9:\n    print('Warning! This PEN is longer than 9 digits.')\n\n# third: make sure that we pad it to the correct length\nelse:\n    pen = pen.rjust(9, '0')\n    print('This PEN is valid:', pen)\n\nThis PEN is valid: 000123456"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#accessing-other-packages",
    "href": "02_core_data_structure_concepts.html#accessing-other-packages",
    "title": "2  Core Concepts",
    "section": "2.5 Accessing Other Packages",
    "text": "2.5 Accessing Other Packages\n\n\n\nImport Packages\n\n\nMost of the power of Python lies in its ability to use libraries, or external packages that are not part of the base Python programming language. These libraries have been written and maintained by other members of the Python community, and will make data cleaning, manipulation, visualization and any other data project much simpler. Throughout this course we will use packages such as:\n\npandas: this is the go-to package for all things data-table.\nmatplotlib: this is the most frequently used plotting package in Python\nseaborn: this is a plotting package built with pandas and data in mind\n\nWhen we set up our Python environment, we already installed many of the packages we will need directly into the conda environment we produced. If you ever need another package, it is simple enough to install again using conda:\n\n\nAnaconda Powershell Prompt\n\n> conda activate ds-env\n> conda install <package>\n\n\n\n\n\n\n\nTo Pip or not to Pip?\n\n\n\nIf you are ever searching for a package you think will aid you in your work, you might come across the pip command. This is a different (yet related) method of installing packages. While it is possible to use pip in tandem with conda commands, it is recommended that you stick to only conda wherever possible.\nAs a rule of thumb, try to conda install package as a first try. If this does not work, search the website for the package for installation instructions. Sometimes it will recommend using a different conda channel (and will provide the code to do so). Sometimes, it is only possible to get the package from pip, in which case using pip inside the conda environment is the only way to go. Just use this as a last resort!\n\n\nOkay great, we have all these awesome libraries that have been built out by others. How do we actually use them? In Python, it is actually fairly simple!\nOption 1: Use import to load an entire library module into a program’s memory. Refer to things from the module as module_name.thing_name\n\n import math\n\n print('pi is', math.pi)\n print('cos(pi) is', math.cos(math.pi))\n\npi is 3.141592653589793\ncos(pi) is -1.0\n\n\nOption 2: If we only need a specific function or tool from the library, use from module import thing\n\nfrom math import cos, pi\n\nprint('cos(pi) is', cos(pi))\n\ncos(pi) is -1.0\n\n\nOption 3: If we really do need the entire library, but we do not want to type the entire long name over and over, create an alias\n\nimport math as m\n\nprint('cos(pi) is', m.cos(m.pi))\n\ncos(pi) is -1.0\n\n\nSome common alias for common libraries include:\n\npandas → pd\nmatplotlib.pyplot → plt\nseaborn → sns\nnumpy → np"
  },
  {
    "objectID": "21_getting_data_with_pandas.html#introduction-to-pandas",
    "href": "21_getting_data_with_pandas.html#introduction-to-pandas",
    "title": "3  Getting Data With Pandas",
    "section": "3.1 Introduction to Pandas",
    "text": "3.1 Introduction to Pandas\nPandas is one of the most widely-used Python libraries for statistics, and excels at processing tabular data. If you have ever had any experience with R, it is modeled extensively on the dataframes used there. In this section we aim to learn about the fundamental objects used in pandas, bring our own data into these objects, and then view basic information about it.\n\n\n\n\n\n\nDid You Know?\n\n\n\nDid you know that the pandas library package is aptly named? It is a portmanteau of the words panel and data. We are literally viewing panels of data!\n\n\nFirst, we need to make sure that we import the library into Python so we can use it:\n\nimport pandas as pd\n\nObjects in pandas are typically two-dimensional tables called dataframes. You can think of this as being similar to a single spreadsheet in excel. Each column (called series) in the dataframe can have a name, as well as an indexing value.\n\n\n\nA pandas dataframe\n\n\nWe can create both single series in pandas or full dataframes. Let’s consider a single series first. We can create this using the pandas Series method, as well as a list of elements we wish to include in our series.\n\nseries_x = pd.Series([1, 2, 3])\nseries_x\n\n0    1\n1    2\n2    3\ndtype: int64\n\n\nWhen this displays in python, we see 2 columns. The left column is always the index, while the right column contains the data values. The series is also given a dtype. This is similar to the datatypes we considered previously, and tells us what type of data is held inside the series. When a series is first created, pandas will try to guess the dtype for the series based on the individual elements inside it. We can make a series made of float numbers:\n\nseries_y = pd.Series([1, 2.0, 3.14])\nseries_y\n\n0    1.00\n1    2.00\n2    3.14\ndtype: float64\n\n\nNotice that here, the first element was actually converted from an int to a float by the program! This will often happen when working with int and float type objects.\nNext, a series of str elements will default to the dtype of object.\n\nseries_z = pd.Series(['settlers', 'of', 'catan'])\nseries_z\n\n0    settlers\n1          of\n2       catan\ndtype: object\n\n\nThe dtype: object is pandas ‘catchall’ Series type, and we want to be cautious when we see this! It is also the Series dtype used when we have mixed data:\n\nseries_mixed = pd.Series([1, 1.0, 'one'])\nseries_mixed\n\n0      1\n1    1.0\n2    one\ndtype: object\n\n\nWhen pandas sees data that cannot be obviously converted into a single datatype, it leaves each individual entry alone, as whatever its original datatype happened to be. Be careful when you see this! Many operations on pandas dataframes apply to the entire column (similar to how a new excel column is often created with a function that applies to a previous column). If this operation is built to only work on a single datatype, we might run into errors! To avoid this, we can utilize the .astype() methods. Possible arguments for astype() include the standard datatypes:\n\nint\nfloat\nstr\nbool\n\nHowever if you need to be more specific, you can be as well. These arguments are all required to be included in quotations, as they refer to aliases for pandas specific datatypes:\n\n'Int8', 'Int32', 'Int64' provide nullable integer types (note the capital I! more on nullable typing below)\n'string' provides access to a series that will be string specific, instead of the general objects type. Pandas recommends using this type for strings whenever possible.\n\n\ndisplay(series_y)\ndisplay(series_y.astype(int))\ndisplay(series_y.astype(bool))\ndisplay(series_y.astype(str))\ndisplay(series_y.astype('string'))\n\n0    1.00\n1    2.00\n2    3.14\ndtype: float64\n\n\n0    1\n1    2\n2    3\ndtype: int64\n\n\n0    True\n1    True\n2    True\ndtype: bool\n\n\n0     1.0\n1     2.0\n2    3.14\ndtype: object\n\n\n0     1.0\n1     2.0\n2    3.14\ndtype: string\n\n\nYou will have noticed that each of these series has an index associated with it. We can access series indicies using the index attribute:\n\nseries_a = pd.Series([2, 4, 6])\ndisplay(series_a)\ndisplay(series_a.index)\n\n0    2\n1    4\n2    6\ndtype: int64\n\n\nRangeIndex(start=0, stop=3, step=1)\n\n\nWhen an index is first assigned to a series, it is automatically assigned as an integer index, with similar properties to a list index (starts at 0, can be sliced, etc.). However we can change this index to be whatever we want by directly modifying the index attribute:\n\nseries_a.index = ['a', 'b', 'c']\ndisplay(series_a.index)\ndisplay(series_a)\n\nIndex(['a', 'b', 'c'], dtype='object')\n\n\na    2\nb    4\nc    6\ndtype: int64\n\n\nA useful feature in pandas is the ability to reindex the dataset. Reindexing a dataset will do two things:\n\nReorder the data according to the order we ask for.\nAdd new rows for indicies that are missing in the original dataset but are included in our new index.\n\nOne popular use for this method is in filling out a timeseries dataset: if there were missing years in a dataset but we do not wish to simply skip over them, we can add the extra years to the index.\n\nseries_a = series_a.reindex(['d', 'c', 'b', 'a'])\ndisplay(series_a.index)\ndisplay(series_a)\n\nIndex(['d', 'c', 'b', 'a'], dtype='object')\n\n\nd    NaN\nc    6.0\nb    4.0\na    2.0\ndtype: float64\n\n\n\n\n\n\n\n\nBeware NaN values!\n\n\n\nSometimes in our datasets, we want to allow a row to contain missing, or null values. The default null value for pandas is NaN. In python, NaN is considered a float value. In the above example, we introduced a missing value for the new ‘d’ index, which defaults to the NaN float value. Because its type is float, this converted our entire series to float as well. If we wish to keep the series as an int, we can coerce it back using the astype() method with one of the nullable integer types. This will introduce a slightly different looking null value that works for integers!\n\nseries_a.astype('Int64')\n\nd    <NA>\nc       6\nb       4\na       2\ndtype: Int64\n\n\n\n\nWe commonly want to make new series out of old ones inside our dataframes. Operations are typically done on an element by element basis. We will see many examples of these in future sessions as we learn to manipulate dataframes, but here is a short example of what we may wish to do:\n\nseries_a = pd.Series([2, 4, 6])\nseries_b = pd.Series([1, 2, 3])\n\ndisplay(series_a + series_b)\ndisplay(series_a > 3)\ndisplay(series_a*5)\n\n0    3\n1    6\n2    9\ndtype: int64\n\n\n0    False\n1     True\n2     True\ndtype: bool\n\n\n0    10\n1    20\n2    30\ndtype: int64"
  },
  {
    "objectID": "21_getting_data_with_pandas.html#bringing-in-our-own-data",
    "href": "21_getting_data_with_pandas.html#bringing-in-our-own-data",
    "title": "3  Getting Data With Pandas",
    "section": "3.2 Bringing in our own data",
    "text": "3.2 Bringing in our own data\nEnough about series. Let’s talk dataframes! This is the main tool in our pandas toolkit. As we showed earlier, it is simply a collection of series all stacked together like an excel spreadsheet. There are many different ways to create a dataframe including:\n\nWithin python itself\nFrom a csv, excel file, or some other local tabular format\nFrom more exotic data sources, such as a parquet file, json, or a website\nFrom a SQL database\n\nFor the purposes of this course, we are going to focus on opening up local datafiles (with the most common type being a csv or excel file), and then utilizing the data once it is in python. To bring in data from a csv or excel file, we utilize the pandas methods read_csv() or read_excel(), with the only required argument being the path to the datafile.\nBut first we need some data! Navigate to this URL, right click on the data, and save the csv as gapfinder.csv in a folder called data inside our project folder. Now that we have a dataset, let’s load it into pandas.\n\n\n\n\n\n\nThe Data Folder\n\n\n\nWhile everyone may organize their projects and folders slightly differently, there are some general principles to adhere to that make project management easier. Arguably the most important of these is to treat input data as read only. If you have an excel spreadsheet, it is tempting to go make changes to the data directly in the spreadsheet: I’ll just tweak a single value here, or add a column there. However, once we start doing this we lose the concept of reproducibility! How did we get certain values if the changes are all hidden in the excel spreadsheet? By keeping data as read only and making all of our changes in the python scripts, the processing is not only reproducible but also transparent to others.\nBecause of this, it is common to have a folder exclusively for raw data. Secondary folders may then be set-up for ‘cleaned’ data, python scripts, analysis outputs and more.\n\n\n\ndf = pd.read_csv('../data/gapfinder.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1699\n      Zimbabwe\n      1987\n      9216418.0\n      Africa\n      62.351\n      706.157306\n    \n    \n      1700\n      Zimbabwe\n      1992\n      10704340.0\n      Africa\n      60.377\n      693.420786\n    \n    \n      1701\n      Zimbabwe\n      1997\n      11404948.0\n      Africa\n      46.809\n      792.449960\n    \n    \n      1702\n      Zimbabwe\n      2002\n      11926563.0\n      Africa\n      39.989\n      672.038623\n    \n    \n      1703\n      Zimbabwe\n      2007\n      12311143.0\n      Africa\n      43.487\n      469.709298\n    \n  \n\n1704 rows × 6 columns\n\n\n\n\n\n\n\n\n\n.. ?\n\n\n\nThe .. used in the above path tells python to look in the directory directly above the one you are currently in - the one where your notebook is saved.\nAs an example, my directory structure looks like:\n/ds_intro_to_python\n│\n└───/introduction_to_python\n│   │   hello_world.ipynb\n│   │   hello_world.py\n│   \n└───/introduction_to_pandas\n│   │   intro_to_pandas.ipynb  <------ (THIS FILE!)\n│\n└───/data\n│   │   gapfinder.csv <--------------- (The file we want access to!)\nThis file is in a different subfolder relative to the csv, so we first ‘back out’ of this folder using .., and then ‘enter’ the data folder using a regular file path. This is called relative pathing and can be useful for accessing data within a single project that will always be in the same spot!\n\n\nThis dataset has 6 columns, 1704 rows, and a mixture of different datatypes. Just like we were able to access the index of a series, we can do the same with a dataframe. Now, we can also access (and change if need be!) the column names as well:\n\ndisplay(df.index)\ndisplay(df.columns)\n\ndf.columns = ['country', 'year', 'pop', 'continent', 'life expectancy', 'gdpPercap']\ndisplay(df.columns)\n\nRangeIndex(start=0, stop=1704, step=1)\n\n\nIndex(['country', 'year', 'pop', 'continent', 'lifeExp', 'gdpPercap'], dtype='object')\n\n\nIndex(['country', 'year', 'pop', 'continent', 'life expectancy', 'gdpPercap'], dtype='object')\n\n\nWe can access each of these series individually if we want. There are two ways to access a series in a dataframe - either with square bracket indexing, or treating the column name as an attribute:\n\ndisplay(df['country'])\ndisplay(df.country)\n\n0       Afghanistan\n1       Afghanistan\n2       Afghanistan\n3       Afghanistan\n4       Afghanistan\n           ...     \n1699       Zimbabwe\n1700       Zimbabwe\n1701       Zimbabwe\n1702       Zimbabwe\n1703       Zimbabwe\nName: country, Length: 1704, dtype: object\n\n\n0       Afghanistan\n1       Afghanistan\n2       Afghanistan\n3       Afghanistan\n4       Afghanistan\n           ...     \n1699       Zimbabwe\n1700       Zimbabwe\n1701       Zimbabwe\n1702       Zimbabwe\n1703       Zimbabwe\nName: country, Length: 1704, dtype: object\n\n\n\n\n\n\n\n\nNo Spaces!\n\n\n\n\n\n\n\n\nAs a general rule of thumb, we never want to include special characters such as spaces, periods, hyphens and so on in our column names, as this will alter pandas capability of calling each of the columns as an attribute. Above, you will notice we reset one of the columns to have a space in the name. If we try to access this series as an attribute now, it will fail:\n\ndf['life expectancy']\n\n0       28.801\n1       30.332\n2       31.997\n3       34.020\n4       36.088\n         ...  \n1699    62.351\n1700    60.377\n1701    46.809\n1702    39.989\n1703    43.487\nName: life expectancy, Length: 1704, dtype: float64\n\n\n\ndf.life expectancy\n\nSyntaxError: invalid syntax (41671440.py, line 1)\n\n\nTry to stick to the same naming conventions for columns as for your python variables: lowercase letters, numbers (but not at the start of the name) and underscores only! (And as a matter of fact, let us change it back now):\n\ndf.columns = ['country', 'year', 'pop', 'continent', 'lifeExp', 'gdpPercap']\n\n\n\nWhen reading data into a dataframe from a csv (or an excel file), there are multiple optional arguments we can use to start the process of data wrangling, which is writing code to shape the data into the format we want it for our analysis. Some important options include:\n\nheader: row number to use as the column names. This allows us to skip past rows in the dataset and start from lower down if need be.\nindex_col: name of the column we might wish to use for the index of the dataframe instead of the default integer list.\nusecols: list of columns we wish to use. If the dataset is large with many columns that we do not care about, we can pull in only those of interest!\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nBring the dataset into pandas again. This time:\n\nuse the country as the index\nonly include the year, continent and population columns.\n\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\n\ndf_challenge = pd.read_csv(\n    '../data/gapfinder.csv',\n    index_col = 'country',\n    usecols = ['country', 'continent', 'year', 'pop']\n)\ndisplay(df_challenge)\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      continent\n    \n    \n      country\n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      1952\n      8425333.0\n      Asia\n    \n    \n      Afghanistan\n      1957\n      9240934.0\n      Asia\n    \n    \n      Afghanistan\n      1962\n      10267083.0\n      Asia\n    \n    \n      Afghanistan\n      1967\n      11537966.0\n      Asia\n    \n    \n      Afghanistan\n      1972\n      13079460.0\n      Asia\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      Zimbabwe\n      1987\n      9216418.0\n      Africa\n    \n    \n      Zimbabwe\n      1992\n      10704340.0\n      Africa\n    \n    \n      Zimbabwe\n      1997\n      11404948.0\n      Africa\n    \n    \n      Zimbabwe\n      2002\n      11926563.0\n      Africa\n    \n    \n      Zimbabwe\n      2007\n      12311143.0\n      Africa\n    \n  \n\n1704 rows × 3 columns\n\n\n\nNotice that, because we specified an index, the index now has a name! You can access the name of the index via df_challenge.index.name:\n\ndf_challenge.index.name\n\n'country'"
  },
  {
    "objectID": "21_getting_data_with_pandas.html#learning-about-our-data",
    "href": "21_getting_data_with_pandas.html#learning-about-our-data",
    "title": "3  Getting Data With Pandas",
    "section": "3.3 Learning about our data",
    "text": "3.3 Learning about our data\nOkay, so now we have a dataset. Great! Now what can we do with it? In the next few sessions, we will explore in detail some of the more in-depth tools that pandas gives us. For now, let’s stick to learning how to view different portions of the data, as well as learning how to describe the overall dataset.\nWhen viewing data, we do not want to be scrolling past multiple lines of individual rows of the data. This might be a shift in mindset if you are used to working with tables of data directly in front of you! An excel spreadsheet just has all the data right there for you to look at! Why not do that here? The simple answer is magnitude. If you only have 10s to 100s of rows of data, seeing it visually is okay. But once you start to deal with thousands, millions or even trillions of rows of data, it’s going to take a while to scroll through the entire thing. At this stage, the important piece of information is how we are treating the data we see, not the actual values.\nTypically, we just want to view a small slice of the data to get an understanding of the types of data we have in our dataset. We have three tools in the toolkit for this:\n\nhead(): This returns the first N rows of data (default N = 5)\ntail(): This returns the last N rows of data (default N = 5)\nsample(): This returns a random sampling of N rows of data (default N = 1)\n\n\nprint('The first 5 rows of data:')\ndisplay(df.head())\n\nprint('The last 3 rows of data:')\ndisplay(df.tail(3))\n\nprint('A random sampling of 7 rows of data:')\ndisplay(df.sample(7))\n\nThe first 5 rows of data:\n\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n  \n\n\n\n\nThe last 3 rows of data:\n\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      1701\n      Zimbabwe\n      1997\n      11404948.0\n      Africa\n      46.809\n      792.449960\n    \n    \n      1702\n      Zimbabwe\n      2002\n      11926563.0\n      Africa\n      39.989\n      672.038623\n    \n    \n      1703\n      Zimbabwe\n      2007\n      12311143.0\n      Africa\n      43.487\n      469.709298\n    \n  \n\n\n\n\nA random sampling of 7 rows of data:\n\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      603\n      Guatemala\n      1967\n      4690773.0\n      Americas\n      50.016\n      3242.531147\n    \n    \n      558\n      Gambia\n      1982\n      715523.0\n      Africa\n      45.580\n      835.809611\n    \n    \n      584\n      Ghana\n      1992\n      16278738.0\n      Africa\n      57.501\n      925.060154\n    \n    \n      76\n      Austria\n      1972\n      7544201.0\n      Europe\n      70.630\n      16661.625600\n    \n    \n      981\n      Mauritius\n      1997\n      1149818.0\n      Africa\n      70.736\n      7425.705295\n    \n    \n      722\n      Iran\n      1962\n      22874000.0\n      Asia\n      49.325\n      4187.329802\n    \n    \n      788\n      Jamaica\n      1992\n      2378618.0\n      Americas\n      71.766\n      7404.923685\n    \n  \n\n\n\n\nOnce we have looked at the data, and it seems to look normal at first glance, we can ask some basic questions about the dataset. How many columns are there? How many rows? Are there null values in any of our columns? What about some basic statistics??\nLuckily for us, pandas has done all of the hard work here. Two valuable methods built into pandas will give us basic information about the overall dataset: .info() and .describe().\n.info() will gives us basic information about each column: what data type it is storing and how many non-null values are in the column.\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1704 entries, 0 to 1703\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   country    1704 non-null   object \n 1   year       1704 non-null   int64  \n 2   pop        1704 non-null   float64\n 3   continent  1704 non-null   object \n 4   lifeExp    1704 non-null   float64\n 5   gdpPercap  1704 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 80.0+ KB\n\n\n.describe() will give us basic statistical information about every numerical column: mean, standard deviation, quartiles, and counts are all included with a call to a single method!\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      count\n      1704.00000\n      1.704000e+03\n      1704.000000\n      1704.000000\n    \n    \n      mean\n      1979.50000\n      2.960121e+07\n      59.474439\n      7215.327081\n    \n    \n      std\n      17.26533\n      1.061579e+08\n      12.917107\n      9857.454543\n    \n    \n      min\n      1952.00000\n      6.001100e+04\n      23.599000\n      241.165876\n    \n    \n      25%\n      1965.75000\n      2.793664e+06\n      48.198000\n      1202.060309\n    \n    \n      50%\n      1979.50000\n      7.023596e+06\n      60.712500\n      3531.846988\n    \n    \n      75%\n      1993.25000\n      1.958522e+07\n      70.845500\n      9325.462346\n    \n    \n      max\n      2007.00000\n      1.318683e+09\n      82.603000\n      113523.132900\n    \n  \n\n\n\n\nFinally, if we want basic information about the non-numerical columns, we can use the value_counts() method. For a given series (or multiple series), this tells us how freqeuntly a given value appears. We will learn more about what this is doing under the hood when we learning about aggregation methods in a later section, but we can apply it to singular text columns here as a teaser\n\ndf.country.value_counts()\n\nAfghanistan          12\nPakistan             12\nNew Zealand          12\nNicaragua            12\nNiger                12\n                     ..\nEritrea              12\nEquatorial Guinea    12\nEl Salvador          12\nEgypt                12\nZimbabwe             12\nName: country, Length: 142, dtype: int64\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\nWhat will happen if we run the following code:\ndf.sample(42).describe()\n\nDo we expect the results to be the same as df.describe()? Why or why not?\nRun the code again. Are the results the same or different than before? Can you explain?\n\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\n\nThis should output a different result from df.describe(). We can break this down into two portions:\n\n\nWe create a new dataframe that holds the sampled dataframe via df.sample(42)\n\n\ndf_sample = df.sample(42)\n\n\nWe are now sending the results of this sampling to the describe method. Because the sampled dataset has only 42 rows that were randomly chosen from the original 1704, it would be an impressive coincidence if all the outputs were identical!\n\n\ndf_sample.describe()\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      count\n      42.000000\n      4.200000e+01\n      42.000000\n      42.000000\n    \n    \n      mean\n      1978.071429\n      2.646120e+07\n      60.637452\n      8089.080828\n    \n    \n      std\n      17.127062\n      5.099286e+07\n      12.574491\n      9032.901581\n    \n    \n      min\n      1952.000000\n      1.916890e+05\n      35.463000\n      464.099504\n    \n    \n      25%\n      1963.250000\n      3.559072e+06\n      50.614500\n      1468.663245\n    \n    \n      50%\n      1977.000000\n      6.432672e+06\n      60.434500\n      5591.529303\n    \n    \n      75%\n      1992.000000\n      2.895611e+07\n      71.421750\n      9719.945968\n    \n    \n      max\n      2007.000000\n      3.011399e+08\n      78.471000\n      42951.653090\n    \n  \n\n\n\n\n\nThe results should be different from the previous call! This is because sample() outputs a random sampling of the dataframe. Everytime we sample the dataset, we get a different subset! Each subset should end up with slightly different statistics if it is small enough relative to the entire dataset!\n\n\ndf.sample(42).describe()\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      count\n      42.000000\n      4.200000e+01\n      42.000000\n      42.000000\n    \n    \n      mean\n      1976.761905\n      3.112765e+07\n      57.949405\n      5672.138801\n    \n    \n      std\n      15.576420\n      1.336669e+08\n      12.206612\n      6809.997438\n    \n    \n      min\n      1952.000000\n      1.276170e+05\n      35.985000\n      378.904163\n    \n    \n      25%\n      1963.250000\n      2.869863e+06\n      47.918750\n      1394.532855\n    \n    \n      50%\n      1972.000000\n      6.248018e+06\n      57.849000\n      3021.661221\n    \n    \n      75%\n      1987.000000\n      9.843238e+06\n      69.729500\n      6730.322005\n    \n    \n      max\n      2002.000000\n      8.720000e+08\n      78.160000\n      33519.476600"
  },
  {
    "objectID": "21_getting_data_with_pandas.html#saving-our-data",
    "href": "21_getting_data_with_pandas.html#saving-our-data",
    "title": "3  Getting Data With Pandas",
    "section": "3.4 Saving our data",
    "text": "3.4 Saving our data\n\n\n\n\n\n\n\n\n\nYes, you can even save to a pickle.\n\n\n\n\n\nWhat do we do once we have cleaned up our data or produced some analysis? It is very likely that we will want to save that clean dataset or analysis to a new file. Pandas to the rescue! As simple as it is to read in data via read_csv() or read_excel(), we can export it back out. While I’ve shown the entire list of to_file() options available in pandas (it’s extensive!), we will focus on to_csv(). Required arguments to this method are:\n\npath_or_buf - full path/filename where you wish to save this file\n\nThat’s it! However, there are some useful optional arguments as well:\n\nindex: True or False. Whether we wish to include the index in our output (default is True). We will often want to set this to False, as the index is just a set of integers labeling the row numbers.\ncolumns: list of columns to keep in the output\n\n\n\n\n\n\n\npd.method() or df.method()?\n\n\n\nSometimes, in order to access a function, we directly access it via the library (pd), or we access it as a method of the dataframe we are using (df). It can be hard to keep track of which functions live where. As a general rule of thumb, if the method is being used to do something to a specific dataframe, it probably belongs to the dataframe object (which is, let’s be honest, most of the functions we might use). Don’t be afraid to use all of your resources to keep it straight! (Yes, google counts as a valid resource). Using the help function is also a quick and easy way to check where a method lives.\nFor example, to find the to_csv() function, we can see that it belongs to the dataframe by checking help(df.to_csv). However, trying to use pd.to_csv will throw an error - a good hint that it was a dataframe method all along!\n\nhelp(pd.to_csv)\n\nAttributeError: module 'pandas' has no attribute 'to_csv'\n\n\nInversely, the read_csv function belongs directly to pandas, and so trying something like this will break:\n\nhelp(df.read_csv)\n\nAttributeError: 'DataFrame' object has no attribute 'read_csv'\n\n\n\n\n\n\n\n\n\n\nChallenge 3\n\n\n\nSave a copy of all of the summary statistics for the gapfinder dataset. Only include the statistics for the pop and lifeExp columns. What happens when we include or exclude the index?\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nFirst, we want to make sure we have a folder to save our outputs to. I have made a folder called outputs that lives at the same level as the data folder. Next, we can invoke the to_csv method on a copy of our descriptive statistics:\n\ndf_descriptive = df.describe()\ndf_descriptive.to_csv('../outputs/challenge_output.csv', columns=['pop', 'lifeExp'])\n\nIn this case, if we exclude the index, we will actually lose information about what each row represents. This is one case when keeping the index will retain valuable information!\n\ndf.describe().to_csv(\n    '../outputs/challenge_output_no_index.csv', \n    columns=['pop', 'lifeExp'],\n    index=False\n    )\n\n\n\n\nUp next, we will learn how to clean our data."
  },
  {
    "objectID": "32_graphical_depictions_of_data.html#introduction-to-matplotlib",
    "href": "32_graphical_depictions_of_data.html#introduction-to-matplotlib",
    "title": "4  Graphical Depictions of Data",
    "section": "4.1 Introduction to matplotlib",
    "text": "4.1 Introduction to matplotlib\nmatplotlib is the most widely used scientific plotting library in Python. For many data related purposes, the sub-library called matplotlib.pyplot is all that is necessary to use. A fantastic feature of the Jupyter environments is that anytime we create a plot, we can view it directly inline with our code, allowing us to make adjustments quickly and easily as we go along.\nWe will typically import this library with the name plt:\n\nimport matplotlib.pyplot as plt\n\nSimple plots are then straightforward to create:\n\nx = [1, 2, 3, 4, 5]\ny = [1, 8, 27, 64, 125]\n\nplt.plot(x, y)\n\n\n\n\n\n\n\n\n\n\nplt.show()\n\n\n\nIn Jupyter environments, running the cell that produces a plot will generate the figure directly below the code, and then the figure is saved with the notebook document for future viewing. However, other Python environments, such as an interactive Python session started from a terminal, or a Python script executed at the command line or within VSCode, require an additional command to display the figure. To do this, we use the basic call:\nplt.show()\nDirectly after creating our plot using plt.plot(...).\nThis command can also be used within a Notebook - for instance to display multiple figures if they are created within a single cell:\n\n# create some lists of data\nx = [1, 2, 3, 4, 5]\ny1 = [2, 4, 6, 8, 10] \ny2 = [1, 4, 9, 16, 25]\n\n# plot and display x vs y1\nplt.plot(x, y1)\nplt.show()\n\n# plot and display x vs y2\nplt.plot(x, y2)\nplt.show()\n\n\n\n\n\n\n\nWe recommend as accepted practice to always include a call to plt.show() if you ever intend to show a plot, in a notebook or otherwise.\n\n\nPlots typically require some set of values to supply to the x value, and an equal length set of values to supply to the y value. If we supply mismatched data, we will get an error:\n\nx = [1, 2, 3, 4, 5]\ny = [1, 8, 27, 64]\nplt.plot(x, y)\n\nValueError: x and y must have same first dimension, but have shapes (5,) and (4,)\n\n\n\n\n\nIn the above examples, we used lists of numbers to supply for our dataset. However, we can also use values directly from pandas dataframes. So let’s work on a new dataset in a pandas dataframe. We will load this dataset from the Seaborn package. Seaborn is another plotting library which we will learn how to use in subsequent sections - but it also has a great built-in dataset that we are going to use to demonstrate matplotlib now, and seaborn later. This dataset looks at species of penguins, and compares various anatomical body part sizes to their species, location and sex.\n\n\n\nload_dataset('penguins')\n\n\n\nfrom seaborn import load_dataset\n\npenguins = load_dataset('penguins')\npenguins.sample(10)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      311\n      Gentoo\n      Biscoe\n      52.2\n      17.1\n      228.0\n      5400.0\n      Male\n    \n    \n      198\n      Chinstrap\n      Dream\n      50.1\n      17.9\n      190.0\n      3400.0\n      Female\n    \n    \n      93\n      Adelie\n      Dream\n      39.6\n      18.1\n      186.0\n      4450.0\n      Male\n    \n    \n      156\n      Chinstrap\n      Dream\n      52.7\n      19.8\n      197.0\n      3725.0\n      Male\n    \n    \n      24\n      Adelie\n      Biscoe\n      38.8\n      17.2\n      180.0\n      3800.0\n      Male\n    \n    \n      289\n      Gentoo\n      Biscoe\n      50.7\n      15.0\n      223.0\n      5550.0\n      Male\n    \n    \n      338\n      Gentoo\n      Biscoe\n      47.2\n      13.7\n      214.0\n      4925.0\n      Female\n    \n    \n      306\n      Gentoo\n      Biscoe\n      43.4\n      14.4\n      218.0\n      4600.0\n      Female\n    \n    \n      130\n      Adelie\n      Torgersen\n      38.5\n      17.9\n      190.0\n      3325.0\n      Female\n    \n    \n      231\n      Gentoo\n      Biscoe\n      49.0\n      16.1\n      216.0\n      5550.0\n      Male\n    \n  \n\n\n\n\nLet’s try comparing the length of the penguin bill to the depth of the bill.\n\nplt.plot(penguins['bill_length_mm'], penguins['bill_depth_mm'])\nplt.show()\n\n\n\n\nOkay, that doesn’t look great. That is because the default behaviour of plot is to draw a line plot, which connects all the data points in the order in which they are given. For this type of comparison, a different matplotlib plot may work better. Some of the basic plots include:\n\nplt.plot(x, y) - produces a line plot of x versus y\nplt.scatter(x ,y) - produces a scatter plot of x versus y\nplt.bar(x, height) - produces a bar plot with bars of height ‘height’ positioned at x. Typically reserved for aggregated data!\nplt.hist(x) - produces a simple box histogram for a single column of data\n\nLet’s try to look at the bills again, but this time with a more appropriate comparison plot:\n\nplt.scatter(penguins['bill_length_mm'], penguins['bill_depth_mm'])\nplt.show()\n\n\n\n\nThat’s looking better. But we still need to clean up our plot a little bit. Matplotlib includes methods for adding axis labels, titles, legends and so on. The trick is in figuring out how to apply these methods to the right plot…\n\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.title('Penguin Beaks')\nplt.show()\n\n\n\n\nWhat happened here! Matplotlib drew an axis for us, but neglected to include the data we had provided above. The key to this is in the way that matplotlib interprets the current figure. Anytime we begin a plot method with plt.method()..., matplotlib recognizes this plot instruction to be part of the current figure. Matplotlib will then go through all of the plot instructions one-by-one, adding each individual piece to the current figure. This continues until we hit the plt.show() line, which tells the program we are done adding to this plot, and any new commands should belong to a new plot.\n\n\n\n\n\n\nJupyter vs. Python\n\n\n\nThere is a slight distinction between how Jupyter will handle figures vs. how a Python script wil handle figures. When Jupyter hits the end of a cell or codeblock, it will automatically show the figure at the end of the output, regardless of whether plt.show() was called or not. This is equivalent to calling plt.show() however - a new codeblock will not recognize the code used to create this plot in a new cell.\n\n\nLet’s finally get this all together on one plot:\n\nplt.scatter(penguins['bill_length_mm'], penguins['bill_depth_mm'])\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.title('Penguin Beaks')\nplt.show()\n\n\n\n\nNote that because of this matplotlib behaviour (include every instruction to plt up until it reaches a plt.show() or end of Jupyter cell), we can take advantage to draw multiple plots on a single axis! Some further behaviour includes:\n\nIf a second plot of the same type as the first is created within the same figure, matplotlib will automatically assign it a new colour\nTo distinguish between plots, the argument label = 'plot name' can be provided during creation of the plot. A final call to plt.legend() after creating all plots will produce a legend that consists of the given labels.\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nUsing what we have learned so far about pandas filtering and matplotlib functionality to produce a plot of bill lengths vs. depths, but colours the three penguin species (Adelie, Chinstrap, and Gentoo) different colours. Include a legend that identifies which species is which colour!\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nWe first split up our data into three separate pieces, each filtered to a single species of penguin. Next we plot all three species without displaying the figure. Finally, we add some tidy up code to add the legend and axis titles, and we are good to go!\nOne thing to note is that it is often good practice to use more than just colour to distinguish between different groups of data. Because of this, adding a variation on the marker style is a good idea as well. We have included that in the solution to this challenge. Be sure to explore all the other style settings available within each matplotlib plot!\n\nadelie = penguins[penguins.species == 'Adelie']\nchinstrap = penguins[penguins.species == 'Chinstrap']\ngentoo = penguins[penguins.species == 'Gentoo']\n\n# start creating our plot, individualizing each plot as necessary\nplt.scatter(adelie.bill_length_mm, adelie.bill_depth_mm, marker='o', label='Adelie')\nplt.scatter(chinstrap.bill_length_mm, chinstrap.bill_depth_mm, marker='x', label='Chinstrap')\nplt.scatter(gentoo.bill_length_mm, gentoo.bill_depth_mm, marker='^', label='Gentoo')\n\n# apply overall plot commands after\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.title('Penguin Beaks')\nplt.legend()\n\n# show our plot\nplt.show()"
  },
  {
    "objectID": "32_graphical_depictions_of_data.html#plotting-directly-from-pandas",
    "href": "32_graphical_depictions_of_data.html#plotting-directly-from-pandas",
    "title": "4  Graphical Depictions of Data",
    "section": "4.2 Plotting Directly From Pandas",
    "text": "4.2 Plotting Directly From Pandas\nThat was a lot of work to make a relatively straightforward plot. While matplotlib is extremely versatile, this same versatility comes at the expense of having to tweak the underlying code frequently to make the plots look the way that we want. Luckily, there exist other libraries that have been built on top of matplotlib that take care of much of the abstraction, so we can get directly to looking at our results as fast as possible!\nOne extremely straightforward way to plot is directly from pandas, which implicitly uses matplotlib.pyplot in the background.\n\npenguins.plot.scatter(x = 'bill_length_mm', y = 'bill_depth_mm')\nplt.show()\n\n\n\n\nFor some plot types, we are also given the ability to provide a grouping category.\n\npenguins.plot.hist(column='flipper_length_mm', by='sex')\nplt.show()\n\n\n\n\nWhile pandas provides a straightforward way to quickly look at the data, it is fairly limited, and we will still end up needing to default back to matplotlib methods to get high-quality plots. The next package will help with this!"
  },
  {
    "objectID": "32_graphical_depictions_of_data.html#introduction-to-seaborn",
    "href": "32_graphical_depictions_of_data.html#introduction-to-seaborn",
    "title": "4  Graphical Depictions of Data",
    "section": "4.3 Introduction to Seaborn",
    "text": "4.3 Introduction to Seaborn\nSeaborn is a library for making statistical graphics in Python. It builds on top of matplotlib but is uniquely built to integrate closely with pandas data structures. Seaborn is dataset-oriented, and was built specifically to let you focus on what the different elements of a plot mean, rather than on the details of how to draw it. Leveraging seaborn allows us to build high quality graphics in a minimal amount of code. This not only allows us to spend more time analysing outputs and results, but also makes the code more transparent and easier for others to follow and understand.\nSeaborn is usually imported with the shorthand sns. On top of this, it is often nice to reset the default matplotlib colour theme and layout to the default seaborn layout, which has more subdued colours and tones. This can be done globally via the .set_theme() method. Changing the theme will apply to any other plots created in a notebook/script, even if they are created specifically by matplotlib. More options for the theme or style (which can also be set via .set_style()) can be found in the Seaborn aesthetics tutorial.\n\nimport seaborn as sns\nsns.set_theme()\n\nWithin seaborn, we will focus on three main plot types:\n\nRelational plots\nDistributions\nCategorical plots\n\nAll three of these modules have a higher figure-level interface (relplot, displot, catplot) that have options to produce the different subvarieties. They also have an axes-level interface (for example, histplot inside the displot category) that allows for more control over the matplotlib backend being used to produce the plot. For our purposes, we will stick to the high level interface as it is capable of producing beautiful plots with a few simple commands.\n\n\n\nSeaborn plot categories.\n\n\n\n\n\n\n\n\nAdvanced Tip - Figure vs. Axes Level?\n\n\n\n\n\n\n\n\nMatplotlib Figures\n\n\nYou will notice that I mentioned figure vs. axes level interfaces for seaborn. This is a small but subtle distinction that is worth explaining in more detail. When we introduced matplotlib, we explained that you can create multiple different plots all on the same figure. This is because matplotlib plots are individually drawn onto a common axis, but each individual plot does not own the axis, titles, legends, and so on. Matplotlib produces a base axis, then draws each individual plot on top, and then adds the extra pieces as requested. All the accompanying fluff (title, legend) that surrounds the axis is part of the overall figure, which we set with commands such as plt.xlabel(), plt.title() and so on.\nSo a figure can be thought of as the overall container holding each plot and accompanying axis labels. In seaborn, the figure-level interface we will be using is just this - figure-level. It is a ‘finished product.’ Producing a plot via the relplot, displot, and catplot methods will produce an entire figure, complete with titles, legends, and anything else we might specify, such as the figure size. However, this entire figure is no longer easily accessible like a matplotlib axis is (we cannot add a plot like this to a different set of matplotlib plots, for example).\nThis can be useful as it will simplify the amount of code needed to create a report-ready figure. However, the drawback is that it reduces the customizability we have to work with. So if we wish to work with customizable axes that we can quickly drop into more complex matplotlib plots, we should use the axes-level interface (eg. using sns.scatterplot() instead of sns.relplot()). If we want to produce standard statistical plots, the figure-level interface is the recommended tool as they produce cleaner plots.\nCheck out the Seaborn tutorial for more details on the differences between these methods!\n\n\n\nBefore we dive into the particulars, here’s a wonderful one line command to show just how powerful seaborn can be!\n\nsns.pairplot(penguins, hue='species', height=1.75)\nplt.show()\n\n\n\n\n\nRelational Plots\n\n \n\nCorrelation \\(\\neq\\) Causation\n\n\n\nRelational plots let us quickly look for patterns in our dataset between two different features. The most common approach is to use scatter or lineplots, depending on the type of data we have. A timeseries, for example, will often be displayed with a lineplot, while any two numerical features can be plotted against each other with a scatterplot.\nWhen we use the high-level relplot method, we need to supply our pandas dataframe, as well as which columns we are interested in plotting. The default plot type is to use a scatterplot:\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm')\nplt.show()\n\n\n\n\nHowever, we can force the line plot by including the optional kind argument:\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', kind='line')\nplt.show()\n\n\n\n\nWhen we utilize the lineplot option, Seaborn will default to automatically aggregating data along the x-axis, displaying uncertainty in the y-axis with confidence intervals. Note that it also doesn’t care about the initial order of the data. Seaborn automatically assumes that we wanted a nicely flowing line from left to right, and will sort the x values accordingly.\n\n\n\n\n\n\nChallenge 2\n\n\n\nUsing the Seaborn load_dataset method, import a dataset called ‘dowjones’. Answer the following questions:\n\nWhat does this dataset include/represent?\nOn what date does the data represented here reach its highest price?\nCreate a line plot to depict the data. Does the plot agree with the answer you came up with in part 2?\n\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\n\nLet’s get our dataset first and take a quick peek\n\n\ndowjones = load_dataset('dowjones')\ndowjones.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n    \n  \n  \n    \n      0\n      1914-12-01\n      55.00\n    \n    \n      1\n      1915-01-01\n      56.55\n    \n    \n      2\n      1915-02-01\n      56.00\n    \n    \n      3\n      1915-03-01\n      58.30\n    \n    \n      4\n      1915-04-01\n      66.45\n    \n  \n\n\n\n\nIt looks like this is a list of dates and prices. Context clues from the name of the dataset hint that this is probably the Dow Jones Industrial Average. From the looks of the Date column, it is probably a monthly average price!\n\nWe could do this one of two ways. We could sort the dataframe by price and then look at the first data in the sorted dataframe. Alternatively, we can use aggregation and filtering. Note that because we do not want to get statistics per varying group, we do not need to do any groupby() before looking for the max value.\n\n\n# option 1: sort and grab first row\ndate_of_max = dowjones.sort_values(by='Price', ascending=False).head(1)\ndisplay(date_of_max)\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n    \n  \n  \n    \n      613\n      1966-01-01\n      985.93\n    \n  \n\n\n\n\n\n# option 2: aggregate and filter \n# determine the max price in the dataset\nmax_price = dowjones['Price'].max()\n\n# use this max price to filter to the date it occured on\ndate_of_max = dowjones[dowjones['Price']==max_price]\n\n# check out the result \ndisplay(date_of_max)\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n    \n  \n  \n    \n      613\n      1966-01-01\n      985.93\n    \n  \n\n\n\n\n\nWe will use Seaborn to build our lineplot.\n\n\nimport datetime as dt\nsns.relplot(\n    data=dowjones,\n    x='Date',\n    y='Price',\n    kind='line'\n)\nplt.show()\n\n\n\n\nIt does indeed look like the plot is peaking around 1966, which matches what we found earlier! It also looks like you can see the stock market crash that started the Great Depression…\n\n\n\nAlthough relational plots are two dimensional in their presentation, we can add a third dimension to the data by applying the hue, size, and style arguments to secondary columns.\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', hue='species')\nplt.show()\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', style='sex')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWhen used on a non-categorical column, the hue and size arguments will provide sequential coloring and sizing:\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', hue='bill_length_mm')\nplt.show()\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', size='bill_length_mm')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can also mix and match multiple different styling choices. Note that here, we want to be cautious about creating our plots using too many different styling choices. While informative, they can be tricky to interpret if we use too many all at once.\n\nsns.relplot(\n    data=penguins, x='body_mass_g', y='flipper_length_mm', \n    hue='species', style='species'\n    )\nplt.show()\n\nsns.relplot(\n    data=penguins, x='body_mass_g', y='flipper_length_mm', \n    hue='sex', size='bill_length_mm'\n    )\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReminder: Use Multiple Lines!\n\n\n\nIn that last example you’ll notice that I broke the code up for the plot onto multiple lines. When you start to have many arguments being supplied to a function, it is often a good idea to put different arguments on different lines for readability. As long as argument stays with the brackets, you can use as many lines or indentations as you want!\n\n\nIf we want to avoid using too many different styling choices, but we would still like to understand how our features vary across different categories, we can make multiple plots using the built in row and col arguments of the relplot objects. By supplying a category to row (col), Seaborn is told to split the dataset into all the different values in the category. Then each subset of the data is plotted on a different row (column) of the data. The end result is a grid of plots, with each plot having a unique subset of the category represented.\n\ng = sns.relplot(\n    data=penguins, x='body_mass_g', y='flipper_length_mm',\n    row='island', col='species', hue='sex',\n    height=2.5\n)\ng.set_titles(size=8)  # include this line because the titles will overlap otherwise!\nplt.show()\n\n\n\n\nFinally, a frequent goal of creating a scatter plot is to identify if there is a trend in the data. Seaborn offers an extra method, lmplot, to allow for quick viewing of best fit lines (including confidence interval estimates) in our datasets:\n\nsns.lmplot(data=penguins, x='body_mass_g', y='flipper_length_mm', hue='sex')\nplt.show()\n\n\n\n\nNote that because we included a third categorical dimension (hue='sex'), we have actually displayed two different best fit lines: the best relationship between body mass and flipper length for male penguins, and the best relationship for female penguins as well.\nThis method is capable of doing more than the default linear regression! Some such arguments available include:\n\nlogistic: boolean. If True, will estimate a logistic regression.\nrobust: boolean. If True, will de-weight far outliers in performing the regression.\norder: integer. If supplied, will fit a polynomial regression with the given order.\n\n\nSummary\nThat is a lot to take in. To summarize the relational plots available in Seaborn:\n\nrelplot is the go-to tool to create a statistical comparison plot between two numerical columns\nThese plots default to scatter, but can be made into lineplots with kind='line'\nstyle, hue, and size can all be adjusted to provide insight into both numerical and categorical features\nrow and col will split our plot out into multiple facets, according to the categories found in the datasets\nlmplot can be used to provide basic regression fits to the data\n\n\n\n\n\n\n\nChallenge 3\n\n\n\nChoose any two numerical features in the penguins dataset. Produce a scatter plot that highlights the differences between species. Separate the results into two plots - one for male and one for female penguins.\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nYou might produce code that looks similar to this. col and row could be interchanged, and maybe you chose to use just one of style or hue. Explore your options!\n\ng = sns.relplot(\n    data=penguins, x='flipper_length_mm', y='bill_length_mm',\n    hue='species', style='species', row='sex'\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDistribution Plots\n\n\n\nSpoooky\n\n\nAnother early step that we often take when analyzing our data is to understand how the features are distributed. We can quickly answer questions such as:\n\nWhat is the range of observations in the dataset?\nIs the data skewed?\nDo we have outliers?\nHow do different subsets compare?\n\nUsing the displot method, we can access multiple different styles of plots to answer these questions. While Seaborn can (and does) get into multivariate distributions, let us stick to univariate distributions: histograms and kernel density estimation (KDE).\nThe default displot option will produce a histogram of whichever column we choose in our dataset:\n\nsns.displot(data=penguins, x='flipper_length_mm')\nplt.show()\n\n\n\n\nSeaborn automatically chooses what it feels is a reasonable number of bins for the dataset, but this is of course customizable via either the binwidth or the bins arguments, which will force the width or number of bins, respectively.\n\nsns.displot(data=penguins, x='flipper_length_mm', binwidth=1)\nplt.show()\n\nsns.displot(data=penguins, x='flipper_length_mm', bins=5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nJust like with our relational plots, we can also condition our histograms on other features, colouring each member of the category separately.\n\nsns.displot(data=penguins, x='flipper_length_mm', hue='sex')\nplt.show()\n\n\n\n\nThe default here is to create an overlapping histogram, but we could create stacked histograms, dodged (side-by-side) histograms, or even use the col or row options to produce multiple plots!\n\nsns.displot(data=penguins, x='flipper_length_mm', hue='sex', multiple='stack')\nplt.show()\n\nsns.displot(data=penguins, x='flipper_length_mm', hue='sex', multiple='dodge')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nsns.displot(data=penguins, x='flipper_length_mm', col='sex', height=3.5)\nplt.show()\n\n\n\n\nWhile the histogram option provides the exact count of the underlying values contained in our dataset, sometimes we may wish to approximate the distribution of data. This is done using kernel density estimation, which plots a smooth, continuous density estimate. Just like with the relational plots, we can access this new plot type using the kind='kde' option.\n\nsns.displot(data=penguins, x='flipper_length_mm', kind='kde')\nplt.show()\n\n\n\n\nJust like we were able to adjust the bin sizes for the histogram, we can adjust the ‘bandwidth’ of our estimation. This will vary the amount of smoothing that is applied in the end distribution:\n\nsns.displot(data=penguins, x='flipper_length_mm', kind='kde', bw_adjust=0.25)\nplt.show()\n\nsns.displot(data=penguins, x='flipper_length_mm', kind='kde', bw_adjust=4)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFinally, we can have both types of plots in action at the same time by including the optional kde argument:\n\nsns.displot(data=penguins, x='flipper_length_mm', kde=True, hue='species')\nplt.show()\n\n\n\n\n\nSummary\nTo summarize distributions available in Seaborn:\n\ndisplot is the go-to tool to create a statistical comparison plot between two numerical columns\nThese plots default to histograms, but other options are available:\n\nkde will produce an estimation of the underlying density\necdf will produce a cumulative distribution function\n\nA shortcut to include both a histogram and a KDE can be used by setting the argument kde=True\nhue can be adjusted to provide insight into extra categorical features\nrow and col will split our plot out into multiple facets, according to the categories found in the datasets\n\n\n\n\n\n\n\nChallenge 4\n\n\n\nCreate a KDE plot of body mass. Does it appear to be bi-modal (2 peaks)? If so, create another plot (or more) to identify what may be the cause. Are male penguins heavier? Is it a specific species? Does the island matter?\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nFirst let’s just look at the body mass variable alone.\n\nsns.displot(\n    data=penguins, x='body_mass_g', kind='kde'\n    )\nplt.show()\n\n\n\n\nHmm.. it’s not immediately obvious that there might be two peaks here, but it does seem to skew a little bit to lower values. Let’s see if looking at the different sexes gives us any more information:\n\nsns.displot(\n    data=penguins, x='body_mass_g', kind='kde', hue='sex',\n    fill=True  # Use fill to fill the KDE plot!\n    )\nplt.show()\n\n\n\n\nOkay, weird. Splitting by sex does start to show two peaks, but it shows up in both male and female! So maybe it’s the species where the peak is showing up?\n\nsns.displot(\n    data=penguins, x='body_mass_g', kind='kde', hue='species', col='sex',\n    fill=True \n    )\nplt.show()\n\n\n\n\nAha - looks like we found a key difference here! While males tend to be heavier than the females on average, there are distinctly different distributions for the three species, with the Gentoo penguins being heavier than both the Adelie and Chinstrap. Doing this sort of exploration via plot is always a useful tool to learn more about our data!\n\n\n\n\n\n\nCategorical Plots\n\n\n\nSuspicious…\n\n\nRelational plots let us quickly view relationships between two sets of numerical features. Categorical plots will allow us to do the same where one (or both) of the features is categorical (divided into discrete groups).\nThe catplot method will provide us a unified high-level interface to a variety of different plots. The default behaviour of catplot is to produce a non-aggregated view of the categories in a strip plot.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm')\nplt.show()\n\n\n\n\nThis is a categorical version of a scatter plot, in which all of the data for a single category is plotted at the same horizontal position, but with a small amount of horizontal ‘jitter’ added so that data points with identical numerical values do not completely overlap. This gives us a general idea of how the data is distributed within a category: do most of the penguins have similar flipper lengths within a species, or do they spread out to cover a wide range?\nThe most commonly used categorical plot however, is the bar plot. Bar plots are something that we are likely all familiar with, as human beings living in a society of.. people. However, typically when you see a bar plot, it is representing some sort of aggregation of our data. What is the average flipper length of the various penguin species? How many penguins live on each island? As the data stands, we do not have this information directly - it is non-aggregated, row level data. We could use our skills with Pandas to group the data according to the categories of interest and apply some statistic measures to the numerical columns of interest, and then use a matplotlib or Seaborn plot to display the results of that aggregation. However, Seaborn has developed tools that allow us to skip the grouping steps ourselves and allow the plotting package to do the grouping and statistical analysis behind the scenes. This has two advantages:\n\nThe code is shorter and more precise, leading to easier understanding for others. This helps with code transparency.\nSeaborn also provides confidence intervals to include in its aggregated bar plots, which would be an extra level of complexity added to a manual grouping of the data.\n\nIn Seaborn, a barplot operates on the full dataset to obtain an estimate of some aggregation function (which is the mean by default). It will also default to providing an estimation of the confidence interval on its estimates. Just like with all of the other Seaborn plots, we can add an additional feature for free via the hue styling choice.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex')\nplt.show()\n\n\n\n\nIf we wish to summarize a different aggregate statistic, there are built in options: estimator='mean', 'median','min', and 'max' will all work.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex', estimator='max')\nplt.show()\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex', estimator='min')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Tip - Callable Estimators!\n\n\n\n\n\nIn addition to the built in estimators, we can also use estimators for external functions. The function is required to take as an input a vector of values, and output a single value that summarizes that vector. The Python library numpy is the go-to resource for mathematical functions in Python. If we use numpy, we can send any sort of estimator to the barplot! Common statistical functions in numpy include sum, prod, mean, std, and var.\n\nimport numpy as np\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex', estimator=np.sum)\nplt.show()\n\n\n\n\n\n\n\nOne last unique piece that you might be interested in is the orientation of the plot. In the above examples, we gave our categorical category to the x feature, and y held the numerical values. If we wish to have a horizontal set of bars instead, we can simply swap these:\n\nsns.catplot(\n    data=penguins, x='flipper_length_mm', y='species',\n    kind='bar', hue='sex')\nplt.show()\n\n\n\n\n\nSummary\nTo summarize the categorical plots available in Seaborn:\n\ncatplot is the go-to tool to create a statistical comparison plot between a numerical and categorical column\nThese plots default to categorical swarmplots (no, I do not know why bar is not the default) but other options are available. These options can be organized into three broad subcategories.\n\nCategorical scatterplots:\n\nswarm\nstrip\n\nCategorical distribution plots:\n\nbox\nviolin\nboxen\n\nCategorical estimate plots:\n\npoint\nbar\ncount\n\n\nhue can be adjusted to provide insight into extra categorical features\nrow and col will split our plot out into multiple facets, according to the categories found in the datasets\nBy swapping the x and y variables, we can choose which orientation the plot will display in\n\n\n\n\n\n\n\nChallenge 5\n\n\n\nWhich penguin species was most frequently included in this dataset? Display your result visually!\n\n\n\n\n\n\n\n\nSolution to Challenge 5\n\n\n\n\n\nTo get the count for each of the species, we need to make a countplot:\n\nsns.catplot(data=penguins, x='species', kind='count')\nplt.show()\n\n\n\n\nDone, in one line of code!\nAs a bonus, if we want to plot the data in descending order, we can pass a list of strings containing the categories to the catplot method:\n\nsns.catplot(\n    data=penguins, x='species', \n    kind='count', \n    order=['Adelie', 'Gentoo', 'Chinstrap']\n    )\nplt.show()"
  },
  {
    "objectID": "32_graphical_depictions_of_data.html#saving-your-plots",
    "href": "32_graphical_depictions_of_data.html#saving-your-plots",
    "title": "4  Graphical Depictions of Data",
    "section": "4.4 Saving your Plots",
    "text": "4.4 Saving your Plots\nWe’ve now done all this work to make amazing incredible charts that produce stunning visuals for our datasets. Chances are, however that you do not want these images to live solely in your Jupyter notebook. You might want it for a presentation or a report. So, we need to save these images so we can access them outside of our Python environment. To do this we use the plt.savefig() method. This method will save the currently open figure. This is the figure that has not yet been shown on screen! Any call to save a plot must come before we display it via plt.show(), as this actually closes the figure/plotting device so we can start our next one!\n\n\n\n\n\n\nNo Show and Tell!\n\n\n\nThis point is important enough that I am repeating it here. Always save your figure before showing it!\n\n\nLet’s create a plot, view it, and save it. We will save it to the same outputs folder we used in a prior section.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex'\n    )\nplt.savefig('../outputs/penguins.png', bbox_inches='tight')\nplt.show()\n\n\n\n\nYou will notice that I used an optional argument here called bbox_inches. This is entirely optional, but when used it will cut out any empty whitespace on the sides of the figure before saving. I find it makes for cleaner looking images at the end, so I will often use this argument. And that’s it, we now have our image saved and ready for use elsewhere!\n\n\n\n\n\n\nAdvanced Tip - Changing the Figure Size\n\n\n\n\n\nWe might want to vary the shape or size of the plot we are producing. There are two ways to do this, depending on the type of figure we have created.\n\nAxes-Level (Matplotlib Style) Figures\nIf we are working directly with matplotlib or the axes-level seaborn functions, this is done by initializing the figure with a figsize command before creating any plot elements:\nplt.figure(figsize=(10, 8))\nplt.plot(...)\nsns.scatterplot(...)\nplt.savefig()\nplt.show()\nThe figsize argument inside the plt.figure method specifies the required width and height in inches.\n\n\nFigure-Level (Seaborn Style) Figures\nIf we are working with the figure-level Seaborn interface, we cannot access the figure attributes externally. However, Seaborn has given us arguments we can pass directly to the plot method itself to set the figure size via the height and aspect arguments.\nsns.relplot(data=..., height=5, aspect=0.5)\nplt.savefig()\nplt.show()\nThe height argument indicates the required height of each subplot in inches, while the aspect argument supplies the aspect ratio such that the width = aspect * height. Note that height applies to every individual subplot in the plot, not the overall image!"
  },
  {
    "objectID": "32_graphical_depictions_of_data.html#online-galleries",
    "href": "32_graphical_depictions_of_data.html#online-galleries",
    "title": "4  Graphical Depictions of Data",
    "section": "4.5 Online Galleries",
    "text": "4.5 Online Galleries\nAll of the tools described above are a great way to start visualizing your data, but there are many other libraries and tools available. If you ever need inspiration for a new visual, or simply need a reminder on what code you would need to create a specific chart, the Python Graph Gallery is a fantastic resource. It contains examples as well as code for over 40 different types of charts using Python code."
  }
]