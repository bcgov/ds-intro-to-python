[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science using Python",
    "section": "",
    "text": "This is a repository to house materials for a 2 day course introducing participants to data science using Python.\nThe goal of this workshop is to teach new-to-programming data professionals to import data, clean up and summarize a data set, and make some static data visualizations using Python. This is an introductory course to programming, specifically programming with Python. Python is a popular computer language for statistics and other scientific disciplines. It is commonly used for statistical analysis, machine-learning, generating high quality visualzations, and automating data workflows.\nThe workshop content will follow best practices for Python for data analysis, giving attendees a foundation in the fundamentals of Python and scientific computing."
  },
  {
    "objectID": "index.html#who-should-take-this-course",
    "href": "index.html#who-should-take-this-course",
    "title": "Introduction to Data Science using Python",
    "section": "Who should take this course?",
    "text": "Who should take this course?\n\nAnyone who works with data or who is interested in learning efficient ways to make meaning from data\nAnyone keen to learn a programming language (no prior experience necessary!)"
  },
  {
    "objectID": "index.html#workshop-schedule",
    "href": "index.html#workshop-schedule",
    "title": "Introduction to Data Science using Python",
    "section": "Workshop Schedule",
    "text": "Workshop Schedule\n\nDaily Schedule\n\n\n\nActivity\nStart Time\nEnd Time\n\n\n\n\nSession 1\n9:00\n10:30\n\n\nBreak\n10:30\n10:45\n\n\nSession 2\n10:45\n12:00\n\n\nLunch\n12:00\n1:00\n\n\nSession 3\n1:00\n2:30\n\n\nBreak\n2:30\n2:45\n\n\nSession 4\n2:45\n4:30\n\n\n\n\nPre-Course Work\n\n\n\n\n\n\nImportant!\n\n\n\nBefore the course starts, we ask that all attendees install Python and its associated packages required for data analysis! Instructions for how to do so are found on the next page. If anyone is having troubles getting Python up and running, please contact us before the course starts so that we can hit the ground running during the workshop.\n\n\n\n\nDay 1\n\nCourse Introduction (20 min)\nGetting Up and Running (60 min) (Lindsay)\nBREAK ☕\nWorking with Code (90 min) (Stuart)\nLUNCH 🍍\nCore Data Structure Concepts (90 min) (Lindsay)\nBREAK 🍩\nGetting Data (90 min) (Lindsay)\n\n\n\nDay 2\n\nData Cleaning (90 min) (Stuart)\nBREAK ☕\nExploring Data with Pandas (90 min) (Stuart)\nLUNCH 🍍\nGraphical Depictions of Data (90 min) (Lindsay)\nBREAK 🍩\nBonus Content: TBD (90 min) (TBD)\n\nFill in this poll to choose our final topic!"
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Introduction to Data Science using Python",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course has been developed and is being given on the traditional territory of the lək̓ʷəŋən speaking peoples, today known as the Esquimalt and Songhees Nations. As this is a data oriented course, we encourage you to check out some of these resources to learn more about these lands, or the lands that you yourself live, work and play on:\n\nFirst Nations in B.C.\nGlobal Territories Map\nMore on the Songhees Nation\n\nParts of the above lesson material are sourced or adapted from Software Carpentry python courses:\n\nPlotting and Programming in Python, Allen Lee (2018)\nProgramming with Python, Azalee Bostroem (2016)\n\nOriginal Work Copyright © Software Carpentry, content modified by the Province of British Columbia.\nThis work is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n.\n\n\n\n\nAllen Lee, Sourav Singh, Nathan Moore. 2018. “Software Carpentry: Plotting and Programming in Python.” https://github.com/swcarpentry/python-novice-inflammation.\n\n\nAzalee Bostroem, Valentina Staneva, Trevor Bekolay. 2016. “Software Carpentry: Programming with Python.” Version 2016.06, 10.5281/zenodo.57492. https://github.com/swcarpentry/python-novice-inflammation."
  },
  {
    "objectID": "00_introduction_to_python.html#motivation",
    "href": "00_introduction_to_python.html#motivation",
    "title": "1  Getting Up and Running",
    "section": "1.1 Motivation",
    "text": "1.1 Motivation\nData has become interwined with the inner workings of nearly every facet of working within the BC Public Service. Whether you have to read an excel spreadsheet, prepare a report based on a survey, comb through csv files to find a specific data source, it is likely that you have worked with a dataset at some point in your career. However, the process of looking at and dealing with data can be messy, error-prone and hard to duplicate. Questions such as ‘wait, how did I get that number again?’ are all too common.\n\n\n\nThe Messy Side of Data\n\n\nThese lessons will teach you how to interact with data in a systematic way using the python ecosystem. By accessing and interpreting data through a set of prescribed methods (developed through the code written), our work with data becomes more accessible, repeatable, and ultimately insightful.\nDuring the course of these lessons, we hope to cover:\n\nPython preliminaries\nExploring and cleaning raw data\nUsing statistical methods\nPlotting results graphically\n\nIf we have time, we may touch on some more advanced python lessons, such as:\n\nPublishing reports\nAccessing the B.C. Data Catalogue\nMachine learning in python"
  },
  {
    "objectID": "00_introduction_to_python.html#before-starting",
    "href": "00_introduction_to_python.html#before-starting",
    "title": "1  Getting Up and Running",
    "section": "1.2 BEFORE STARTING!!",
    "text": "1.2 BEFORE STARTING!!\nSo that we can hit the ground running with this workshop, we are asking that everyone get some basic python tools downloaded and installed before the workshop starts. Tools that we will use include Anaconda (or Miniconda) as well as VSCode. A basic knowledge of the command line/powershell interface will be useful as well, but we will try to keep our use of this to a minimum.\n\n\n\n\n\n\nImportant\n\n\n\nIf you are having issues installing anything we have requested prior to the start of the workshop, please let us know so we can work with you so that we can hit the ground running!\n\n\n\nAnaconda/Miniconda is used to download, install, and organize both python and any packages/libraries we use within python. The actual program doing the organizing is conda, while we will use an anaconda prompt (not the anaconda powershell prompt!) to do the installs and interface with python.\nVSCode is a tool used to write, edit and test code known as an IDE (Integrated Development Environment). It is available for more languages than just python, and its versatility has made it a widespread tool within the BCPS.\n\n\nInstall our Python Tools\nIf you do not have administrative rights on your computer:\nDownload Anaconda and VSCode from the B.C. Government Software Centre:\n\nInstall Anaconda (Anaconda3X64 2022.05 Gen P0)\nInstall VSCode (VSCodeX64 1.55.2 Gen P0)\n\nIf you do have administrative rights on your computer:\n\nIf you have administrative rights on your computer, we suggest downloading the lightweight version of Anaconda called Miniconda.\n\nLink to instructions here!\n\nFind the latest version of VSCode here.\n\n\n\nInstall some Python Packages\nMost of the time, when using python we are not using it by itself, but in conjunction with powerful libraries that have already been built to make our data analysis easier. In order to use these tools, we have to install them as well. Using a package manager such as conda makes our life much easier, as we can safely install tools into local environments where every library installed is checked for compatability with every other library. By utilizing the local conda environments, we maintain clean working spaces that can be easily reproduced between workstations.\nLet’s run through the basic steps of setting up a conda environment, installing python and some packages, and testing that it worked!\n\n\n\n\n\n\nPrompt not Powershell!\n\n\n\nIn order to get anaconda to play nicely with VS Code later, it is important that we do the following set of steps inside an Anconda Prompt and not the Anaconda Powershell Prompt! These are similar but not identical environments.\n\n\n\nOpen an anaconda prompt from your search bar.\n\n\n\n\nAnaconda Prompt\n\n\n\nMake sure that we have an appropriate version of conda installed. If this is already the installed version, great! If not, press Enter or type Y and hit Enter when asked to proceed (which will apply throughout all of these steps).\n\n\n\nAnaconda Prompt\n\n> conda install conda=4.12\n\n\n\n\nSetting the Conda Version\n\n\n\n\n\n\n\n\n\nPlease update conda?\n\n\n\nThe command prompt might give instructions to update conda to a newer version at this point. Ignore this: as of right now the most recent version has an issue that is not worth working around!\n\n\n\nTo make conda play nicely with VSCode later on, type the following command in the prompt window as well:\n\n\n\nAnaconda Prompt\n\n> conda init\n\n\nInside the anaconda prompt, create a new local conda environment named ds-env using the following commands:\n\n\n\nAnaconda Prompt\n\n> conda create --name ds-env\n> conda activate ds-env\n\n\n\n\nCreating a Conda Environment\n\n\n\nYou should notice that running this second command switches the name in brackets at the beginning of your prompt from (base) to (ds-env). This means we have successfully created a new, empty environment to work in.\n\nInstall python and some useful datascience packages by typing the following commands into the same prompt window:\n\n\n\nAnaconda Prompt\n\n> conda install python=3.9\n> conda install notebook jupyterlab ipywidgets matplotlib seaborn numpy scikit-learn pandas openpyxl\n\n\nMake sure that python installed successfully. From the same anaconda prompt, simply type python. If this causes no error, success! Try typing this command in the python environment that started to make sure the packages installed as well:\n\n\nimport pandas\npandas.__version__\n\n\n\n\nTesting the python installation\n\n\n\nIf this all works with no errors, python was successfully installed.\n\n\nSetup our VSCode Environment\nStill with me? Great. Here’s a cute otter as congratulations for making it this far.\n\n\n\nThe cutest.\n\n\n\nWe have just a few more steps to go.\n\nOpen the VSCode program.\nOn the left toolbar, find the extensions tab (It looks like 4 squares). Search for the python extension and install this extension.\nFor those using Windows computers, change your default terminal to the command prompt:\n\nFrom anywhere inside VSCode, hit Ctrl + Shift + P. This will open up the command palette.\nStart typing Terminal: Select Default Profile until this option pops up.\nChoose this option, and then click on Command Prompt\n\n\nThat’s it. We are ready to go!"
  },
  {
    "objectID": "00_introduction_to_python.html#hello-world",
    "href": "00_introduction_to_python.html#hello-world",
    "title": "1  Getting Up and Running",
    "section": "1.3 Hello World",
    "text": "1.3 Hello World\ni.e. the how many different ways can we print Hello World! to our screen? section\n\n\n\nHello World!\n\n\n\nThere are many different ways in which we can interact with python. These include:\n\nFrom the command line\nInside a Jupyter Notebook\nFrom a file (inside VSCode)\n\nIn this next section, we are going to have a brief introduction to all of these methods of interaction.\n\n\n\n\n\n\nTip: Using the command line\n\n\n\nIt’s worth pointing out that the methods that we will focus on in this course will rely on using VSCode or JupyterLab and all of its inner workings. However, if you are comfortable with the command line, we can also access any of these methods directly from there as well, you just need to be able to move to directories before typing commands. If you use the command line, I recommend using an anaconda powershell prompt, as this allows for the easiest use and access to conda commands and only the smallest of headaches.\n\n\n\nStep 0\nIn all cases, we will want to have a folder from which we wish to work out of. Take some time to set up a folder somewhere you won’t lose it. For me, I’ve simply made a folder called Intro to Python on my C: drive that will hold any course materials we use/create here.\nNext, to make any interactions with python, we will want to open VSCode and work from here. When we first open VSCode, you should be prompted to open a folder. We are going to work out of that Intro to Python folder, so open it here. After doing this, we should now have a VSCode screen open that will look something like this:\n\n\n\nVS Code\n\n\n\nWe have 3 main areas that we can utilize:\n\nTo the left (area A): is the current folder and subfolder directory list. We can make new files directly from here.\nTo the right (area B): this is where files we are working on will live. For some file types, preview windows will be available as well.\nTo the bottom (area C): this is where we can open and run commands from the command line (or terminal).\n\nNow remember, we set up a special environment that contains python and our data science packages. We want to make sure we are always using this environment, so in the open terminal, re-type conda activate ds-env and this terminal will now be open in this environment. We also want to check that VSCode itself (and not just the terminal) will use the same environment. We again access the command palette with Ctrl + Shift + P, and begin typing Python: Select Interpreter. Click on this, and choose the ds-env option. We are good to go!\n\n\n\n\n\n\nConda Environments\n\n\n\nAlthough it does add an extra level of set-up whenever we start a python project, having these conda environments ends up being incredibly important for not only reproducibility, but making sure that packages work well together. When in doubt as to if you are using the correct environment, double check that the terminal you are using has (ds-env) in brackets at the start of a line.\n\n\n\n\nFrom the command line/terminal\nLet’s start with an easy one. To start a python session from a terminal, simply type python at the command line, and the terminal will automatically open a python interface. You will know you are inside the python interface if your command lines now start with >>>. Now, let’s do the classic Hello World command for python:\n\nprint('Hello World!')\n\nHello World!\n\n\nTo exit the python interface and return to the regular terminal, you can type exit() and return to the terminal.\n\n\nFrom a file (in VSCode)\nNext up, let’s run an entire python file to tell us hello. Inside our directory, create a new file called hello_world.py. Note that .py extension - this signifies that the content inside will be python code. Inside this file, let’s have two lines of code:\n\n\nhello_world.py\n\nprint(\"Hello World!\")\nprint(\"Otters are the best animal.\")\n\nTo run this code, first save the file, and then simply click the play button (triangle in the top right!). Note that this will display an output in a terminal at the bottom of VSCode. VSCode takes the python file you told it to run, and will run every line of code individually. Thus, we get two lines of output for the two print statements.\nBut wait, there’s more! In VSCode we can run individual lines of code within a file as well. Simply move your cursor to the line you wish to run, and hit Shift+Enter.\nNote the difference here. Instead of running the entire file, VSCode actually opened up a python window inside our terminal, and ran the single line of code, just like we did before.\n\n\nChallenge 1\n\n\n\n\n\n\nChallenge 1\n\n\n\nRun the other line of code, and then add and run a third line of code that prints your favourite TV show.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nPress Shift+Enter while the cursor is on the other line of code.\nAdd a line of code such as:\nprint(\"C'mon son! You know the best show is Psych!\")\nSave the file, and again press Shift+Enter while on this line.\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\nChallenge 2\n\n\n\nTry clicking the play button again. What happens here? Can you explain why?\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\nAn error will occur in the terminal.\nThis is because VSCode tried to run the entire program again, but from inside an already open python program (which we opened when we ran a single line using the Shift+Enter method).\nTo fix this, we can either exit() the current python execution in the current terminal, or Kill/Delete the open terminal. This button can be found by hovering over the Python symbol at the right hand side of the terminals.\n\n\n\n\n\nFrom JupyterLab\nJupyterLab is an application with a web-based user interface that enables one to work with documents and activities such as Jupyter notebooks, text editors, terminals, and even custom components in a flexible, integrated, and extensible manner.\nTo start JupyterLab, you can use Anaconda Navigator, a GUI that comes packaged with Anaconda if you wish. However it is nearly always easier to access it from the command line. Inside VSCode, navigate to a new terminal.\n\n\n\n\n\n\nConda Reminder!\n\n\n\nMake sure that you double check that this new terminal is opened with the ds-env!\n\n\nFrom the terminal, to launch a new JupyterLab session, simply type:\n\n\nterminal\n\n> jupyter lab\n\nThis should open up a screen that looks something like this:\n\n\n\nJupyterLab\n\n\n\n\n\n\n\n\n\nFun Fact\n\n\n\nYou might have noticed by this point that the author of this section prefers dark mode. So if any of your programs are popping up in a different/lighter colour scheme, that’s okay!\n\n\nWe will be using the JupyterLab interface quite a bit, so let’s get used to the key pieces. Similar to VSCode, we have a few key areas to utilize:\n\nOn the left (area A), we have the sidebar that contains commonly used tabs. These include the file directory system (which defaults to the folder from which the session was launched), a list of open files, running kernels and terminals, a table of contents for any markdown that is written, and possibly a set of extensions.\nOn the right (area B), we have the main work area, where we can open new notebooks, terminals, files, etc. Here, when we have multiple open tabs, we can drag them around the main area to produce smaller panels displaying multiple pieces of work.\n\nGenerally speaking, we will be using Jupyter Notebooks, which have a .ipynb file extension. Like python files, these notebooks can run python code. However, they also support markdown (text that can be added to support source code with explanations) as well as inline viewing of data tables, plots and more. This allows us to mix source code, text and images all in one file that we can quickly use for anything such as:\n\nAnswering ‘how did I get this number in that report?’\nLooking at the number.\nUpdating the number with an updated dataset.\nPlotting the number with other numbers, and then looking at all of those numbers.\n\nLet’s open up a notebook (click the Python 3 icon underneath the Notebook heading). This will open up a new tab called Untitled.ipynb.\n\n\nChallenge 3\n\n\n\n\n\n\nChallenge 3\n\n\n\nRename the notebook to hello_world.ipynb\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nOn either the tab to the top, or the filename that popped up on the left panel, right click the file Untitled.ipynb and click Rename notebook.... Enter your new name here.\n\n\n\nIn this new file, we have a single blue box (this is a single cell). We can type multiple lines of code inside a single cell. Within the blue box, pressing Enter will let you add a new line to the cell. To execute a cell, we press Shift+Enter. This will execute every line of code within that cell. If there is output to display, it will display in the space directly below the cell. Pressing the + at the top of the tab will add a new cell (as will running the bottom-most cell). The ‘active’ cell will always be the one that is highlighted with a blue box.\n\n\n\n\n\n\nChallenge 4\n\n\n\nPrint ‘Hello World’ inside the Jupyter Notebook.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nInside the blue box, write the python code:\nprint('Hello World!')\nWhile still on this cell (highlighted in blue), press Shift+Enter. We should see the output directly below!\n\n\n\nWhen we are done with a JupyterLab session, we must shutdown the server. From the Menu Bar select the File menu and then choose Shut Down at the bottom of the dropdown menu. You will be prompted to confirm that you wish to shutdown the JupyterLab server (don’t forget to save your work!). Click Shut Down to shutdown the JupyterLab server.\n\n\n\n\n\n\nJupyterLab and Us!\n\n\n\nWe will be using JupyterLab for the vast majority of our work throughout this course because of the ease of use in writing and executing code all in one place. Make sure you are comfortable with:\n\nopening JupyterLab to a specific folder\ncreating and renaming notebooks\nadding python code to multiple cells within a notebook\nexecuting entire code blocks\n\nIf you are unsure about any of these pieces, please ask for help!"
  },
  {
    "objectID": "13_working_with_code.html#organizing-coding-work",
    "href": "13_working_with_code.html#organizing-coding-work",
    "title": "2  Working with Code",
    "section": "2.1 Organizing coding work",
    "text": "2.1 Organizing coding work\nMost data projects consist of some version of a) finding some data, b) cleaning it up, and c) generating some insights. In an Excel world, all three stages often happen in a single document. Often over many tabs. Almost always with little documentation. If the project is purely “ask a one time question and get a one time answer and literally never need to look at the file again”, then this approach is arguably efficient and effective. But what if we want to update the insights over time or apply the question to a different dataset? What if someone wants to reproduce what we did? How about verify if what we did was valid or fix something that has gone wrong? These are some of the reasons why good file management practice is key to data project success.\nHaving a strategy ahead of time about how you organize your work when you are starting a project will pay handsome dividends, both for you and for anyone who will be accessing your work down the road. And when you use code (e.g. R or Python) to do a lot of the processing of the data in your project, it is unavoidable that you will have to make some decisions about how to manage your work, which includes managing your files.\n\n\n\nDon’t be this guy!\n\n\n Treat data as read only\nOne of the first things that you will do in a coding environment is to identify your data source and read or ingest that data into your environment to process it further. You should however, leave that originating data unaltered! Once the data is in your coding environment, that is where the processing will take place, and that process will end with some “improved” version of the data being produced.\nClean and analyse your data with code\nIn many cases your data will be “dirty”. That is to say, filled with missing data, spelling mistakes, duplicate records, data type errors, you name it! Your code will clean it. Once you have your clean data, your code will generate insights from it. All of this code (in our case, Python code) will take the form of scripts in separate files. All of these scripts should be stored in a folder separate from the original data itself.\nTreat generated output as disposable\nAs you should be able to take the original data as well as the scripts to process it into something useful, you can recreate the outputs at any time. So while you might have a place where you store the processed data, you should treat it as disposable. Disposable essentially means reproducible. It is a bit like having a kitchen pantry with ingredients for making a cake along with the recipe to guide us through the steps to make it. We should be able to make the same cake over and over again with a stocked pantry and access to the recipe. But if we lose either the stocked pantry or the recipe, we won’t be able to make any more cake. As the chef treats the cake as edible, the data scientist treats the output (plots, data summaries, model results) as disposable.\nOrganize your files\nWhile there is no perfect template for how to organize your work, below is an example that shows how you might consider organizing your work for a relatively straightforward data project. Let’s say we have a project that is about analyzing marmots in BC. MarmotsBC could be the top level directory, it is the project name, without spaces. Under this project the remaining files could be organized within the following structure:\n/MarmotsBC          top level directory, \n│                   contains project name, without spaces\n│\n└─── /docs          supporting text and word processed documents \n│                   that are meant for humans to read\n│\n└─── /data          the original (meta)data that acts as an input \n│                   and will be treated as read-only\n│\n└─── /src           your scripts, may further be divided \n│                   into /functions, /test, etc.\n│\n└─── /bin           programs brought in from elsewhere\n│\n└─── /output        data written by your code to this location, \n│                   could be input for another process\n│\n└─── /analysis      the charts, stats, figures and the like \n                   generated from your scripts\nName and split files when needed\nWhen you are naming files themselves, try to have the names reflect their content or function. You will also find that as your project grows, documents such as scripts may need to be split apart and renamed. For example, you might create some functions as you analyse your data in a single script. At some point you will likely want to create a separate script file just for the functions. When splitting, make sure you eliminate redundent content between files.\nVersion control\nWhen working alone and especially when working within a team, it is helpful to leverage tools that help ensure that everyone’s work is kept up to date and organized. There are tools such as git/GitHub that are made precisely for this purpose. If you hear people talking about repos, merges, commits, pull requests, etc, they are probably talking about the git world. It is beyond the scope of this tutorial to dive into this topic, but it is something to keep in mind. Here is a good intro to all things “git”..\n\n\n\nWhere ideas work (and are kept organized and up-to-date)\n\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nWhat are three activities associated with a project that are made more difficult and put at risk if project files are not kept well organized? Who is likely to be affected? Take three minutes or so working in groups of 3-4 people seated near you to come up with your responses.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nActivities that will be affected include:\n\ninterpreting outputs\nfixing errors that arise\nextending the project.\n\nExisting or future team members will be most directly affected. Ultimately downstream users of the data may also be impacted.\n\n\n\n\n\n\n\n\n\nChallenge 2\n\n\n\nIf output is generally considered the most usable thing for end-users of data, why should it be considered as disposable?\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\nOutput can always be reproduced if the data itself and details about how the data is transformed (cleaned, munged, analyzed, etc.) is available. The loss of things that can be generated from data and code is not a serious risk to the project."
  },
  {
    "objectID": "13_working_with_code.html#getting-coding-help",
    "href": "13_working_with_code.html#getting-coding-help",
    "title": "2  Working with Code",
    "section": "2.2 Getting coding help",
    "text": "2.2 Getting coding help\nBeing effective with using python, or any other coding language, requires that you hone your self-serve skills. Coding requires that you are comfortable with asking and getting questions answered using a number of techniques. Here are a few to know and use.\nTopical help\nIn whatever developing environment (sometimes called an IDE - Integrated Development Environment) you choose to work with, there are usually very helpful links readily available. In the Jupyter Lab environment, for example, you will find such help under the Help choice in the top menu.\n\n\n\nFinding Jupyter Lab’s Help\n\n\nWhen you click on Help you see an expanded range of topics:\n\n\n\nTopical Help at your fingertips\n\n\nUnder the topics listed you will find treasure troves of helpful documentation relating to Python. Whether you are relatively new to the language or have years of experience already with it, one of the most helpful sections you will find there is Python Reference:\n\n\n\nOfficial Python Documentation\n\n\nEach section provides useful content that is related to the version of Python you are working with! One of the particularly helpful sections for those new to Python is the Tutorial section, where you will find tutorials on several of the topics in this course, plus many more.\n Functions help\nFunctions in Python have excellent documentation that you should be able to access fairly readily. Below are screen shots from the Jupyter Notebook environment that show two distinct ways to access. The example below shows how to get detail about a specific function by calling help() function itself and passing in the name of the function of interest:\n\nhelp(round) # Run code as normal to get help information\n\nHelp on built-in function round in module builtins:\n\nround(number, ndigits=None)\n    Round a number to a given precision in decimal digits.\n    \n    The return value is an integer if ndigits is omitted or None.  Otherwise\n    the return value has the same type as the number.  ndigits may be negative.\n\n\n\n\nWithin Jupyter environments, it is also possible to place the cursor inside the cell with the function and hit Shift-tab to get information:\n\nround() # Shift-tab to get drop-down help in Jupyter \n\nTypeError: round() missing required argument 'number' (pos 1)\n\n\n \n\n\n Syntax error help\nWhen developing code, you will go through a process over and over where you write some code and then test it. If it works, you keep going, adding complexity and content to your code. But what happens if it does not work? Fortunately Python delivers some clues about what may have gone wrong.\nHere are a couple of examples of syntax errors, where python is essentially saying “I don’t understand what you mean”!\n\n# Forgot to close the quotation marks after the quote\nname = 'Feng\n\nSyntaxError: EOL while scanning string literal (3038418245.py, line 2)\n\n\n \n\n\nThese kinds of errors are usually because of careless typing such as leaving off a quote or comma or bracket somewhere (or adding one too many).\n\n# An extra '=' in the assignment\nage = = 52\n\nSyntaxError: invalid syntax (3914520000.py, line 2)\n\n\n \n\n\nSyntax errors are generally mildly annoying, however, they are usually not too complex, and relatively easy to find and fix.\n Runtime error help\nPython reports also runtime errors, also sometimes called exceptions, which are thrown when something goes wrong while a program is executing. This is more like python saying “I understand what you are saying, but I am running into some trouble in doing what you are asking”.\nSome common types of runtime errors when your code:\n\ntries to divide something by zero\nperforms an operation on incompatible data types\nuses an identifier which has not been defined\naccesses a list element, dictionary value or object attribute which doesn’t exist\ntries to access a file which doesn’t exist\n\nIn the example below, the code is telling python to import a package that is not yet installed.\n\n# Python cannot find this library\nimport shap\n\nModuleNotFoundError: No module named 'shap'\n\n\n \n\n  In the example below, the issue is not a syntax error (although it might look like a typo that one might associate with being a syntax error).\n\nage = 54\nyears_to_100 = 100 - aege  # \"misspelled\" assigned variable called 'age'\n\nNameError: name 'aege' is not defined\n\n\n \n\n  The above examples introduce the concept of traceback, which is a way of helping the coder locate the origin of the error experienced. A traceback will show up in output that is to be read “from the bottom up”. In the above example, we see that the program does not recognize the object “aege” when it encounters it in cell line 2. Note that this does not necessarily mean that the problem should be fixed at the location indicated. In the above example, a reexamination of the code might reveal that the original name assigned was incorrectly spelled. Were that the case, then the original name assignment should be updated, not the reference to it in cell line 2.\n Diving deeper\nThe first and best way to work though why your code failed is to look at the information provided at the bottom of the output. As in the examples above, much of the time you will have enough information there to give you some ideas about what the cause of the problem is and a starting point for modifying the code and running it again.\nMany times, however, you will find that your code throws errors you still don’t understand. Other times, you would like to get a better understanding of some topic you have come across. For those moments, don’t be shy to use Google to get going by typing in the search field those terms that are narrow enough to describe your problem as succinctly as possible.\n\n\n\nGlobal community and solutions to challenges or fixes for bugs\n\n\nAmong the more popular entries you’ll find on the search engine response page come from stackoverflow or other coding community sites, and from blogs or other sources. It may also pay to look at “videos” among your Google search responses and you will likely see many YouTube offerings of varying degrees of relevance. There is a whole ecosystem of python developers (polished and not-so-much) who offer content that may be of help. These are often very helpful.\nImagine we were working through the “SyntaxError: EOL while scanning string literal” error above. You know that it’s a syntax error, so you examine your code for something that does not look right based on what you know about writing code. What happens if you still are getting an error? Then a Google search might be helpful.\n\n\n\nError deep dive\n\n\nThe highlighted summary information shows us that it understands that we are referring to a python issue, not a general coding one. So that is a good sign. It summarizes from a page on the realpython.com site. Moreover it suggests different ways that others have extended the search that you might find helpful. While the suggested site itself is not an official python site, it nonetheless is a quality site about python that will likely be worthwhile to review. Once there, you will find more useful information.\n\n\n\n\n\n\n\nChallenge 3\n\n\n\nYou want to find out what the print() function does. How would you look to answer this question? What does it do, does it send a print job to a printer?\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nUse one of the two most efficient ways to obtain information\n\nhelp(print) # One way to get the info\n\nHelp on built-in function print in module builtins:\n\nprint(...)\n    print(value, ..., sep=' ', end='\\n', file=sys.stdout, flush=False)\n    \n    Prints the values to a stream, or to sys.stdout by default.\n    Optional keyword arguments:\n    file:  a file-like object (stream); defaults to the current sys.stdout.\n    sep:   string inserted between values, default a space.\n    end:   string appended after the last value, default a newline.\n    flush: whether to forcibly flush the stream.\n\n\n\n\nprint() # Shift-tab another way to get the info\n\n\n\n\nDoes not send to a physical printer! Prints the values passed into it to a stream or to an output (usually to output shown on a monitor).\n\n\n\n\n\n\n\n\n\nChallenge 4\n\n\n\nSomeone asks you want the term “iterable” means in the context of a python function? How would you go about finding an answer? What does it mean?\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nYou could use help off of the menu. Might be quicker to do a Google search.\nIterable is an object which can be looped over or iterated over with the help of a for loop. Objects like lists, tuples, sets, dictionaries, strings, etc. are called iterables. In short and simpler terms, iterable is anything that you can loop over.\nYou may or may not know what “loops” are, but perhaps have heard of lists and such. Particularly at the beginning of your Python journey, there will be terms that you will not yet be familiar with that will show up in explanations of other things. Don’t worry! As with anything, this will occur less and less often over time."
  },
  {
    "objectID": "13_working_with_code.html#coding-in-style",
    "href": "13_working_with_code.html#coding-in-style",
    "title": "2  Working with Code",
    "section": "2.3 Coding in style",
    "text": "2.3 Coding in style\nWorking with python can be challenging. When you are first learning it, there may seem to be so many different rules around using certain kinds of brackets (and not others) at certain times, rules around indenting lines, etc. Many of these rules result in your code refusing to run. Often your code may run, however, it may not necessarily be easy to follow for someone reading through it. In an enterprise environment, your code will likely be read and referenced by others, so making it easy to decipher is important! \n\n\n\nStyle matters!\n\n\nFortunately, there are some widely accepted guidelines about coding style that you should be aware of and strive towards over time. In the Python world, the style guide you should know about is “PEP8” which stands for Python Enhancements Proposals.\nThere are many specific rules in PEP8, here are some to be aware of from the start.\nComments\nWhen coding, it is good practice to comment on code, but only when the code is likely to be non-obvious to others who are likely to review it. Comments are preceded by a “#”, which within Python, ensures that they are recognized as non-executable and therefore no threat to actually affecting your code. Below are two examples of how you can add comments to your code (if necessary).\n\n# The bog standard first line of code learned in any language\nprint(\"Hello World\")\n\nprint(\"Hello World\")  # New programmer's first code in any language \n\nHello World\nHello World\n\n\nSpacing within code\nThere are a number of little conventions about spacing that just have to be learned. In the code below look at the comments that go along to see what is considered good style and what is considered good and what is not.\n\n# Good, is a space on either side of the assignment operator \na = print(\"hello\")  \n\n# Poor, as the assignment operator lacks spaces around it\na=print(\"hello\")    \n\nhello\nhello\n\n\n\n# Good, is no space between call and left parenthesis\nprint(2)  \n\n# Poor, should be no space between call and left parenthesis\nprint (2) \n\n2\n2\n\n\nBelow are some examples of how to incorporate spaces when you have operators with different priorities (i.e. remember PEMDAS or BODMAS from high school?). In general, add space for those instances with lower priority:\n# Good style\ni = i + 1 \nx = x*2 - 1\nc = (a+b) * (a-b)\n\n# Poor style\ni=i+1 \nx = x * 2 - 1\nc = (a + b) * (a - b)\nThere are many other conventions about spacing, but the ones above are some of the more common ones to watch out for.\nIndentation\nA line of code that completes a meaningful coherent action of some sort, is kind of like a complete sentence in grammar. As with sentences, a line of code can be short and sweet, long and convoluted, and anywhere in between. Where lines of code are longer than 79 characters, certain style guide rules come into play. The examples below show how to carry over content from one line to the next line.\n# Good - aligned with opening delimeter\nanimals = animals.drop('weight', 'size', 'age', 'location', 'eye colour', \n                       'intelligence', 'cost', 'number held in zoos')\n\n# Good - hanging indents (usually four spaces, should not use tabs)\nanimals = animals.drop(\n    'weight', 'size', 'age', 'location', 'eye colour', 'intelligence', \n    'cost', 'number held in zoos')\nIt should be noted that many programmers use tabs instead of four spaces! While PEP8 is not a fan of tabbing, as this practice is widespread and effective, it is common to come across advocates. Mixing tabbing and individual spaces, however, may result in syntax errors being thrown. Also for multi-line constructs (such as those in the examples above), the closing brace/bracket/parenthesis may appear on the line under the last line code for the piece of code.\nBlank lines\nBlank lines should be used sparingly, but they can be used where appropriate. It can help your code “breath”! A general rule of thumb is that there should be a single blank row between blocks of code, and two blank rows between functions.\n\n# Good - different kinds of coding elements together and separated by blank lines\nx = [1,2,3]\ny = [2,4,1]\n\nplt.plot(x, y)\n\nplt.xlabel('x - axis')\nplt.ylabel('y - axis')\n\nplt.title('My first graph!')\nplt.show()\n\n# Less good - harder for the eye to differentiate how the code is structured\nx = [1,2,3]\ny = [2,4,1]\nplt.plot(x, y)\nplt.xlabel('x - axis')\nplt.ylabel('y - axis')\nplt.title('My first graph!')\nplt.show()\n\nNameError: name 'plt' is not defined\n\n\nOne can ignore the error thrown above as we have not imported the needed package to get the code to run. What is to notice is how the different similar blocks of code are separated with blank lines to make it more readable. \n\n\n\n\n\n\nChallenge 5\n\n\n\nBased on what we have covered, what, if any are the style code issues.\n\n# Example 1\nprint (last_name) \n\n# Example 2\ncelcius = (fahrenheit - 32) / 1.8  \n\n# Example 3\nindex = ['snail', 'pig', 'elephant', 'rabbit', 'giraffe', 'coyote', 'horse', 'giraffe', 'ping pong table']\n\n\n\n\n\n\n\n\n\nSolution to Challenge 5\n\n\n\n\n\nExample 1: there should be no space between the function call and the “(”\nExample 2: there should be no space between “fahrenheit”, the “-” operator, and “32”\nExample 3: line exceeds 79 characters so should be spaced over two lines\n\n\n\n Consistency is sometimes more important than being right\nAll of the rules above are about coding the “right” or “correct” way. There are times when you can consider breaking, or at least bending some of the rules for the greater good of being “consistent”. This is especially the case when you are working on code as part of a project. Sometimes it is better to be consistent than right. This is often true if you are working on code as part of another project with some unique style practices. Keeping internally consistent with these practices will lead to overall greater readability, which is the greater goal of coding style."
  },
  {
    "objectID": "02_core_data_structure_concepts.html#variables-assignment",
    "href": "02_core_data_structure_concepts.html#variables-assignment",
    "title": "3  Core Concepts",
    "section": "3.1 Variables & Assignment",
    "text": "3.1 Variables & Assignment\nWe can assign values to variables in Python that we can use over and over. Variables are always assigned using the format:\n\nvariable_name = 'variable_value'\nfirst_name = 'Loki'\nage = 1054\n\nWhere the name of the variable is always to the left, and whatever value we wish to assign being on the right of =.\nSome rules regarding naming variables:\n\nNames may only contain letters, digits, and underscores\nAre case sensitive\nMust not start with a digit\n\nTypically, variables starting with _ or __ have special meaning, so we will try to stick to starting variables with letters only\n\n\nTo display the value we have previously assigned to a variable, we can use the print function:\n\nprint(first_name, 'is', age, 'Earth years old.')\n\nLoki is 1054 Earth years old.\n\n\n\nChallenge 1\n\n\n\n\n\n\nChallenge 1\n\n\n\nIn the next cell, run the following command:\nprint(last_name)\nWhat happens? Why? How can we fix it?\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nVariables cannot be referenced before they are assigned, so we will run into an error:\n\nprint(last_name)\n\nNameError: name 'last_name' is not defined\n\n\nTo fix this, we simply need to create another cell that assigns this variable, then we can go back and run the print command.\n\nlast_name = 'Odinson'\n\n\nprint(last_name)\n\nOdinson\n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\nChallenge 2\n\n\n\nFill the table below showing the values of the variables in this program after each statement is executed.\n\n\n\nCommand\nValue of x\nValue of y\nValue of swap\n\n\n\n\nx = 1.0\n\n\n\n\n\ny = 3.0\n\n\n\n\n\nswap = x\n\n\n\n\n\nx = y\n\n\n\n\n\ny = swap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\n\n\n\nCommand\nValue of x\nValue of y\nValue of swap\n\n\n\n\nx = 1.0\n1.0\nnot defined\nnot defined\n\n\ny = 3.0\n1.0\n3.0\nnot defined\n\n\nswap = x\n1.0\n3.0\n1.0\n\n\nx = y\n3.0\n3.0\n1.0\n\n\ny = swap\n3.0\n1.0\n1.0\n\n\n\n\n\n\n\n\n\n\n\n\nJupyter Notebooks: Order of Execution\n\n\n\nIf you noticed in the last challenge, you could go back to a previous cell above where you assigned a variable, and the print command would work. This is because, in a Jupyter notebook, it is the order of execution of cells that is important, not the order in which they appear. Python will remember all the code that was run previously, including any variables you have defined, irrespective of cell order.\nAfter a long day of work and to prevent confusion, it can be helpful to use the Kernel → Restart & Run All option which clears the interpreter and runs everything from a clean slate going top to bottom."
  },
  {
    "objectID": "02_core_data_structure_concepts.html#lists-indexing",
    "href": "02_core_data_structure_concepts.html#lists-indexing",
    "title": "3  Core Concepts",
    "section": "3.2 Lists & Indexing",
    "text": "3.2 Lists & Indexing\n\n\n\n#sorrynotsorry R\n\n\n\nLists\nAn important aspect of Pythonic programming is the use of indicies to allow us to slice and dice our datasets. We will learn a bit more about indexing here through the introduction of lists. A list is an ordered list of items in Python, where the items can take on any datatype (even another list!). We create a list by putting values inside square brackets and separate items with commas:\n\nmy_list = [1, 'two', 3.0, True]\nprint(my_list)\n\n[1, 'two', 3.0, True]\n\n\n\n\nIndexing\nTo access the elements of a list we use indices, the numbered positions of elements in the list. These positions are numbered starting at 0, so the first element has an index of 0. Python has made it easy to count backwards as well: the last index can be accessed using index -1, the second last with -2 and so on.\n\n\n\n\n\n\n0-Based Indexing!\n\n\n\nIf you have used other coding languages, such as R, you may notice that different programming languages start counting from different numbers. In R, you start your indexing from 1, but in Python it is 0. It’s important to keep this in mind!\n\n\n\nprint('First element:', my_list[0])\nprint('Last element:', my_list[-1])\nprint('Second last element:', my_list[2])\nprint('Also second last element:', my_list[-2])\n\nFirst element: 1\nLast element: True\nSecond last element: 3.0\nAlso second last element: 3.0\n\n\nStrings also have indices, pointing to the character in each string. These work in the same way as lists.\n\nprint(first_name)\nprint(first_name[0])\n\nLoki\nL\n\n\nHowever, there is one important difference between lists and strings: we can change values in a list, but we cannot change individual characters in a string. For example:\n\nprint(my_list)\nmy_list[0] = 'changing the first element!'\nprint(my_list)\n\n[1, 'two', 3.0, True]\n['changing the first element!', 'two', 3.0, True]\n\n\nwill work. However:\n\nprint(first_name)\nfirst_name[0] = 'N'\n\nLoki\n\n\nTypeError: 'str' object does not support item assignment\n\n\nWill throw an error.\n\n\n\n\n\n\nMutable vs Immutable\n\n\n\nData which can be modified in place is called mutable, while data which cannot be modified is called immutable. Strings and numbers are immutable. This does not mean that variables with string or number values are constants, but when we want to change the value of a string or number variable, we can only replace the old value with a completely new value.\nLists and arrays, on the other hand, are mutable: we can modify them after they have been created. We can change individual elements, append new elements, or reorder the whole list. For some operations, like sorting, we can choose whether to use a function that modifies the data in-place or a function that returns a modified copy and leaves the original unchanged.\nBe careful when modifying data in-place. If two variables refer to the same list, and you modify the list value, it will change for both variables!\n\n\nWe can use indicies for more than just accessing single elements from an ordered object such as a list or a string. We can also slice our dataset to give us different portions of the list. We do this using the slice notation [start:stop], where start is the integer index of the first element we want and stop is the integer index of the element just after the last element we want. If either of start or stop is left out, it is assumed that you want to default with either starting from the beginning of the list or ending at the end.\n\nnumber_list = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nprint('0:5  --> ', number_list[0:5])\nprint('_:5  --> ', number_list[:5])\nprint('3:7  --> ', number_list[3:5])\nprint('3:_  --> ', number_list[3:])\nprint('3:-1 --> ', number_list[3:-1])\n\n0:5  -->  [0, 1, 2, 3, 4]\n_:5  -->  [0, 1, 2, 3, 4]\n3:7  -->  [3, 4]\n3:_  -->  [3, 4, 5, 6, 7, 8, 9]\n3:-1 -->  [3, 4, 5, 6, 7, 8]\n\n\nWe can also use a step-size to indicate how often we want to pick up an element of the list. By altering the slice notation to [start:stop:step] we will be telling the code to only include those elements at each step after start, ending at the final step that occurs just before running into stop. This allows us to reverse lists as well:\n\nprint('All evens:', number_list[0::2])\nprint('All odds: ', number_list[1::2])\nprint('Just 1 and 4:', number_list[1:5:3])\nprint('Reversed: ', number_list[-1::-1])\n\nAll evens: [0, 2, 4, 6, 8]\nAll odds:  [1, 3, 5, 7, 9]\nJust 1 and 4: [1, 4]\nReversed:  [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\n\n\n\nChallenge 3\n\n\n\n\n\n\nChallenge 3\n\n\n\nGiven the following string:\n\nfull_name = 'Peregrin Fool of a Took Pippin'\n\nWhat would these expressions return?\n\nfull_name[2:8]\nfull_name[11:] (without a value after the colon)\nfull_name[:4] (without a value before the colon)\nfull_name[:] (just a colon)\nfull_name[11:-3]\nfull_name[-5:-3]\nWhat happens when you choose a stop value which is out of range? (i.e., try full_name[0:42] or full_name[:103])\n\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\n\n'regrin '\n'ol of a Took Pippin'\n'Pere'\n'Peregrin Fool of a Took Pippin'\n'ol of a Took Pip'\n'ip'\nIf a part of the slice is out of range, the operation does not fail. full_name[:] gives the same result as full_name[0:42], and the same result as full_name[:103].\n\n\n\n\n\n\nDictionaries\nAnother object that exists in Python is the dictionary. It is similar to a list in that it can hold a variety of different types of objects inside of it. However an important difference is in how we access these objects. With a list (or string), we have an ordered arrangement of items that we access with an integer index. However, we access the values in a dictionary with a key, which can be anything we want.\nLet’s build a dictionary, which is denoted in Python with curly {} brackets:\n\nmy_dict = {\n    'first_key': 'some value',\n    'A': ['a', 'differerent', 'type', 'of', 'object'],\n    2: False\n}\n\nprint(my_dict)\n\n{'first_key': 'some value', 'A': ['a', 'differerent', 'type', 'of', 'object'], 2: False}\n\n\nHere we listed three key - value pairs. The key comes before the value, with a colon between. Commas separate different pairs. Now that we have a dictionary, we access it the same way as a list, with square [] brackets:\n\nprint(my_dict['first_key'])\nprint()\nprint(my_dict['A'])\nprint()\nprint(my_dict[2])\n\nsome value\n\n['a', 'differerent', 'type', 'of', 'object']\n\nFalse\n\n\nUnlike a list, dictionaries are unordered, and so we cannot perform integer indexing or slicing of these elements:\n\nmy_dict[0]\n\nKeyError: 0\n\n\n\nmy_dict[0:5]\n\nTypeError: unhashable type: 'slice'\n\n\nDictionaries are an abstract data type that can take a while to get used to! They can be a powerful tool in Python. Common uses for dictionaries include:\n\nCreating searchable parameter lists for models\nSupplying extra arguments to functions\nStoring complex outputs or datasets\n\nWe will not need to use dictionaries frequently in this course. However, they will become useful when we learn more about data tables and aggregation methods later on, and so gaining familiarity now is beneficial!"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#data-types-operations",
    "href": "02_core_data_structure_concepts.html#data-types-operations",
    "title": "3  Core Concepts",
    "section": "3.3 Data Types & Operations",
    "text": "3.3 Data Types & Operations\nEvery value in a program has a specific type. In this course, you will run across four basic types in Python:\n\nInteger (int): positive or negative whole numbers like 42 or 90210\nFloating point numbers (float): real fractional numbers like 3.14159 or -87.6\nCharacter strings (str): text written either in single or double quotes.\nBoolean (bool): the logical values of True and False\n\nIf you are unsure what type anything is, we can use the built in function type. Note that this works on variables as well.\n\nprint(type(42))\nprint(type(3.14))\nprint(type('Otter'))\nprint(type(True))\n\n<class 'int'>\n<class 'float'>\n<class 'str'>\n<class 'bool'>\n\n\n\n\n\n\n\n\nMessy Numbers\n\n\n\nWhen you start to have really long integers, it starts to look really messy (how many thousands are in 1982137092 at a glance?) Luckily, Python allows us to use _ inside our integers to space out our digits. Thus we could write that instead as 1_902_137_092. Isn’t that nicer!\n\n\n\nBasic Arithmetic\nThe type of a variable controls what operations can be performed on it. For example, we can subtract floats and ints, but we cannot subtract strings:\n\nprint(42-12)\nprint(3.14-15)\nprint('hello' - 'h')\n\n30\n-11.86\n\n\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\n\n\nHowever, we can add strings together:\n\nmy_sentence = 'Adding' + ' ' + 'strings' + ' ' + 'concatenates them.'\nprint(my_sentence)\n\nAdding strings concatenates them.\n\n\nAs well as multipling a string by an integer to get a repeated string:\n\nrepeated_string = '=+'*10\nprint(repeated_string)\n\n=+=+=+=+=+=+=+=+=+=+\n\n\nAs we saw above, we can mix and match both of the numerical types, however we will get an error if we try to mix a string with a number:\n\nprint(1 + '2')\n\nTypeError: unsupported operand type(s) for +: 'int' and 'str'\n\n\nIn order to have a sensical operation, we need to convert variables to a common type before doing our operation. We can convert variables using the type name as a function.\n\nprint(1 + int('2'))\nprint(str(1) + '2')\n\n3\n12\n\n\n\n\n\n\n\n\nInt of a Float?\n\n\n\nNote that when converting floats to integers, it will always round down! That is, int(3.6) = int(3.1) = 3!\n\n\nOne last bit of math you might come across are the various types of division:\n\n/ performs regular floating-point division\n// performs integer floor division\n% returns the remainder of integer division\n\n\nprint('5 / 3 :', 5 / 3)\nprint('5 // 3:', 5 // 3)\nprint('5 % 3 :', 5 % 3)\n\n5 / 3 : 1.6666666666666667\n5 // 3: 1\n5 % 3 : 2\n\n\n\n\nBuilt in Functions\nPython has multiple pre-built functions that come in handy. We have already made use of the print() command frequently, and learned how to use type() to tell us what type of data our variables are. Here are some other frequently used functions:\n\nlen() : Tells us the length of a list, string, or other ordered object. Does not work on numbers!\nhelp() : Gives help for other functions\nmin() : Gives the mininum value in a list of options.\nmax() : Gives the maximum value in a list of options.\nround(): Rounds a value to a given decimal length.\n\nNote that, similar to the arithmetic operations above, these built in functions must operate on logically consistent datatypes. We can find the min of 2 strings, or 4 numbers, but we cannot compare a string to a float.\nEvery function in python will take 0 or more arguments that are passed to a function. For example, len() takes exactly one argument, and returns the length of that argument:\n\n print(len('this string is how long?'))\n\n24\n\n\nSome functions, such as min() and max() take a variable number of arguments:\n\n print(min(1,2,3,4))\n print(max('a', 'b', 'c'))\n\n1\nc\n\n\nWhile others have default values that do not need to be provided at all.\n\n\nChallenge 4\n\n\n\n\n\n\nChallenge 4\n\n\n\nUse the help() and round() functions to print out the value of 2.71828182845904523536 to 0 and 2 decimal places.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nThe result of help() tells us that round() has a default option:\n\nhelp(round)\n\nHelp on built-in function round in module builtins:\n\nround(number, ndigits=None)\n    Round a number to a given precision in decimal digits.\n    \n    The return value is an integer if ndigits is omitted or None.  Otherwise\n    the return value has the same type as the number.  ndigits may be negative.\n\n\n\nThus, we can use the round() function with and without the default ndigits to get our answer:\n\neulers_number = 2.71828182845904523536\nprint(round(eulers_number))\nprint(round(eulers_number, 2))\n\n3\n2.72\n\n\n\n\n\n\n\n\n\n\n\nHelp in Jupyter Lab!\n\n\n\nIn Jupyter notebooks, we can also get help by starting a line with ?. For example, ?round will display the help information about the round() function.\n\n\n\n\nA Quick Intro to Boolean Logic\nWe can ask Python to take different actions, depending on a condition, with the if statement:\n\nnum = 37\nif num > 100:\n    print('greater')\nelse:\n    print('not greater')\nprint('done')\n\nnot greater\ndone\n\n\nThe if keyword tells Python we want to make a choice. We then use : to end the conditional we would like to consider, and indentation to specify our if block of code that should execute if the condition is met. If the condition is not met, the body of the else block gets executed instead.\nIn either case, ‘done’ will always print as it is in neither indented block.\n\n\n\nFollowing a Logical Flow\n\n\nConditional statements do not need to include an else block. If there is no block and the condition is False, Python simply does nothing:\n\nnum = 37\nif num > 100:\n    print('greater')\nprint('done')\n\ndone\n\n\nWe can also chain several tests together using elif. Python will go through the code line by line, looking for a condition that is met. If no condition is met, it will execute the else block (or nothing if there is no else).\n\nnum = 45\nif num < 42:\n    print('This is not the answer.')\nelif num > 42:\n    print('This is also not the answer.')\nelse:\n    print('This is the answer to life, the universe, and everything.')\n\nThis is also not the answer.\n\n\nThere are multiple different comparisons we can make in Python:\n\n>: greater than\n<: less than\n==: equal to (note the double ‘=’ here!)\n!=: does not equal\n>=: greater than or equal to\n<=: less than or equal to\n\nAnd these can be used in conjunction with each other using the special keywords and, or, and not. and will evaluate to True if both parts are True, while or will evaluate to True if either side is. not will evaluate the condition, and then return the opposite result.\n\ncondition_1 =  1 > 0  # True\ncondition_2 = -1 > 0  # False\n\nprint('Testing and: ')\nif condition_1 and condition_2:\n    print('both parts are true')\nelse:\n    print('at least one part is false')\n\nprint()\nprint('Testing or: ')\nif condition_1 or condition_2:\n    print('at least one part is true')\nelse:\n    print('both parts are false')\n\nprint()\nprint('Testing not: ')\nif not condition_1:\n    print('condition_1 was false')\nelse:\n    print('condition_1 was true')\n\nTesting and: \nat least one part is false\n\nTesting or: \nat least one part is true\n\nTesting not: \ncondition_1 was true\n\n\n\n\n\n\n\n\nMultiple Conditions Cause Confusion\n\n\n\nJust like with arithmetic, you can and should use parentheses whenever there is possible ambiguity. A good general rule is to always use parentheses when mixing and and or in the same condition.\n\n\n\n\nChallenge 5\n\n\n\n\n\n\nChallenge 5\n\n\n\nWhat will be the output of the following block of code?\nnum = 42 \nanimal = 'otter'\n\nif num==42 and animal=='mouse':\n    print('Correct, that is the animal that found the answer')\nelif num==42 and animal!='mouse':\n    print('Almost, the number is correct but not the animal.')\n    animal = 'dolphin'\nelif num!=42 or animal=='mouse':\n    print('Almost, the animal is correct but not the number.')\n    num = 5\nelif (1>3) or (4>3):\n    print('This has nothing to do with it, we just needed an or statement')\nelse:\n    print('Not even close, those pesky mice need to work harder.')\n    num = 19\n    animal = 'kangaroo'\n\nprint('The end result is the number', number, 'and animal', animal)\n\n\n\n\n\n\n\n\nSolution to Challenge 5\n\n\n\n\n\n\n\nAlmost, the number is correct but not the animal.\nThe end result is the number 42 and animal dolphin\n\n\n\n\n\n\n\n\n\n\n\nElement-Wise Logic\n\n\n\nBefore we move on from our foray into boolean logic, let us make a brief mention of the &, |, and ~ symbols. These are similar, but not identical, to and, or and not. Where and is used for boolean logic on scalars, & is used for boolean logic on vectors, and will do an element-by-element comparison. This will be important when we introduce data structures later on!"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#methods-chaining",
    "href": "02_core_data_structure_concepts.html#methods-chaining",
    "title": "3  Core Concepts",
    "section": "3.4 Methods & Chaining",
    "text": "3.4 Methods & Chaining\n\n\n\nObject Oriented Programming\n\n\nSo far we have seen built in functions that can be applied to a variety of different datatypes (as long as the datatype makes sense for that particular function). However, there are some functions that we apply specifically to a particular class of objects - we call these functions methods. Methods have parentheses, just like functions, but they come after the variable to denote that the method belongs to this particular object.\nWe have met classes already: all of our basic datatypes (strings, integers, floats, booleans) are different classes of objects in Python. An individual instance of a class is considered an object of that class. Understanding how to use methods will become useful when we reach the pandas portion of the course, which is our main tool when looking at, cleaning, and summarizing data.\nLet’s consider the string class. Here are a few common methods associated with it:\n\nlower(): coverts all characters in the string to lowercase\nupper(): converts all characters in the string to uppercase\nindex(): returns the position of the first occurrence of a substring in a string\nrjust(): right aligns the string according to the width specified\nisnumeric(): returns True if all characters are numeric\nreplace(): replaces all occurrences of a substring with another substring\n\n\n\n\n\n\n\nGetting Help For Methods\n\n\n\nYou will notice that trying to find help on a method will not work if you only specify the method. Because these are not built in functions, and only belong to instances of a class, you need to specify the object together with the method to use help.\nFor example, help(lower) will result in an error, whereas help(\"any string\".lower) will give you the help you were looking for.\n\n\nLet’s see some of these in action. You’ll notice that when being used, methods don’t always have an argument supplied to them. That is because the first argument is always the object is being applied to. If a method requires secondary arguments, these are subsequently included in the parentheses.\n\nobject.method(a, b, c, ...) ↔︎ method(object, a, b, c, ...)\n\n\nmy_string = 'Peter Piper Picked a Peck of Pickled Peppers'\nprint(my_string.lower())\nprint(my_string.isnumeric())\n\npeter piper picked a peck of pickled peppers\nFalse\n\n\nWe can also chain methods together. Each subsequent method (reading from left to right) acts on the output of the previous method. Chaining can be done in a single line, or over multiple lines (which helps for readability).\n\nprint(my_string.upper().replace('P', 'M'))\n\n# Chaining over multiple lines can be done in 2 ways:\n# 1. Enclose the entire operation in brackets\nchain_1 = (my_string\n    .upper()\n    .replace('P', 'M')\n)\n\n# 2. Use the character \"\\\" to denote an operation is continuing on the next line\nchain_2 = my_string \\\n    .upper() \\\n    .replace('P', 'M')\n\nprint(chain_1)\nprint(chain_2)\n\nMETER MIMER MICKED A MECK OF MICKLED MEMMERS\nMETER MIMER MICKED A MECK OF MICKLED MEMMERS\nMETER MIMER MICKED A MECK OF MICKLED MEMMERS\n\n\n\nChallenge 6\n\n\n\n\n\n\nChallenge 6\n\n\n\nWhat happens if we try to run the block of code:\nprint(my_string.isnumeric().upper())\n\n\n\n\n\n\n\n\nSolution to Challenge 6\n\n\n\n\n\nWe will get an error:\n\nprint(my_string.isnumeric().upper())\n\nAttributeError: 'bool' object has no attribute 'upper'\n\n\nThis is because the code is read from left to right. In this case, the output of isnumeric() is a boolean object, which does not have the method upper() anymore:\n\nprint(my_string.isnumeric())\nprint(type(my_string.isnumeric()))\n\nFalse\n<class 'bool'>\n\n\n\n\n\n\n\nChallenge 7\n\n\n\n\n\n\nChallenge 7\n\n\n\nA common data string you see across government are various personal IDs. One example is the Personal Education Number (PEN), which is a nine-digit number assigned to you when you enter the K-12 School System. Oftentimes when looking at such an ID, any leading zeros that are an actual part of the PEN get stripped away, leading to unevenly sized strings of IDs across the dataset.\nWrite a piece of code for PEN IDs that does the following:\n\nChecks to make sure that the ID is entirely numeric. If it is not, print out a warning that this is an invalid PEN.\nIf the PEN is numeric, make sure that it is less than or equal to 9 digits long. If it is longer, print out a warning that this is an invalid PEN.\nIf the PEN is too short, make sure to pad it with the appropriate number of 0’s to the left. Print out the correct PEN.\n\nTry your code out with the following PENs:\npen_1 = '12345678x'\npen_2 = '123456789'\npen_3 = '1234567890'\npen_4 = 123456789\npen_5 = '123456'\n\n\n\n\n\n\n\n\nSolution to Challenge 7\n\n\n\n\n\n\npen_1 = '123456'\n\n# first! make sure we are looking at strings so we can use the string method!\npen = str(pen_1)\n\n# first check for numerical:\nif not pen.isnumeric():\n    print('Warning! This PEN has non-numeric characters.')\n\n# second, check that it isn't too long\nelif len(pen)>9:\n    print('Warning! This PEN is longer than 9 digits.')\n\n# third: make sure that we pad it to the correct length\nelse:\n    pen = pen.rjust(9, '0')\n    print('This PEN is valid:', pen)\n\nThis PEN is valid: 000123456"
  },
  {
    "objectID": "02_core_data_structure_concepts.html#accessing-other-packages",
    "href": "02_core_data_structure_concepts.html#accessing-other-packages",
    "title": "3  Core Concepts",
    "section": "3.5 Accessing Other Packages",
    "text": "3.5 Accessing Other Packages\n\n\n\nImport Packages\n\n\nMost of the power of Python lies in its ability to use libraries, or external packages that are not part of the base Python programming language. These libraries have been written and maintained by other members of the Python community, and will make data cleaning, manipulation, visualization and any other data project much simpler. Throughout this course we will use packages such as:\n\npandas: this is the go-to package for all things data-table.\nmatplotlib: this is the most frequently used plotting package in Python\nseaborn: this is a plotting package built with pandas and data in mind\n\nWhen we set up our Python environment, we already installed many of the packages we will need directly into the conda environment we produced. If you ever need another package, it is simple enough to install again using conda:\n\n\nAnaconda Powershell Prompt\n\n> conda activate ds-env\n> conda install <package>\n\n\n\n\n\n\n\nTo Pip or not to Pip?\n\n\n\nIf you are ever searching for a package you think will aid you in your work, you might come across the pip command. This is a different (yet related) method of installing packages. While it is possible to use pip in tandem with conda commands, it is recommended that you stick to only conda wherever possible.\nAs a rule of thumb, try to conda install package as a first try. If this does not work, search the website for the package for installation instructions. Sometimes it will recommend using a different conda channel (and will provide the code to do so). Sometimes, it is only possible to get the package from pip, in which case using pip inside the conda environment is the only way to go. Just use this as a last resort!\n\n\nOkay great, we have all these awesome libraries that have been built out by others. How do we actually use them? In Python, it is actually fairly simple!\nOption 1: Use import to load an entire library module into a program’s memory. Refer to things from the module as module_name.thing_name\n\n import math\n\n print('pi is', math.pi)\n print('cos(pi) is', math.cos(math.pi))\n\npi is 3.141592653589793\ncos(pi) is -1.0\n\n\nOption 2: If we only need a specific function or tool from the library, use from module import thing\n\nfrom math import cos, pi\n\nprint('cos(pi) is', cos(pi))\n\ncos(pi) is -1.0\n\n\nOption 3: If we really do need the entire library, but we do not want to type the entire long name over and over, create an alias\n\nimport math as m\n\nprint('cos(pi) is', m.cos(m.pi))\n\ncos(pi) is -1.0\n\n\nSome common alias for common libraries include:\n\npandas → pd\nmatplotlib.pyplot → plt\nseaborn → sns\nnumpy → np"
  },
  {
    "objectID": "11_getting_data_with_pandas.html#introduction-to-pandas",
    "href": "11_getting_data_with_pandas.html#introduction-to-pandas",
    "title": "4  Getting Data With Pandas",
    "section": "4.1 Introduction to Pandas",
    "text": "4.1 Introduction to Pandas\nPandas is one of the most widely-used Python libraries for statistics, and excels at processing tabular data. If you have ever had any experience with R, it is modeled extensively on the dataframes used there. In this section we aim to learn about the fundamental objects used in pandas, bring our own data into these objects, and then view basic information about it.\n\n\n\n\n\n\nDid You Know?\n\n\n\nDid you know that the pandas library package is aptly named? It is a portmanteau of the words panel and data. We are literally viewing panels of data!\n\n\nFirst, we need to make sure that we import the library into Python so we can use it:\n\nimport pandas as pd\n\nObjects in pandas are typically two-dimensional tables called dataframes. You can think of this as being similar to a single spreadsheet in excel. Each column (called series) in the dataframe can have a name, as well as an indexing value.\n\n\n\nA pandas dataframe\n\n\nWe can create both single series in pandas or full dataframes. Let’s consider a single series first. We can create this using the pandas Series method, as well as a list of elements we wish to include in our series.\n\nseries_x = pd.Series([1, 2, 3])\nseries_x\n\n0    1\n1    2\n2    3\ndtype: int64\n\n\nWhen this displays in python, we see 2 columns. The left column is always the index, while the right column contains the data values. The series is also given a dtype. This is similar to the datatypes we considered previously, and tells us what type of data is held inside the series. When a series is first created, pandas will try to guess the dtype for the series based on the individual elements inside it. We can make a series made of float numbers:\n\nseries_y = pd.Series([1, 2.0, 3.14])\nseries_y\n\n0    1.00\n1    2.00\n2    3.14\ndtype: float64\n\n\nNotice that here, the first element was actually converted from an int to a float by the program! This will often happen when working with int and float type objects.\nNext, a series of str elements will default to the dtype of object.\n\nseries_z = pd.Series(['settlers', 'of', 'catan'])\nseries_z\n\n0    settlers\n1          of\n2       catan\ndtype: object\n\n\nThe dtype: object is pandas ‘catchall’ Series type, and we want to be cautious when we see this! It is also the Series dtype used when we have mixed data:\n\nseries_mixed = pd.Series([1, 1.0, 'one'])\nseries_mixed\n\n0      1\n1    1.0\n2    one\ndtype: object\n\n\nWhen pandas sees data that cannot be obviously converted into a single datatype, it leaves each individual entry alone, as whatever its original datatype happened to be. Be careful when you see this! Many operations on pandas dataframes apply to the entire column (similar to how a new excel column is often created with a function that applies to a previous column). If this operation is built to only work on a single datatype, we might run into errors! To avoid this, we can utilize the .astype() methods. Possible arguments for astype() include the standard datatypes:\n\nint\nfloat\nstr\nbool\n\nHowever if you need to be more specific, you can be as well. These arguments are all required to be included in quotations, as they refer to aliases for pandas specific datatypes:\n\n'Int8', 'Int32', 'Int64' provide nullable integer types (note the capital I! more on nullable typing below)\n'string' provides access to a series that will be string specific, instead of the general objects type. Pandas recommends using this type for strings whenever possible.\n\n\ndisplay(series_y)\ndisplay(series_y.astype(int))\ndisplay(series_y.astype(bool))\ndisplay(series_y.astype(str))\ndisplay(series_y.astype('string'))\n\n0    1.00\n1    2.00\n2    3.14\ndtype: float64\n\n\n0    1\n1    2\n2    3\ndtype: int64\n\n\n0    True\n1    True\n2    True\ndtype: bool\n\n\n0     1.0\n1     2.0\n2    3.14\ndtype: object\n\n\n0     1.0\n1     2.0\n2    3.14\ndtype: string\n\n\nYou will have noticed that each of these series has an index associated with it. We can access series indicies using the index attribute:\n\nseries_a = pd.Series([2, 4, 6])\ndisplay(series_a)\ndisplay(series_a.index)\n\n0    2\n1    4\n2    6\ndtype: int64\n\n\nRangeIndex(start=0, stop=3, step=1)\n\n\nWhen an index is first assigned to a series, it is automatically assigned as an integer index, with similar properties to a list index (starts at 0, can be sliced, etc.). However we can change this index to be whatever we want by directly modifying the index attribute:\n\nseries_a.index = ['a', 'b', 'c']\ndisplay(series_a.index)\ndisplay(series_a)\n\nIndex(['a', 'b', 'c'], dtype='object')\n\n\na    2\nb    4\nc    6\ndtype: int64\n\n\nA useful feature in pandas is the ability to reindex the dataset. Reindexing a dataset will do two things:\n\nReorder the data according to the order we ask for.\nAdd new rows for indicies that are missing in the original dataset but are included in our new index.\n\nOne popular use for this method is in filling out a timeseries dataset: if there were missing years in a dataset but we do not wish to simply skip over them, we can add the extra years to the index.\n\nseries_a = series_a.reindex(['d', 'c', 'b', 'a'])\ndisplay(series_a.index)\ndisplay(series_a)\n\nIndex(['d', 'c', 'b', 'a'], dtype='object')\n\n\nd    NaN\nc    6.0\nb    4.0\na    2.0\ndtype: float64\n\n\n\n\n\n\n\n\nBeware NaN values!\n\n\n\nSometimes in our datasets, we want to allow a row to contain missing, or null values. The default null value for pandas is NaN. In python, NaN is considered a float value. In the above example, we introduced a missing value for the new ‘d’ index, which defaults to the NaN float value. Because its type is float, this converted our entire series to float as well. If we wish to keep the series as an int, we can coerce it back using the astype() method with one of the nullable integer types. This will introduce a slightly different looking null value that works for integers!\n\nseries_a.astype('Int64')\n\nd    <NA>\nc       6\nb       4\na       2\ndtype: Int64\n\n\n\n\nWe commonly want to make new series out of old ones inside our dataframes. Operations are typically done on an element by element basis. We will see many examples of these in future sessions as we learn to manipulate dataframes, but here is a short example of what we may wish to do:\n\nseries_a = pd.Series([2, 4, 6])\nseries_b = pd.Series([1, 2, 3])\n\ndisplay(series_a + series_b)\ndisplay(series_a > 3)\ndisplay(series_a*5)\n\n0    3\n1    6\n2    9\ndtype: int64\n\n\n0    False\n1     True\n2     True\ndtype: bool\n\n\n0    10\n1    20\n2    30\ndtype: int64"
  },
  {
    "objectID": "11_getting_data_with_pandas.html#bringing-in-our-own-data",
    "href": "11_getting_data_with_pandas.html#bringing-in-our-own-data",
    "title": "4  Getting Data With Pandas",
    "section": "4.2 Bringing in our own data",
    "text": "4.2 Bringing in our own data\nEnough about series. Let’s talk dataframes! This is the main tool in our pandas toolkit. As we showed earlier, it is simply a collection of series all stacked together like an excel spreadsheet. There are many different ways to create a dataframe including:\n\nWithin python itself\nFrom a csv, excel file, or some other local tabular format\nFrom more exotic data sources, such as a parquet file, json, or a website\nFrom a SQL database\n\nFor the purposes of this course, we are going to focus on opening up local datafiles (with the most common type being a csv or excel file), and then utilizing the data once it is in python. To bring in data from a csv or excel file, we utilize the pandas methods read_csv() or read_excel(), with the only required argument being the path to the datafile.\nBut first we need some data! Navigate to this URL, right click on the data, and save the csv as gapfinder.csv in a folder called data inside our project folder. Now that we have a dataset, let’s load it into pandas.\n\n\n\n\n\n\nThe Data Folder\n\n\n\nWhile everyone may organize their projects and folders slightly differently, there are some general principles to adhere to that make project management easier. Arguably the most important of these is to treat input data as read only. If you have an excel spreadsheet, it is tempting to go make changes to the data directly in the spreadsheet: I’ll just tweak a single value here, or add a column there. However, once we start doing this we lose the concept of reproducibility! How did we get certain values if the changes are all hidden in the excel spreadsheet? By keeping data as read only and making all of our changes in the python scripts, the processing is not only reproducible but also transparent to others.\nBecause of this, it is common to have a folder exclusively for raw data. Secondary folders may then be set-up for ‘cleaned’ data, python scripts, analysis outputs and more.\n\n\n\ndf = pd.read_csv('../data/gapfinder.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1699\n      Zimbabwe\n      1987\n      9216418.0\n      Africa\n      62.351\n      706.157306\n    \n    \n      1700\n      Zimbabwe\n      1992\n      10704340.0\n      Africa\n      60.377\n      693.420786\n    \n    \n      1701\n      Zimbabwe\n      1997\n      11404948.0\n      Africa\n      46.809\n      792.449960\n    \n    \n      1702\n      Zimbabwe\n      2002\n      11926563.0\n      Africa\n      39.989\n      672.038623\n    \n    \n      1703\n      Zimbabwe\n      2007\n      12311143.0\n      Africa\n      43.487\n      469.709298\n    \n  \n\n1704 rows × 6 columns\n\n\n\n\n\n\n\n\n\n.. ?\n\n\n\nThe .. used in the above path tells python to look in the directory directly above the one you are currently in - the one where your notebook is saved.\nAs an example, my directory structure looks like:\n/ds_intro_to_python\n│\n└───/introduction_to_python\n│   │   hello_world.ipynb\n│   │   hello_world.py\n│   \n└───/introduction_to_pandas\n│   │   intro_to_pandas.ipynb  <------ (THIS FILE!)\n│\n└───/data\n│   │   gapfinder.csv <--------------- (The file we want access to!)\nThis file is in a different subfolder relative to the csv, so we first ‘back out’ of this folder using .., and then ‘enter’ the data folder using a regular file path. This is called relative pathing and can be useful for accessing data within a single project that will always be in the same spot!\n\n\nThis dataset has 6 columns, 1704 rows, and a mixture of different datatypes. Just like we were able to access the index of a series, we can do the same with a dataframe. Now, we can also access (and change if need be!) the column names as well:\n\ndisplay(df.index)\ndisplay(df.columns)\n\ndf.columns = ['country', 'year', 'pop', 'continent', 'life expectancy', 'gdpPercap']\ndisplay(df.columns)\n\nRangeIndex(start=0, stop=1704, step=1)\n\n\nIndex(['country', 'year', 'pop', 'continent', 'lifeExp', 'gdpPercap'], dtype='object')\n\n\nIndex(['country', 'year', 'pop', 'continent', 'life expectancy', 'gdpPercap'], dtype='object')\n\n\nWe can access each of these series individually if we want. There are two ways to access a series in a dataframe - either with square bracket indexing, or treating the column name as an attribute:\n\ndisplay(df['country'])\ndisplay(df.country)\n\n0       Afghanistan\n1       Afghanistan\n2       Afghanistan\n3       Afghanistan\n4       Afghanistan\n           ...     \n1699       Zimbabwe\n1700       Zimbabwe\n1701       Zimbabwe\n1702       Zimbabwe\n1703       Zimbabwe\nName: country, Length: 1704, dtype: object\n\n\n0       Afghanistan\n1       Afghanistan\n2       Afghanistan\n3       Afghanistan\n4       Afghanistan\n           ...     \n1699       Zimbabwe\n1700       Zimbabwe\n1701       Zimbabwe\n1702       Zimbabwe\n1703       Zimbabwe\nName: country, Length: 1704, dtype: object\n\n\n\n\n\n\n\n\nNo Spaces!\n\n\n\n\n\n\n\n\nAs a general rule of thumb, we never want to include special characters such as spaces, periods, hyphens and so on in our column names, as this will alter pandas capability of calling each of the columns as an attribute. Above, you will notice we reset one of the columns to have a space in the name. If we try to access this series as an attribute now, it will fail:\n\ndf['life expectancy']\n\n0       28.801\n1       30.332\n2       31.997\n3       34.020\n4       36.088\n         ...  \n1699    62.351\n1700    60.377\n1701    46.809\n1702    39.989\n1703    43.487\nName: life expectancy, Length: 1704, dtype: float64\n\n\n\ndf.life expectancy\n\nSyntaxError: invalid syntax (41671440.py, line 1)\n\n\nTry to stick to the same naming conventions for columns as for your python variables: lowercase letters, numbers (but not at the start of the name) and underscores only! (And as a matter of fact, let us change it back now):\n\ndf.columns = ['country', 'year', 'pop', 'continent', 'lifeExp', 'gdpPercap']\n\n\n\nWhen reading data into a dataframe from a csv (or an excel file), there are multiple optional arguments we can use to start the process of data wrangling, which is writing code to shape the data into the format we want it for our analysis. Some important options include:\n\nheader: row number to use as the column names. This allows us to skip past rows in the dataset and start from lower down if need be.\nindex_col: name of the column we might wish to use for the index of the dataframe instead of the default integer list.\nusecols: list of columns we wish to use. If the dataset is large with many columns that we do not care about, we can pull in only those of interest!\n\n\nChallenge 1\n\n\n\n\n\n\nChallenge 1\n\n\n\nBring the dataset into pandas again. This time:\n\nuse the country as the index\nonly include the year, continent and population columns.\n\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\n\ndf_challenge = pd.read_csv(\n    '../data/gapfinder.csv',\n    index_col = 'country',\n    usecols = ['country', 'continent', 'year', 'pop']\n)\ndisplay(df_challenge)\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      continent\n    \n    \n      country\n      \n      \n      \n    \n  \n  \n    \n      Afghanistan\n      1952\n      8425333.0\n      Asia\n    \n    \n      Afghanistan\n      1957\n      9240934.0\n      Asia\n    \n    \n      Afghanistan\n      1962\n      10267083.0\n      Asia\n    \n    \n      Afghanistan\n      1967\n      11537966.0\n      Asia\n    \n    \n      Afghanistan\n      1972\n      13079460.0\n      Asia\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      Zimbabwe\n      1987\n      9216418.0\n      Africa\n    \n    \n      Zimbabwe\n      1992\n      10704340.0\n      Africa\n    \n    \n      Zimbabwe\n      1997\n      11404948.0\n      Africa\n    \n    \n      Zimbabwe\n      2002\n      11926563.0\n      Africa\n    \n    \n      Zimbabwe\n      2007\n      12311143.0\n      Africa\n    \n  \n\n1704 rows × 3 columns\n\n\n\nNotice that, because we specified an index, the index now has a name! You can access the name of the index via df_challenge.index.name:\n\ndf_challenge.index.name\n\n'country'"
  },
  {
    "objectID": "11_getting_data_with_pandas.html#learning-about-our-data",
    "href": "11_getting_data_with_pandas.html#learning-about-our-data",
    "title": "4  Getting Data With Pandas",
    "section": "4.3 Learning about our data",
    "text": "4.3 Learning about our data\nOkay, so now we have a dataset. Great! Now what can we do with it? In the next few sessions, we will explore in detail some of the more in-depth tools that pandas gives us. For now, let’s stick to learning how to view different portions of the data, as well as learning how to describe the overall dataset.\nWhen viewing data, we do not want to be scrolling past multiple lines of individual rows of the data. This might be a shift in mindset if you are used to working with tables of data directly in front of you! An excel spreadsheet just has all the data right there for you to look at! Why not do that here? The simple answer is magnitude. If you only have 10s to 100s of rows of data, seeing it visually is okay. But once you start to deal with thousands, millions or even trillions of rows of data, it’s going to take a while to scroll through the entire thing. At this stage, the important piece of information is how we are treating the data we see, not the actual values.\nTypically, we just want to view a small slice of the data to get an understanding of the types of data we have in our dataset. We have three tools in the toolkit for this:\n\nhead(): This returns the first N rows of data (default N = 5)\ntail(): This returns the last N rows of data (default N = 5)\nsample(): This returns a random sampling of N rows of data (default N = 1)\n\n\nprint('The first 5 rows of data:')\ndisplay(df.head())\n\nprint('The last 3 rows of data:')\ndisplay(df.tail(3))\n\nprint('A random sampling of 7 rows of data:')\ndisplay(df.sample(7))\n\nThe first 5 rows of data:\n\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n  \n\n\n\n\nThe last 3 rows of data:\n\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      1701\n      Zimbabwe\n      1997\n      11404948.0\n      Africa\n      46.809\n      792.449960\n    \n    \n      1702\n      Zimbabwe\n      2002\n      11926563.0\n      Africa\n      39.989\n      672.038623\n    \n    \n      1703\n      Zimbabwe\n      2007\n      12311143.0\n      Africa\n      43.487\n      469.709298\n    \n  \n\n\n\n\nA random sampling of 7 rows of data:\n\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      989\n      Mexico\n      1977\n      63759976.0\n      Americas\n      65.032\n      7674.929108\n    \n    \n      63\n      Australia\n      1967\n      11872264.0\n      Oceania\n      71.100\n      14526.124650\n    \n    \n      798\n      Japan\n      1982\n      118454974.0\n      Asia\n      77.110\n      19384.105710\n    \n    \n      287\n      Chile\n      2007\n      16284741.0\n      Americas\n      78.553\n      13171.638850\n    \n    \n      58\n      Argentina\n      2002\n      38331121.0\n      Americas\n      74.340\n      8797.640716\n    \n    \n      1369\n      Slovak Republic\n      1957\n      3844277.0\n      Europe\n      67.450\n      6093.262980\n    \n    \n      1009\n      Montenegro\n      1957\n      442829.0\n      Europe\n      61.448\n      3682.259903\n    \n  \n\n\n\n\nOnce we have looked at the data, and it seems to look normal at first glance, we can ask some basic questions about the dataset. How many columns are there? How many rows? Are there null values in any of our columns? What about some basic statistics??\nLuckily for us, pandas has done all of the hard work here. Two valuable methods built into pandas will give us basic information about the overall dataset: .info() and .describe().\n.info() will gives us basic information about each column: what data type it is storing and how many non-null values are in the column.\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1704 entries, 0 to 1703\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   country    1704 non-null   object \n 1   year       1704 non-null   int64  \n 2   pop        1704 non-null   float64\n 3   continent  1704 non-null   object \n 4   lifeExp    1704 non-null   float64\n 5   gdpPercap  1704 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 80.0+ KB\n\n\n.describe() will give us basic statistical information about every numerical column: mean, standard deviation, quartiles, and counts are all included with a call to a single method!\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      count\n      1704.00000\n      1.704000e+03\n      1704.000000\n      1704.000000\n    \n    \n      mean\n      1979.50000\n      2.960121e+07\n      59.474439\n      7215.327081\n    \n    \n      std\n      17.26533\n      1.061579e+08\n      12.917107\n      9857.454543\n    \n    \n      min\n      1952.00000\n      6.001100e+04\n      23.599000\n      241.165876\n    \n    \n      25%\n      1965.75000\n      2.793664e+06\n      48.198000\n      1202.060309\n    \n    \n      50%\n      1979.50000\n      7.023596e+06\n      60.712500\n      3531.846988\n    \n    \n      75%\n      1993.25000\n      1.958522e+07\n      70.845500\n      9325.462346\n    \n    \n      max\n      2007.00000\n      1.318683e+09\n      82.603000\n      113523.132900\n    \n  \n\n\n\n\nFinally, if we want basic information about the non-numerical columns, we can use the value_counts() method. For a given series (or multiple series), this tells us how freqeuntly a given value appears. We will learn more about what this is doing under the hood when we learning about aggregation methods in a later section, but we can apply it to singular text columns here as a teaser\n\ndf.country.value_counts()\n\nAfghanistan          12\nPakistan             12\nNew Zealand          12\nNicaragua            12\nNiger                12\n                     ..\nEritrea              12\nEquatorial Guinea    12\nEl Salvador          12\nEgypt                12\nZimbabwe             12\nName: country, Length: 142, dtype: int64\n\n\n\nChallenge 2\n\n\n\n\n\n\nChallenge 2\n\n\n\nWhat will happen if we run the following code:\ndf.sample(42).describe()\n\nDo we expect the results to be the same as df.describe()? Why or why not?\nRun the code again. Are the results the same or different than before? Can you explain?\n\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\n\nThis should output a different result from df.describe(). We can break this down into two portions:\n\n\nWe create a new dataframe that holds the sampled dataframe via df.sample(42)\n\n\ndf_sample = df.sample(42)\n\n\nWe are now sending the results of this sampling to the describe method. Because the sampled dataset has only 42 rows that were randomly chosen from the original 1704, it would be an impressive coincidence if all the outputs were identical!\n\n\ndf_sample.describe()\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      count\n      42.000000\n      4.200000e+01\n      42.000000\n      42.000000\n    \n    \n      mean\n      1980.452381\n      1.486846e+07\n      59.704595\n      7244.834637\n    \n    \n      std\n      18.329241\n      2.234070e+07\n      13.409214\n      9081.995419\n    \n    \n      min\n      1952.000000\n      1.788480e+05\n      33.779000\n      380.995843\n    \n    \n      25%\n      1963.250000\n      3.617754e+06\n      47.463750\n      1621.618005\n    \n    \n      50%\n      1982.000000\n      5.281828e+06\n      61.348500\n      3455.584349\n    \n    \n      75%\n      1997.000000\n      1.576996e+07\n      71.400000\n      7917.584161\n    \n    \n      max\n      2007.000000\n      8.645902e+07\n      80.884000\n      36797.933320\n    \n  \n\n\n\n\n\nThe results should be different from the previous call! This is because sample() outputs a random sampling of the dataframe. Everytime we sample the dataset, we get a different subset! Each subset should end up with slightly different statistics if it is small enough relative to the entire dataset!\n\n\ndf.sample(42).describe()\n\n\n\n\n\n  \n    \n      \n      year\n      pop\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      count\n      42.000000\n      4.200000e+01\n      42.000000\n      42.000000\n    \n    \n      mean\n      1982.357143\n      3.719869e+07\n      59.541786\n      6275.476615\n    \n    \n      std\n      15.979898\n      1.702401e+08\n      13.083672\n      6692.722794\n    \n    \n      min\n      1952.000000\n      1.820530e+05\n      39.327000\n      430.070692\n    \n    \n      25%\n      1972.000000\n      3.837270e+06\n      46.901000\n      875.022540\n    \n    \n      50%\n      1982.000000\n      6.993469e+06\n      61.347000\n      4064.876223\n    \n    \n      75%\n      1992.000000\n      1.068234e+07\n      71.711750\n      8302.843684\n    \n    \n      max\n      2007.000000\n      1.110396e+09\n      78.123000\n      24703.796150"
  },
  {
    "objectID": "11_getting_data_with_pandas.html#saving-our-data",
    "href": "11_getting_data_with_pandas.html#saving-our-data",
    "title": "4  Getting Data With Pandas",
    "section": "4.4 Saving our data",
    "text": "4.4 Saving our data\n\n\n\n\n\n\n\n\n\nYes, you can even save to a pickle.\n\n\n\n\n\nWhat do we do once we have cleaned up our data or produced some analysis? It is very likely that we will want to save that clean dataset or analysis to a new file. Pandas to the rescue! As simple as it is to read in data via read_csv() or read_excel(), we can export it back out. While I’ve shown the entire list of to_file() options available in pandas (it’s extensive!), we will focus on to_csv(). Required arguments to this method are:\n\npath_or_buf - full path/filename where you wish to save this file\n\nThat’s it! However, there are some useful optional arguments as well:\n\nindex: True or False. Whether we wish to include the index in our output (default is True). We will often want to set this to False, as the index is just a set of integers labeling the row numbers.\ncolumns: list of columns to keep in the output\n\n\n\n\n\n\n\npd.method() or df.method()?\n\n\n\nSometimes, in order to access a function, we directly access it via the library (pd), or we access it as a method of the dataframe we are using (df). It can be hard to keep track of which functions live where. As a general rule of thumb, if the method is being used to do something to a specific dataframe, it probably belongs to the dataframe object (which is, let’s be honest, most of the functions we might use). Don’t be afraid to use all of your resources to keep it straight! (Yes, google counts as a valid resource). Using the help function is also a quick and easy way to check where a method lives.\nFor example, to find the to_csv() function, we can see that it belongs to the dataframe by checking help(df.to_csv). However, trying to use pd.to_csv will throw an error - a good hint that it was a dataframe method all along!\n\nhelp(pd.to_csv)\n\nAttributeError: module 'pandas' has no attribute 'to_csv'\n\n\nInversely, the read_csv function belongs directly to pandas, and so trying something like this will break:\n\nhelp(df.read_csv)\n\nAttributeError: 'DataFrame' object has no attribute 'read_csv'\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\nChallenge 3\n\n\n\nSave a copy of all of the summary statistics for the gapfinder dataset. Only include the statistics for the pop and lifeExp columns. What happens when we include or exclude the index?\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nFirst, we want to make sure we have a folder to save our outputs to. I have made a folder called outputs that lives at the same level as the data folder. Next, we can invoke the to_csv method on a copy of our descriptive statistics:\n\ndf_descriptive = df.describe()\ndf_descriptive.to_csv('../outputs/challenge_output.csv', columns=['pop', 'lifeExp'])\n\nIn this case, if we exclude the index, we will actually lose information about what each row represents. This is one case when keeping the index will retain valuable information!\n\ndf.describe().to_csv(\n    '../outputs/challenge_output_no_index.csv', \n    columns=['pop', 'lifeExp'],\n    index=False\n    )\n\n\n\n\nUp next, we will learn how to clean our data."
  },
  {
    "objectID": "22_cleaning_it_up.html#cleaning-and-data-science",
    "href": "22_cleaning_it_up.html#cleaning-and-data-science",
    "title": "5  Cleaning Data",
    "section": "5.1 Cleaning and data science",
    "text": "5.1 Cleaning and data science\nImagine you have finally gotten that dataset that you need to work with to answer those questions you’ve been asked to clear up. But wait! Before you get started in earnest working away with generating insights, it is important that you take a closer look at the “quality” of the data.\nThere is much to look at and consider when it comes to the topic of data cleaning. You probably have heard it said that 80% (or some high percentage) of a data scientist’s time is spent cleaning data, that is, putting it in a form that will better suit its downstream uses.\nNot surprisingly, data cleaning is a topic difficult to cover systematically in an introductory course.\n\n\n\nIt’s not so bad, promise!\n\n\nSo in this tutorial we will focus on a few cleaning techniques - with pandas - that are likely going to be leveraged over and over again when you work with data in python.\n\nHow to identify and clean up missing data\nWhen and how to change datatypes\nWhen and how to modify values in your dataset"
  },
  {
    "objectID": "22_cleaning_it_up.html#dealing-with-missing-data",
    "href": "22_cleaning_it_up.html#dealing-with-missing-data",
    "title": "5  Cleaning Data",
    "section": "5.2 Dealing with Missing Data",
    "text": "5.2 Dealing with Missing Data\nOne of the most common issues in data cleaning has to do with data that is missing from your dataset. Most data, even data that is meticulously collected and managed, is likely to have empty cells. Empty data can be harmless, or it can represent a challenge to you, depending on what you need your data to do once you have finished working with it.\nThe best way to learn about these concepts is to see them in action with real (or real-ish) data! Before we get started with fetching the data, let’s go ahead and get pandas for today’s session.\n\n# import pandas\nimport pandas as pd \n\nNow, imagine we had our own online streaming service. We have access to some data that will help us predict what our users would like to see. To fetch it, run the code below:\n\n# import data\nmr = \"https://raw.githubusercontent.com/bcgov/\" \\\n        \"ds-intro-to-python/main/data/movieratings.csv\"\nmovie_ratings = pd.read_csv(mr)\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      0\n      Floriana\n      NaN\n      5.0\n      5.0\n      3.0\n      NaN\n    \n    \n      1\n      Raymundo\n      4.0\n      NaN\n      NaN\n      NaN\n      5.0\n    \n    \n      2\n      Jung\n      5.0\n      NaN\n      NaN\n      5.0\n      NaN\n    \n    \n      3\n      Kumar\n      5.0\n      NaN\n      4.0\n      NaN\n      4.0\n    \n    \n      4\n      Maria\n      5.0\n      4.0\n      5.0\n      NaN\n      NaN\n    \n  \n\n\n\n\nThis is a 10 row dataframe, with each row containing a movie rater as well as their ratings for selected movies that they have given ratings for. For interpretative purposes, each movie is rated on a 5 point scale, with 5 being super awesome and 1 being dismal. Not every movie is rated by every rater, so several cells are blank. These are the missing data cells and are marked with “NaN”, indicating missing data.\nThis dataset, while small and made up, exemplifies a classic “recommendation engine” use case problem - there are lots of raters and lots of movies, but an awful lot of missing data. So what one chooses to do with missing data is a key cleaning question.\n\nLocating Missing Data\nIn pandas, there is a widely used method named isna() which can be applied to a whole dataframe or just to a particular series or column.\nBelow we apply it to the entire dataframe to get a cell by cell view of whether the cell data is missing or not (True means it is missing, False means it is not):\n\n#find the missing values\nmovie_ratings.isna()\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      0\n      False\n      True\n      False\n      False\n      False\n      True\n    \n    \n      1\n      False\n      False\n      True\n      True\n      True\n      False\n    \n    \n      2\n      False\n      False\n      True\n      True\n      False\n      True\n    \n    \n      3\n      False\n      False\n      True\n      False\n      True\n      False\n    \n    \n      4\n      False\n      False\n      False\n      False\n      True\n      True\n    \n    \n      5\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      6\n      False\n      True\n      True\n      False\n      False\n      False\n    \n    \n      7\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      8\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      9\n      False\n      False\n      True\n      True\n      True\n      True\n    \n  \n\n\n\n\nIt is probably helpful to note there is a similar function called isnull() that is an alias for isna(). You may run into both, and both will return the same results. There is also a notna()(notnull()) function, which is the inverse function of isna()(isnull()), returning True for values that are not NA. For simplicity, let’s just stick with isna() in this tutorial.\nTo get a better summary overview (i.e. one that summarizes with numbers) of how many missing values there are per column, then we need to chain togther a .sum() function to the line of code like so:\n\n###listing out the missing values in the dataframe for each series\nmovie_ratings.isna().sum()\n\nRater           0\nStar Wars       2\nFinding Nemo    5\nForrest Gump    3\nParasite        4\nCitizen Kane    4\ndtype: int64\n\n\nThe reason that this works is because a boolean value of True is represented in pandas by the value of 1, while False is represented by a 0. Therefore, we can apply the sum()method to the dataframe with series being returned that contains the counts of missing items in each column.\nNow we see no missing data for Rater, but missing data for each of the movies. For our use case, missing data is a real challenge. With pandas, we can deal with this in many different ways.\n\n\nDropping rows and columns\nOne cleaning option we have is to drop entire rows or columns with missing values in them from the dataframe. The method is called dropna()and takes a number of parameters:\n\naxis: {0 or ‘index’, 1 or ‘columns’}, default 0\nhow: {‘any’, ‘all’}, default ‘any’\nsubset: column label or sequence of labels\n\nImagine we would like to drop all of the rows with any missing values in them. Since the axis=0 and how=any parameters are the defaults, we can write an elegant and simple line of code:\n\nreduced_rows = movie_ratings.dropna()\nreduced_rows.head(11)\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      5\n      Arthur\n      2.0\n      2.0\n      3.0\n      3.0\n      3.0\n    \n    \n      7\n      Martina\n      5.0\n      5.0\n      5.0\n      5.0\n      5.0\n    \n    \n      8\n      Orson\n      1.0\n      1.0\n      1.0\n      2.0\n      5.0\n    \n  \n\n\n\n\nWe still have three raters who have given us ratings for each movie. The dataset is nice and clean now, although it has come at a pretty big cost, losing seven rows of data! If we go in the other direction, and insert a parameter of how='all', then we have the opposite problem, with no rows being eliminated (as there were no instances where all of the columns contained missing data for a given row)!\nAnother option is to use subset to eliminate rows on the basis of whether values are missing in a specified subset. Let’s say we want to have just the rows of data where there is not missing ratings data for the film “Parasite”. We would use subset= parameter and identify the column we want to clean up:\n\npar_df = movie_ratings.dropna(subset='Parasite')\npar_df.head(11)\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      0\n      Floriana\n      NaN\n      5.0\n      5.0\n      3.0\n      NaN\n    \n    \n      2\n      Jung\n      5.0\n      NaN\n      NaN\n      5.0\n      NaN\n    \n    \n      5\n      Arthur\n      2.0\n      2.0\n      3.0\n      3.0\n      3.0\n    \n    \n      6\n      Marcellus\n      NaN\n      NaN\n      4.0\n      5.0\n      4.0\n    \n    \n      7\n      Martina\n      5.0\n      5.0\n      5.0\n      5.0\n      5.0\n    \n    \n      8\n      Orson\n      1.0\n      1.0\n      1.0\n      2.0\n      5.0\n    \n  \n\n\n\n\nWe can see that, while there is no NaN in the Parasite column, we can see they are still there sprinkled throughout other columns.\nNow what would happen if we applied that same drop logic to all columns?\n\nreduced_cols = movie_ratings.dropna(axis=1)\nreduced_cols.head()\n\n\n\n\n\n  \n    \n      \n      Rater\n    \n  \n  \n    \n      0\n      Floriana\n    \n    \n      1\n      Raymundo\n    \n    \n      2\n      Jung\n    \n    \n      3\n      Kumar\n    \n    \n      4\n      Maria\n    \n  \n\n\n\n\nWe see that we pretty much all the movie columns have been removed, as each of these columns had at least one missing value! So be careful when using dropna() as it can be a powerful eraser of data. Whether you should use it or how you should will depend on the downstream uses for your data.\n\n\n\nSaving the dataframe “inplace”\nInplace is an argument used in different functions such as dropna() that we have just looked at. The default value of this attribute is False, which means whatever the function does to the dataframe, it does so as a copy of the object. When the argument is inplace=True, however, whatever changes are made by the given function are made to the original object inplace, not just to a copy of it!\nLet’s look at how this works starting with fetching the data from the source:\n\n# Fetching the data to make sure we start again with the original\nmr = \"https://raw.githubusercontent.com/bcgov/\" \\\n        \"ds-intro-to-python/main/data/movieratings.csv\"\nmovie_ratings = pd.read_csv(mr)\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      0\n      Floriana\n      NaN\n      5.0\n      5.0\n      3.0\n      NaN\n    \n    \n      1\n      Raymundo\n      4.0\n      NaN\n      NaN\n      NaN\n      5.0\n    \n    \n      2\n      Jung\n      5.0\n      NaN\n      NaN\n      5.0\n      NaN\n    \n    \n      3\n      Kumar\n      5.0\n      NaN\n      4.0\n      NaN\n      4.0\n    \n    \n      4\n      Maria\n      5.0\n      4.0\n      5.0\n      NaN\n      NaN\n    \n  \n\n\n\n\nNow let’s “modify” the frame with inplace=False with the code below and see what happens:\n\n# Making change to dataframe with inplace=False\nmovie_ratings.dropna(how='any', inplace=False)\n\n# Looking at the dataframe\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      0\n      Floriana\n      NaN\n      5.0\n      5.0\n      3.0\n      NaN\n    \n    \n      1\n      Raymundo\n      4.0\n      NaN\n      NaN\n      NaN\n      5.0\n    \n    \n      2\n      Jung\n      5.0\n      NaN\n      NaN\n      5.0\n      NaN\n    \n    \n      3\n      Kumar\n      5.0\n      NaN\n      4.0\n      NaN\n      4.0\n    \n    \n      4\n      Maria\n      5.0\n      4.0\n      5.0\n      NaN\n      NaN\n    \n  \n\n\n\n\nOk, it kind of looks like “nothing happened”! And in a way, with the code we ran, nothing did happen, as the result was not assigned to a new object. Soooo, what’s the point of that? Any guesses?\nWhen we run exactly the same code except with the parameter inplace set to 'True', the dropped data is actually gone!\n\n# Making change to dataframe with inplace=False\nmovie_ratings.dropna(how='any', inplace=True)\n\n# Looking at the dataframe\nmovie_ratings.head()\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      5\n      Arthur\n      2.0\n      2.0\n      3.0\n      3.0\n      3.0\n    \n    \n      7\n      Martina\n      5.0\n      5.0\n      5.0\n      5.0\n      5.0\n    \n    \n      8\n      Orson\n      1.0\n      1.0\n      1.0\n      2.0\n      5.0\n    \n  \n\n\n\n\nSo now if we want to work with the “movie_ratings” object without the columns we just dropped, we would have to go back and rerun the read_csv() function and fetch the original data again! It is therefore important when writing your cleaning code that you use this parameter carefully.\n\n\nFilling in missing data\nSometimes it is the right cleaning decision to fill in missing data with some values.\nTake the example of our movie ratings dataframe. The basic way to fill in missing values would you to specify a value to take the place of each missing value. One approach is to calculate and plug in the mean value - which we know to be (3.85) for the entire dataframe:\n\n# Fetching the data, yet again, to make sure we start again with the original\nmr = \"https://raw.githubusercontent.com/bcgov/\" \\\n        \"ds-intro-to-python/main/data/movieratings.csv\"\nmovie_ratings = pd.read_csv(mr)\n\n# using fillna and a passing in a constant\nfilled_385 = movie_ratings.fillna(3.85)\nfilled_385.head(10)\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      0\n      Floriana\n      3.85\n      5.00\n      5.00\n      3.00\n      3.85\n    \n    \n      1\n      Raymundo\n      4.00\n      3.85\n      3.85\n      3.85\n      5.00\n    \n    \n      2\n      Jung\n      5.00\n      3.85\n      3.85\n      5.00\n      3.85\n    \n    \n      3\n      Kumar\n      5.00\n      3.85\n      4.00\n      3.85\n      4.00\n    \n    \n      4\n      Maria\n      5.00\n      4.00\n      5.00\n      3.85\n      3.85\n    \n    \n      5\n      Arthur\n      2.00\n      2.00\n      3.00\n      3.00\n      3.00\n    \n    \n      6\n      Marcellus\n      3.85\n      3.85\n      4.00\n      5.00\n      4.00\n    \n    \n      7\n      Martina\n      5.00\n      5.00\n      5.00\n      5.00\n      5.00\n    \n    \n      8\n      Orson\n      1.00\n      1.00\n      1.00\n      2.00\n      5.00\n    \n    \n      9\n      Luke\n      5.00\n      3.85\n      3.85\n      3.85\n      3.85\n    \n  \n\n\n\n\nOk, that’s good. Good, but not great for our use case! Maybe we could avail ourselves to some options within the method= parameter with the parentheses the fillna() function. With a specification of bfill we can fill a NaN value with the preceding valid value in that column. ffill fills with the value after it.\nWhat would that look like for our dataset?\n\nbfilled_mr = movie_ratings.fillna(method='bfill')\nbfilled_mr.head(10)\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      0\n      Floriana\n      4.0\n      5.0\n      5.0\n      3.0\n      5.0\n    \n    \n      1\n      Raymundo\n      4.0\n      4.0\n      4.0\n      5.0\n      5.0\n    \n    \n      2\n      Jung\n      5.0\n      4.0\n      4.0\n      5.0\n      4.0\n    \n    \n      3\n      Kumar\n      5.0\n      4.0\n      4.0\n      3.0\n      4.0\n    \n    \n      4\n      Maria\n      5.0\n      4.0\n      5.0\n      3.0\n      3.0\n    \n    \n      5\n      Arthur\n      2.0\n      2.0\n      3.0\n      3.0\n      3.0\n    \n    \n      6\n      Marcellus\n      5.0\n      5.0\n      4.0\n      5.0\n      4.0\n    \n    \n      7\n      Martina\n      5.0\n      5.0\n      5.0\n      5.0\n      5.0\n    \n    \n      8\n      Orson\n      1.0\n      1.0\n      1.0\n      2.0\n      5.0\n    \n    \n      9\n      Luke\n      5.0\n      NaN\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nNotice that the bottom row stays NaN when we use bfill. That’s because it can not find a value to backfill with for this row! Using forward fill creates the same issue for the first row. For our use case, admittedly bfill and ffill probably are sub-optimal solutions.\nSince we have some ratings data already for each movie, one approach would be to fill in the missing data with the mean rating for that movie from the valid data.\nLet’s try that approach on a single column. For the movie Parasite, we fill in the missing values for that column with the average value:\n\nmovie_ratings['Parasite'] = movie_ratings['Parasite']. \\\n  fillna(movie_ratings['Parasite'].mean())\nmovie_ratings.head(10)\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      0\n      Floriana\n      NaN\n      5.0\n      5.0\n      3.000000\n      NaN\n    \n    \n      1\n      Raymundo\n      4.0\n      NaN\n      NaN\n      3.833333\n      5.0\n    \n    \n      2\n      Jung\n      5.0\n      NaN\n      NaN\n      5.000000\n      NaN\n    \n    \n      3\n      Kumar\n      5.0\n      NaN\n      4.0\n      3.833333\n      4.0\n    \n    \n      4\n      Maria\n      5.0\n      4.0\n      5.0\n      3.833333\n      NaN\n    \n    \n      5\n      Arthur\n      2.0\n      2.0\n      3.0\n      3.000000\n      3.0\n    \n    \n      6\n      Marcellus\n      NaN\n      NaN\n      4.0\n      5.000000\n      4.0\n    \n    \n      7\n      Martina\n      5.0\n      5.0\n      5.0\n      5.000000\n      5.0\n    \n    \n      8\n      Orson\n      1.0\n      1.0\n      1.0\n      2.000000\n      5.0\n    \n    \n      9\n      Luke\n      5.0\n      NaN\n      NaN\n      3.833333\n      NaN\n    \n  \n\n\n\n\nAnd we can extend that for all numeric columns at once!\n\nmovie_ratings = movie_ratings.fillna(movie_ratings.mean(numeric_only=True))\nmovie_ratings.head(10)\n\n\n\n\n\n  \n    \n      \n      Rater\n      Star Wars\n      Finding Nemo\n      Forrest Gump\n      Parasite\n      Citizen Kane\n    \n  \n  \n    \n      0\n      Floriana\n      4.0\n      5.0\n      5.000000\n      3.000000\n      4.333333\n    \n    \n      1\n      Raymundo\n      4.0\n      3.4\n      3.857143\n      3.833333\n      5.000000\n    \n    \n      2\n      Jung\n      5.0\n      3.4\n      3.857143\n      5.000000\n      4.333333\n    \n    \n      3\n      Kumar\n      5.0\n      3.4\n      4.000000\n      3.833333\n      4.000000\n    \n    \n      4\n      Maria\n      5.0\n      4.0\n      5.000000\n      3.833333\n      4.333333\n    \n    \n      5\n      Arthur\n      2.0\n      2.0\n      3.000000\n      3.000000\n      3.000000\n    \n    \n      6\n      Marcellus\n      4.0\n      3.4\n      4.000000\n      5.000000\n      4.000000\n    \n    \n      7\n      Martina\n      5.0\n      5.0\n      5.000000\n      5.000000\n      5.000000\n    \n    \n      8\n      Orson\n      1.0\n      1.0\n      1.000000\n      2.000000\n      5.000000\n    \n    \n      9\n      Luke\n      5.0\n      3.4\n      3.857143\n      3.833333\n      4.333333\n    \n  \n\n\n\n\nWhile far from perfect, now we have some filled in data that has some reasonable chance of adding value.\nSimilar to fillna() is a method called interpolate(), which takes the average of the values near the missing values and plugs them in. To do this, we pass in the interpolate() in similar fashion to how we used fillna() above.\n\n\n\n\n\n\nChallenge 1\n\n\n\nYou are given a dataset for several days time frame in April/May 2021 of the average daily temperature in celcius for three Canadian cities. You would like to ultimately create a plot of the temperatures but don’t want to have gaps in the data. Run the code below to get the data and see what it looks like.\n\nimport pandas as pd\n\ntemps = \"https://raw.githubusercontent.com/bcgov/\" \\\n        \"ds-intro-to-python/main/data/citytemps.csv\"\ncity_temps = pd.read_csv(temps)\ncity_temps.head(10)\n\n\n\n\n\n  \n    \n      \n      Day\n      Victoria\n      Regina\n      Sudbury\n    \n  \n  \n    \n      0\n      25-Feb-21\n      15.0\n      NaN\n      7.0\n    \n    \n      1\n      26-Feb-21\n      18.0\n      -11.0\n      NaN\n    \n    \n      2\n      27-Feb-21\n      NaN\n      -6.0\n      NaN\n    \n    \n      3\n      28-Feb-21\n      12.0\n      -2.0\n      4.0\n    \n    \n      4\n      01-Mar-21\n      NaN\n      NaN\n      7.0\n    \n    \n      5\n      02-Mar-21\n      NaN\n      NaN\n      NaN\n    \n    \n      6\n      03-Mar-21\n      NaN\n      NaN\n      14.0\n    \n    \n      7\n      04-Mar-21\n      15.0\n      7.0\n      11.0\n    \n    \n      8\n      05-Mar-21\n      13.0\n      NaN\n      9.0\n    \n    \n      9\n      06-Mar-21\n      11.0\n      4.0\n      14.0\n    \n  \n\n\n\n\nUnfortunately you notice that several of the days have data missing. Write code that leverages the interpolate() function to fill in the missing data. Run the code. Is this an appropriate solution for the problem? What are the pros and cons to this approach.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nCode you would write is below.\n\ninterpolate_temps = city_temps.interpolate()\ninterpolate_temps.head(10)\n\n\n\n\n\n  \n    \n      \n      Day\n      Victoria\n      Regina\n      Sudbury\n    \n  \n  \n    \n      0\n      25-Feb-21\n      15.00\n      NaN\n      7.0\n    \n    \n      1\n      26-Feb-21\n      18.00\n      -11.00\n      6.0\n    \n    \n      2\n      27-Feb-21\n      15.00\n      -6.00\n      5.0\n    \n    \n      3\n      28-Feb-21\n      12.00\n      -2.00\n      4.0\n    \n    \n      4\n      01-Mar-21\n      12.75\n      0.25\n      7.0\n    \n    \n      5\n      02-Mar-21\n      13.50\n      2.50\n      10.5\n    \n    \n      6\n      03-Mar-21\n      14.25\n      4.75\n      14.0\n    \n    \n      7\n      04-Mar-21\n      15.00\n      7.00\n      11.0\n    \n    \n      8\n      05-Mar-21\n      13.00\n      5.50\n      9.0\n    \n    \n      9\n      06-Mar-21\n      11.00\n      4.00\n      14.0\n    \n  \n\n\n\n\nHere we can see how the values “smooth” out the data. Were we to go ahead and plot the data, it would make more sense.\n\n\n\nDealing with missing data is one normal task in cleaning, changing data and header content is another, let’s take a look at that next."
  },
  {
    "objectID": "22_cleaning_it_up.html#modifying-existing-data",
    "href": "22_cleaning_it_up.html#modifying-existing-data",
    "title": "5  Cleaning Data",
    "section": "5.3 Modifying existing data",
    "text": "5.3 Modifying existing data\nNow let’s grad a different dataset, this one is quite redacted snip of the 2014 Mental Health in Tech Survey https://www.kaggle.com/datasets/osmi/mental-health-in-tech-survey.\n\ntechhealth = \"https://raw.githubusercontent.com/bcgov/\" \\\n        \"ds-intro-to-python/main/data/techhealth.csv\"\nm_health = pd.read_csv(techhealth)\nm_health.head()\n\n\n\n\n\n  \n    \n      \n      Timestamp\n      Age\n      Gender\n      Country\n      self employed\n      family history\n      treatment\n      work interfere\n      remote work\n      tech company\n      benefits\n      leave\n      mental health consequence\n    \n  \n  \n    \n      0\n      27/08/2014 11:35\n      46\n      Male\n      United States\n      No\n      No\n      Yes\n      Often\n      Yes\n      Yes\n      Yes\n      Don't know\n      Maybe\n    \n    \n      1\n      27/08/2014 11:36\n      41\n      Male\n      United States\n      No\n      No\n      Yes\n      Never\n      No\n      No\n      Don't know\n      Don't know\n      Maybe\n    \n    \n      2\n      27/08/2014 11:36\n      33\n      male\n      United States\n      No\n      Yes\n      Yes\n      Rarely\n      No\n      Yes\n      Yes\n      Don't know\n      No\n    \n    \n      3\n      27/08/2014 11:37\n      35\n      male\n      United States\n      No\n      Yes\n      Yes\n      Sometimes\n      No\n      No\n      Yes\n      Very easy\n      Yes\n    \n    \n      4\n      27/08/2014 11:42\n      35\n      M\n      United States\n      No\n      No\n      Yes\n      Rarely\n      Yes\n      Yes\n      Yes\n      Very easy\n      No\n    \n  \n\n\n\n\n This abridged version of the dataset contains 24 cases and 13 variables:\n\nTimestamp\nAge\nGender\nCountry\nSelf Employed: Are you self-employed?\nFamily History: Do you have a family history of mental illness?\nTreatment: Have you sought treatment for a mental health condition?\nWork Interfere: If you have a mental health condition, do you feel that it interferes with your work?\nRemote Work: Do you work remotely (outside of an office) at least 50% of the time?\nTech Company: Is your employer primarily a tech company/organization?\nBenefits: Does your employer provide mental health benefits?\nLeave: How easy is it for you to take medical leave for a mental health condition?\nMental Health Consequence: Do you think that discussing a mental health issue with your employer would have negative consequences?\n\n\nCleaning the header row\nA good practice, both in terms of ensuring a consistent convention for column names and for the ability to write more efficient code, is to ensure that all column header names do not have spaces in them and are all in lower case. To do this with our current dataset, we use the str.replace() function on all the header row, identified in pandas via .columns:\n\nm_health.columns = m_health.columns.str.replace(' ', '_')\nm_health.columns = m_health.columns.str.lower()\nm_health.head(10)\nm_health.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 24 entries, 0 to 23\nData columns (total 13 columns):\n #   Column                     Non-Null Count  Dtype \n---  ------                     --------------  ----- \n 0   timestamp                  24 non-null     object\n 1   age                        24 non-null     int64 \n 2   gender                     24 non-null     object\n 3   country                    24 non-null     object\n 4   self_employed              24 non-null     object\n 5   family_history             24 non-null     object\n 6   treatment                  24 non-null     object\n 7   work_interfere             24 non-null     object\n 8   remote_work                24 non-null     object\n 9   tech_company               24 non-null     object\n 10  benefits                   24 non-null     object\n 11  leave                      24 non-null     object\n 12  mental_health_consequence  24 non-null     object\ndtypes: int64(1), object(12)\nmemory usage: 2.6+ KB\n\n\nThe other thing to notice that there are no missing values! So we can just concentrate on other cleaning tasks.\n\n\nEnsuring the right datatypes\nA good practice is to notice whether there are series in the dataframe that are improperly datatyped (that is, where the “Dtype” indicated does not correspond to the datatype that it should be).\nTaking a look at the data, we notice that “Timestamp” is datatyped as an “object”, which in this context means a “string”. However, that is sub-optimal because the content is time-oriented, and by coding it as an object/string, we limit a lot of its value. If we identify datatype as a time oriented one, that can give us advantages downstream in being able to work with the data more elegantly.\nPandas has functions specifically designed to cast datatypes from one to another, as was covered in the previous section with the astype() function.\nTo cast a variable as a datetime datatype, we can use the pandas to_datetime() function and pass in the pandas series you want to modify:\n\nm_health['timestamp'] = pd.to_datetime(m_health['timestamp'])\nm_health.head()\nm_health.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 24 entries, 0 to 23\nData columns (total 13 columns):\n #   Column                     Non-Null Count  Dtype         \n---  ------                     --------------  -----         \n 0   timestamp                  24 non-null     datetime64[ns]\n 1   age                        24 non-null     int64         \n 2   gender                     24 non-null     object        \n 3   country                    24 non-null     object        \n 4   self_employed              24 non-null     object        \n 5   family_history             24 non-null     object        \n 6   treatment                  24 non-null     object        \n 7   work_interfere             24 non-null     object        \n 8   remote_work                24 non-null     object        \n 9   tech_company               24 non-null     object        \n 10  benefits                   24 non-null     object        \n 11  leave                      24 non-null     object        \n 12  mental_health_consequence  24 non-null     object        \ndtypes: datetime64[ns](1), int64(1), object(11)\nmemory usage: 2.6+ KB\n\n\nEt voila - timestamp is now of the datatime64 datatype! Were we wanting to leverage timestamp’s date time qualities, we could now do so more easily. It should be added, that this example works well because the existing data is nicely formatted to be understood as data/time datatype data. In the real world, if the timestamp column has different date types, or isn’t actually a date column at all, the method raises exceptions that you will have to work through. For example, look at what happens if we try to force a different variable into a data/time data type:\n\nm_health['tech_company'] = pd.to_datetime(m_health['tech_company'])\nm_health.head()\nm_health.info()\n\nParserError: Unknown string format: Yes\n\n\nThe error let’s us know that this data typing conversion will not work as coded.\n\n\nRecoding values\nValue recoding is a data cleaning task to put the column values into more manageable categories. This is a common data recoding task that most data analyst type folks will be familiar with. Pandas alone has many functions that solve recoding issues in different ways. We will just highlight a few in this tutorial.\nWith this dataset we’ve called there are a few columns that we would like to make changes to so that it will be easier to analyze the data in the way we want. The value_counts() method shows the range of responses and their frequency in the dataframe.\n\nm_health['gender'].value_counts()\n\nMale        12\nmale         6\nM            2\nFemale       1\nm            1\nF            1\nCis Male     1\nName: gender, dtype: int64\n\n\nWith that knowledge, we can utilize the replace() function and pass in custom lists with all of the responses that should be replaced by a single specfied value.\n\nm_health['gender'].replace(['Male ', 'male', 'M', 'm', 'Male', 'Cis Male'],\n                            'Male', inplace=True)\nm_health['gender'].replace(['Female ', 'female', 'F'],\n                            'Female', inplace=True)\nm_health['gender'].value_counts()\n\nMale      22\nFemale     2\nName: gender, dtype: int64\n\n\nAnother task is to put categorical objects into a form that is more amenable to computational tasks. Let’s take a look at a categorial variable that consists of responses of “Yes” and “No”.\n\nm_health['self_employed'].value_counts()\n\nNo     23\nYes     1\nName: self_employed, dtype: int64\n\n\nIf we want to leverage this data purely as a segment to split other metrics with, we would probably leave as is. But perhaps we would like to treat it as a dummy variable so we can do more sophisticated math with it.\nFor our self-employed variable, we begin by implementing replace() to transform the existing content to numbers as shown below:\n\nm_health['self_employed'].replace(['Yes'], '1', inplace=True)\nm_health['self_employed'].replace(['No'], '0', inplace=True)\nm_health['self_employed'].value_counts()\n\n0    23\n1     1\nName: self_employed, dtype: int64\n\n\nThe pandas astype() function is able to interpret the 1s and 0s as integers and so we can transform the series into an int datatype.\n\nm_health['self_employed'] = m_health['self_employed'] \\\n        .astype(dtype='int')\nm_health.head()\n\n\n\n\n\n  \n    \n      \n      timestamp\n      age\n      gender\n      country\n      self_employed\n      family_history\n      treatment\n      work_interfere\n      remote_work\n      tech_company\n      benefits\n      leave\n      mental_health_consequence\n    \n  \n  \n    \n      0\n      2014-08-27 11:35:00\n      46\n      Male\n      United States\n      0\n      No\n      Yes\n      Often\n      Yes\n      Yes\n      Yes\n      Don't know\n      Maybe\n    \n    \n      1\n      2014-08-27 11:36:00\n      41\n      Male\n      United States\n      0\n      No\n      Yes\n      Never\n      No\n      No\n      Don't know\n      Don't know\n      Maybe\n    \n    \n      2\n      2014-08-27 11:36:00\n      33\n      Male\n      United States\n      0\n      Yes\n      Yes\n      Rarely\n      No\n      Yes\n      Yes\n      Don't know\n      No\n    \n    \n      3\n      2014-08-27 11:37:00\n      35\n      Male\n      United States\n      0\n      Yes\n      Yes\n      Sometimes\n      No\n      No\n      Yes\n      Very easy\n      Yes\n    \n    \n      4\n      2014-08-27 11:42:00\n      35\n      Male\n      United States\n      0\n      No\n      Yes\n      Rarely\n      Yes\n      Yes\n      Yes\n      Very easy\n      No\n    \n  \n\n\n\n\nNow we see that variable as a series of int-typed 0s and 1s. \n\n\n\n\n\n\nChallenge 2\n\n\n\nYou would like to clean up the scale used in the question about whether one’s mental illness affects one’s work.\n\nFind the column (pandas series) that needs to be transformed and transform its values using replace() with numerical values that make sense.\nDisplay the modified dataframe showing the replaced values.\n\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\nFirst, find the distribution of cases per value label:\n\nm_health['work_interfere'].value_counts()\n\nSometimes    12\nNever         5\nRarely        4\nOften         3\nName: work_interfere, dtype: int64\n\n\nThen show how to use replace to transform the scale into a numeric one:\n\nm_health['work_interfere'].replace(['Never'], '0', inplace=True)\nm_health['work_interfere'].replace(['Rarely'], '1', inplace=True)\nm_health['work_interfere'].replace(['Sometimes'], '2', inplace=True)\nm_health['work_interfere'].replace(['Often'], '3', inplace=True)\nm_health['work_interfere'].value_counts()\n\n2    12\n0     5\n1     4\n3     3\nName: work_interfere, dtype: int64\n\n\nDistribution of values looks good. Let’s take a look at the datatypes for the dataframe object.\n\nm_health.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 24 entries, 0 to 23\nData columns (total 13 columns):\n #   Column                     Non-Null Count  Dtype         \n---  ------                     --------------  -----         \n 0   timestamp                  24 non-null     datetime64[ns]\n 1   age                        24 non-null     int64         \n 2   gender                     24 non-null     object        \n 3   country                    24 non-null     object        \n 4   self_employed              24 non-null     int64         \n 5   family_history             24 non-null     object        \n 6   treatment                  24 non-null     object        \n 7   work_interfere             24 non-null     object        \n 8   remote_work                24 non-null     object        \n 9   tech_company               24 non-null     object        \n 10  benefits                   24 non-null     object        \n 11  leave                      24 non-null     object        \n 12  mental_health_consequence  24 non-null     object        \ndtypes: datetime64[ns](1), int64(2), object(10)\nmemory usage: 2.6+ KB\n\n\nBut it is still typed as an object. So change datatype to int.\n\nm_health['work_interfere'] = m_health['work_interfere'] \\\n        .astype(dtype='int')\nm_health.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 24 entries, 0 to 23\nData columns (total 13 columns):\n #   Column                     Non-Null Count  Dtype         \n---  ------                     --------------  -----         \n 0   timestamp                  24 non-null     datetime64[ns]\n 1   age                        24 non-null     int64         \n 2   gender                     24 non-null     object        \n 3   country                    24 non-null     object        \n 4   self_employed              24 non-null     int64         \n 5   family_history             24 non-null     object        \n 6   treatment                  24 non-null     object        \n 7   work_interfere             24 non-null     int64         \n 8   remote_work                24 non-null     object        \n 9   tech_company               24 non-null     object        \n 10  benefits                   24 non-null     object        \n 11  leave                      24 non-null     object        \n 12  mental_health_consequence  24 non-null     object        \ndtypes: datetime64[ns](1), int64(3), object(9)\nmemory usage: 2.6+ KB\n\n\nFinally, show the dataframe itself.\n\nm_health\n\n\n\n\n\n  \n    \n      \n      timestamp\n      age\n      gender\n      country\n      self_employed\n      family_history\n      treatment\n      work_interfere\n      remote_work\n      tech_company\n      benefits\n      leave\n      mental_health_consequence\n    \n  \n  \n    \n      0\n      2014-08-27 11:35:00\n      46\n      Male\n      United States\n      0\n      No\n      Yes\n      3\n      Yes\n      Yes\n      Yes\n      Don't know\n      Maybe\n    \n    \n      1\n      2014-08-27 11:36:00\n      41\n      Male\n      United States\n      0\n      No\n      Yes\n      0\n      No\n      No\n      Don't know\n      Don't know\n      Maybe\n    \n    \n      2\n      2014-08-27 11:36:00\n      33\n      Male\n      United States\n      0\n      Yes\n      Yes\n      1\n      No\n      Yes\n      Yes\n      Don't know\n      No\n    \n    \n      3\n      2014-08-27 11:37:00\n      35\n      Male\n      United States\n      0\n      Yes\n      Yes\n      2\n      No\n      No\n      Yes\n      Very easy\n      Yes\n    \n    \n      4\n      2014-08-27 11:42:00\n      35\n      Male\n      United States\n      0\n      No\n      Yes\n      1\n      Yes\n      Yes\n      Yes\n      Very easy\n      No\n    \n    \n      5\n      2014-08-27 11:42:00\n      24\n      Male\n      United Kingdom\n      0\n      No\n      Yes\n      2\n      No\n      Yes\n      No\n      Don't know\n      Maybe\n    \n    \n      6\n      2014-08-27 11:42:00\n      35\n      Male\n      United States\n      0\n      No\n      No\n      2\n      Yes\n      Yes\n      Yes\n      Somewhat difficult\n      Yes\n    \n    \n      7\n      2014-08-27 11:43:00\n      27\n      Male\n      Canada\n      0\n      Yes\n      Yes\n      2\n      No\n      Yes\n      No\n      Very difficult\n      Maybe\n    \n    \n      8\n      2014-08-27 11:43:00\n      18\n      Male\n      Netherlands\n      0\n      No\n      No\n      3\n      No\n      Yes\n      No\n      Somewhat difficult\n      Yes\n    \n    \n      9\n      2014-08-27 11:43:00\n      30\n      Male\n      United States\n      0\n      No\n      Yes\n      2\n      No\n      Yes\n      Don't know\n      Don't know\n      No\n    \n    \n      10\n      2014-08-27 11:43:00\n      38\n      Female\n      United States\n      0\n      Yes\n      Yes\n      2\n      No\n      Yes\n      Yes\n      Somewhat easy\n      No\n    \n    \n      11\n      2014-08-27 11:43:00\n      28\n      Male\n      United Kingdom\n      0\n      No\n      No\n      2\n      No\n      Yes\n      Don't know\n      Don't know\n      No\n    \n    \n      12\n      2014-08-27 11:43:00\n      34\n      Male\n      United States\n      0\n      No\n      No\n      2\n      No\n      Yes\n      No\n      Don't know\n      No\n    \n    \n      13\n      2014-08-27 11:43:00\n      26\n      Male\n      Canada\n      1\n      No\n      No\n      2\n      No\n      Yes\n      No\n      Don't know\n      No\n    \n    \n      14\n      2014-08-27 11:44:00\n      30\n      Male\n      United States\n      0\n      Yes\n      Yes\n      1\n      No\n      Yes\n      Yes\n      Don't know\n      Maybe\n    \n    \n      15\n      2014-08-27 12:12:00\n      31\n      Male\n      United States\n      0\n      No\n      No\n      0\n      No\n      Yes\n      Yes\n      Somewhat easy\n      No\n    \n    \n      16\n      2014-08-27 12:13:00\n      40\n      Male\n      United States\n      0\n      No\n      Yes\n      0\n      No\n      Yes\n      No\n      Very difficult\n      No\n    \n    \n      17\n      2014-08-27 12:14:00\n      34\n      Male\n      United States\n      0\n      No\n      No\n      0\n      No\n      Yes\n      Don't know\n      Somewhat easy\n      Maybe\n    \n    \n      18\n      2014-08-27 12:15:00\n      25\n      Female\n      Canada\n      0\n      No\n      Yes\n      2\n      No\n      Yes\n      Yes\n      Don't know\n      Maybe\n    \n    \n      19\n      2014-08-27 12:15:00\n      29\n      Male\n      United States\n      0\n      No\n      No\n      0\n      No\n      Yes\n      Don't know\n      Don't know\n      No\n    \n    \n      20\n      2014-08-27 12:16:00\n      24\n      Male\n      United States\n      0\n      Yes\n      No\n      1\n      No\n      Yes\n      Don't know\n      Somewhat easy\n      Maybe\n    \n    \n      21\n      2014-08-27 12:17:00\n      31\n      Male\n      Mexico\n      0\n      Yes\n      Yes\n      2\n      No\n      Yes\n      Don't know\n      Don't know\n      No\n    \n    \n      22\n      2014-08-27 12:18:00\n      33\n      Male\n      United States\n      0\n      No\n      Yes\n      2\n      No\n      Yes\n      No\n      Somewhat easy\n      Maybe\n    \n    \n      23\n      2014-08-27 12:18:00\n      30\n      Male\n      United States\n      0\n      No\n      Yes\n      3\n      No\n      Yes\n      No\n      Somewhat easy\n      No"
  },
  {
    "objectID": "21_exploring_data_structures.html#exploring-and-understanding-data",
    "href": "21_exploring_data_structures.html#exploring-and-understanding-data",
    "title": "6  Exploring Data Structures",
    "section": "6.1 Exploring and Understanding Data",
    "text": "6.1 Exploring and Understanding Data\nGetting a high level summary of the data is important but data is particularly valuable when refined. Your analysis will start to come alive when we start to do some slicing and dicing and grouping of data or even creating additional variables. In an Excel world, this is like when you use filter options for columns, or create pivot tables, or when you create a formula in a new column to create a new variable. In data science parlance, this is the kind of thing that is referred to as data wrangling - getting the data you want in the form you want it in.\nIn the world of python, this usually means working with a library or package you have already been introduced to called pandas. It lets you do so much!\nThe first dataset we’ll look at is one that looks at data for countries around the world and shows population counts, life expectancy and GDP per capita over a number of years. Let’s import the pandas library and then use the .read_csv() function to get some population by country data. We will read the data into an object that will call country_metrics, so that it is easier for us to remember what it consists of.\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/bcgov/\"\\\n    \"ds-intro-to-python/main/data/gapfinder.csv\"\ncountry_metrics = pd.read_csv(url)\n\nIt’s usually a good idea right away to take a quick look at the data to make sure what we have read in makes sense. The .info() method prints the number of columns, column labels, column data types, memory usage, range index, and the number of cells in each column (non-null values).\n\n\n\n\n\n\nReminder: Naming variables!\n\n\n\nWhen you create a variable or object, you can name it pretty much whatever you want. You will see widespread use of the name “df” on the internet, which stands for “dataframe”. However, as it is good practice to name your data something that relates to the contents. Naming it something intuitive will help reduce chance for confusion as you develop code. Objects you create should be some noun that is at the same time intuitive but not overly verbose. Also remember the form is important too. Remember not to not leave spaces in your names, and to always be consistent in the naming conventions you use (e.g. camelCase, underscore_case, etc.).\n\n\n\ncountry_metrics.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1704 entries, 0 to 1703\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   country    1704 non-null   object \n 1   year       1704 non-null   int64  \n 2   pop        1704 non-null   float64\n 3   continent  1704 non-null   object \n 4   lifeExp    1704 non-null   float64\n 5   gdpPercap  1704 non-null   float64\ndtypes: float64(3), int64(1), object(2)\nmemory usage: 80.0+ KB\n\n\nSo this data is looking good so far. We can see that we have created a pandas dataframe within our python environment. There are 1204 rows of data and six columns or variables. You can see there are only non-null values… so happily there is no missing data to worry about.\nThe .head() pandas method lets us look at actual data in a somewhat similar way to how one does in an Excel spreadsheet.\n\ncountry_metrics.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n  \n\n\n\n\nWhile we don’t learn too too much from this look, but we do see some sample data to see and start getting familiar with visually."
  },
  {
    "objectID": "21_exploring_data_structures.html#selecting-columns-variables",
    "href": "21_exploring_data_structures.html#selecting-columns-variables",
    "title": "6  Exploring Data Structures",
    "section": "6.2 Selecting columns (variables)",
    "text": "6.2 Selecting columns (variables)\nIn the “real world” of data it is not uncommon to see hundreds or even thousands of columns in a single dataframe, so knowing how to pare down our dataset so it is not overly bloated is important.\n\n\n\nSelecting Columns\n\n\n\nFirst things first. If we want to look the contents of a single column, we could do it like this, specifying the column name after the . without parentheses:\n\ncountry_metrics.country\n\n0       Afghanistan\n1       Afghanistan\n2       Afghanistan\n3       Afghanistan\n4       Afghanistan\n           ...     \n1699       Zimbabwe\n1700       Zimbabwe\n1701       Zimbabwe\n1702       Zimbabwe\n1703       Zimbabwe\nName: country, Length: 1704, dtype: object\n\n\nWhen we want to select more than one column to include in our dataframe, we use the convention df[['col1', 'col2']] to select the columns we want.\nIn our example below, we will create a new dataframe called narrow_country_metrics. We are primarily interested in getting the population metric from the original dataset, and since we want to be able to analyze it by year, country, and continent, we will add those into our new dataframe also. So the dataset will be more narrow than the one it was created from.\nThere are two ways to approach this. The first way to approach is to identify all the columns you want to keep.\n\nnarrow_country_metrics = country_metrics[\n    ['country', 'year', 'pop', 'continent']\n    ]\nnarrow_country_metrics.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n    \n  \n\n\n\n\nThe second way is use the pandas .drop() method to eliminate the columns from the original object that you do not want to keep. Thus in the end we can achieve the same result as the example above by simply dropping two variables instead of naming four. With this method, we also need to add the specification axis = 1, which indicates that it is columns (and not rows) being referenced for being dropped.\n\nnarrow_country_metrics = country_metrics.drop(\n    ['lifeExp', 'gdpPercap'], axis=1\n    )\nnarrow_country_metrics.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n    \n  \n\n\n\n\nEither way we have a more manageable dataset to work with called narrow_country_metrics. The original dataframe country_metrics is still there, and it still the same shape as before, we have not changed it.\nHowever, it is important to realize that in python it is very common to change an object by referring to that object on both sides of the assignment operator. In the code below, we narrow the narrow_country_metrics dataset down even further by removing the column pop as well.\n\nnarrow_country_metrics = narrow_country_metrics.drop(['pop'], axis=1)\nnarrow_country_metrics.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      continent\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      Asia\n    \n    \n      1\n      Afghanistan\n      1957\n      Asia\n    \n  \n\n\n\n\nIf we want to have the pop variable back in the narrow_country_metrics object, we will need to recreate it from the source it came from, where pop was still intact.\n\n\n\n\n\n\nReminder on Assignment!\n\n\n\nWhen you create a variable or dataframe object in python, to the left of the = sign you always put the name of the object you want to make or modify. To the right, that’s where you put the content that shows how the existing object is to be modified. It may feel counterintuitive but don’t be concerned, it is the standard way that the code is structured to be understood.\n\n\n\n\n\n\n\n\nChallenge 1\n\n\n\nLet’s say you want to narrow down the dataset to include only the country and the year. How would you do it? Don’t forget to run your code so it also shows a view of the result so you can confirm the code worked as you wanted it to.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nFirst you would like to create a new object to hold this narrowed down version of the dataset, then use the .head() to see some of the resulting data.\n\nchallenge1_df = country_metrics[['country', 'year']]\nchallenge1_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n    \n    \n      1\n      Afghanistan\n      1957"
  },
  {
    "objectID": "21_exploring_data_structures.html#selecting-rows",
    "href": "21_exploring_data_structures.html#selecting-rows",
    "title": "6  Exploring Data Structures",
    "section": "6.3 Selecting rows",
    "text": "6.3 Selecting rows\nOf course, in data analysis we are usually interested in looking at just some rows of data, not all rows all the time.\n\n\n\nSelecting Rows\n\n\n\nRelational operators\nWhen we want to look at just selected rows (i.e. select rows that have certain values within a given column) we can supply a condition that must be met for a given row in that column. To do this, we must use one of the following comparison operators, which are also called relational operators:\n\nequal to ==\nnot equal to !=\nless than <\ngreater than >\nless than or equal to <=\ngreater than or equal to >=\n\nLet’s try an example. The most commonly used relational operator is likely equal to (==). In the example below we have a statement that pandas evaluates line by line in the dataframe as to whether it corresponds to a boolean value of True or False.\n\ncountry_metrics['year'] == 1972\n\n0       False\n1       False\n2       False\n3       False\n4        True\n        ...  \n1699    False\n1700    False\n1701    False\n1702    False\n1703    False\nName: year, Length: 1704, dtype: bool\n\n\nWhen we create an object from this evaluation in pandas, it returns to that object only the rows that evaluate as True:\n\nfiltered_country_metrics = country_metrics[country_metrics['year'] == 1972]\nfiltered_country_metrics.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n    \n      16\n      Albania\n      1972\n      2263554.0\n      Europe\n      67.690\n      3313.422188\n    \n  \n\n\n\n\n Logical operators\nIn python, it is common to use the logical operators and, or, and not to evaluate expressions. For example, as in the code below, python evaluates the and to see whether BOTH operands are true:\n\nx = 5\nprint(x > 3 and x < 10)\n\nTrue\n\n\nHowever, when we ask pandas to evaluate whether a set of logical relations exist within a pandas series object, we must use pandas bitwise logical operators, whose syntax is different:\n\nand &\nor |\nnot ~\n\nWhen combining multiple conditional statements in a pandas series object, each condition must be surrounded by parentheses () within the square brackets []. In the example below, we use an & to indicate that both conditions must be true for a row to be returned:\n\nfiltered_country_metrics = country_metrics[\n    (country_metrics['year'] == 1972) &   \n    (country_metrics['country'] == 'Albania')\n    ]\nfiltered_country_metrics.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      16\n      Albania\n      1972\n      2263554.0\n      Europe\n      67.69\n      3313.422188\n    \n  \n\n\n\n\nIn the example above, there is only one row of data that matches the condition.\nWe can use the | which indicates that if either or both of the conditions are true for a given row, that row is returned into the object:\n\nfiltered_country_metrics = country_metrics[\n    (country_metrics['year'] == 1972) | \n    (country_metrics['country'] == 'Albania')\n    ]\nfiltered_country_metrics.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n    \n      12\n      Albania\n      1952\n      1282697.0\n      Europe\n      55.230\n      1601.056136\n    \n    \n      13\n      Albania\n      1957\n      1476505.0\n      Europe\n      59.280\n      1942.284244\n    \n    \n      14\n      Albania\n      1962\n      1728137.0\n      Europe\n      64.820\n      2312.888958\n    \n    \n      15\n      Albania\n      1967\n      1984060.0\n      Europe\n      66.220\n      2760.196931\n    \n  \n\n\n\n\nIn the example above, there are many rows that satisfy one or the other condition. As an aside, were we to want to know how many rows fulfill this | situation, we could have called the .info() method instead of .head().\n\n\n\n\n\n\nChallenge 2\n\n\n\nYour director has come to you and asked if you know what the life expectancy has been in Canada since 1992. How would you use pandas code to get the data you need? And after having run the code, what’s the answer?\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\nFirst you would like to create a new object to hold this narrowed down version of the dataset, then use some method like .print() or .head() to see some of the resulting data.\n\nchallenge2_df = country_metrics[\n    (country_metrics['year'] >= 1992) & \n    (country_metrics['country'] == 'Canada')\n    ]    \nchallenge2_df.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      248\n      Canada\n      1992\n      28523502.0\n      Americas\n      77.950\n      26342.88426\n    \n    \n      249\n      Canada\n      1997\n      30305843.0\n      Americas\n      78.610\n      28954.92589\n    \n    \n      250\n      Canada\n      2002\n      31902268.0\n      Americas\n      79.770\n      33328.96507\n    \n    \n      251\n      Canada\n      2007\n      33390141.0\n      Americas\n      80.653\n      36319.23501\n    \n  \n\n\n\n\n\n\n\nYou will find that it is very common within python/pandas to see the .iloc() function used, so it is worthwhile to give that a brief introduction also.\nThis function enables the selection of data from a particular row or column, according to the integer based position index as shown in the image below:\n\n\n\nRow and column values iloc() refers to\n\n\n\nBy referring to these locations, we can use iloc() to retrieve the exact cells we want to see. For example, if we just want the row of data corresponding to the index integer value 5, we would run the following code:\n\ncountry_metrics.iloc[5]\n\ncountry      Afghanistan\nyear                1977\npop           14880372.0\ncontinent           Asia\nlifeExp           38.438\ngdpPercap      786.11336\nName: 5, dtype: object\n\n\nWe can also modify the above call to look at column(s)! For example, we specify the row(s) of interest to the left of a comma in the square brackets, and the column(s) of interest to the right of it. In the example below, the code will retrieve just the content found at the intersection of row 5 and column 3:\n\ncountry_metrics.iloc[5,3]\n\n'Asia'\n\n\nWe can also retrieve ranges of rows and columns respectively by employing a colon along with the starting and ending rows or columns we want. In the example below, we want to get rows “0” through “2” and the columns “0” through “3”:\n\ncountry_metrics.iloc[0:2,0:3]\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n    \n  \n\n\n\n\nIt is a flexible tool that can be very helpful as you build and review your code."
  },
  {
    "objectID": "21_exploring_data_structures.html#sorting-rows",
    "href": "21_exploring_data_structures.html#sorting-rows",
    "title": "6  Exploring Data Structures",
    "section": "6.4 Sorting rows",
    "text": "6.4 Sorting rows\nSo now you’ve mastered how to select rows and columns you want, congratulations! Instead of hunting and pecking for insights, one way to quickly make some sense of the data is to sort it - something you probably do in Excel all the time.\nLet’s say we wanted to know more about Asian countries since 2002. First we should go ahead and create a dataframe that consists of just the data we are wanting to look at.\n\ncountries_2000s = country_metrics[\n    (country_metrics['year'] >= 2002) & \n    (country_metrics['continent'] == 'Asia')\n    ]    \ncountries_2000s.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      10\n      Afghanistan\n      2002\n      25268405.0\n      Asia\n      42.129\n      726.734055\n    \n    \n      11\n      Afghanistan\n      2007\n      31889923.0\n      Asia\n      43.828\n      974.580338\n    \n    \n      94\n      Bahrain\n      2002\n      656397.0\n      Asia\n      74.795\n      23403.559270\n    \n    \n      95\n      Bahrain\n      2007\n      708573.0\n      Asia\n      75.635\n      29796.048340\n    \n    \n      106\n      Bangladesh\n      2002\n      135656790.0\n      Asia\n      62.013\n      1136.390430\n    \n  \n\n\n\n\nThis is a nice start. But to see some meaningful insights it is helpful to sort the metric we are interested in something other than the default way the data appear positioned in the dataframe.\nThe .sort_values() method let’s us specify a column (or multiple columns) we want to sort and enter an argument that indicates in which direction we would like it to be sorted, from low to high or vice-versa. We want life expectancy from high to low, so the ascending parameter should be set to False.\n\ncountries_2000s = countries_2000s.sort_values('lifeExp', ascending=False)\ncountries_2000s.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      803\n      Japan\n      2007\n      127467972.0\n      Asia\n      82.603\n      31656.06806\n    \n    \n      671\n      Hong Kong China\n      2007\n      6980412.0\n      Asia\n      82.208\n      39724.97867\n    \n    \n      802\n      Japan\n      2002\n      127065841.0\n      Asia\n      82.000\n      28604.59190\n    \n    \n      670\n      Hong Kong China\n      2002\n      6762476.0\n      Asia\n      81.495\n      30209.01516\n    \n    \n      767\n      Israel\n      2007\n      6426679.0\n      Asia\n      80.745\n      25523.27710\n    \n  \n\n\n\n\nThat was pretty straightforward and it helps us get more insights. Looks like Japan, in 2007, was the Asian country with the highest life expectancy.\nOften one will want to sort by more than one column. To do that, instead of passing in a single column, we pass in a list of the columns we want to sort on. We can also control whether we would like each column to be sorted in ascending or descending order by passing in a respective list of boolean values in the ascending= parameter. In the example below, the code indicates that we want to sort first by ‘country’ in reverse alphabetical order, then by life expectancy from high to low.\n\ncountries_2000s = countries_2000s.sort_values(\n    ['country', 'lifeExp'], ascending=[False, False]\n    )\ncountries_2000s.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      1679\n      Yemen Rep.\n      2007\n      22211743.0\n      Asia\n      62.698\n      2280.769906\n    \n    \n      1678\n      Yemen Rep.\n      2002\n      18701257.0\n      Asia\n      60.308\n      2234.820827\n    \n    \n      1667\n      West Bank and Gaza\n      2007\n      4018332.0\n      Asia\n      73.422\n      3025.349798\n    \n    \n      1666\n      West Bank and Gaza\n      2002\n      3389578.0\n      Asia\n      72.370\n      4515.487575\n    \n    \n      1655\n      Vietnam\n      2007\n      85262356.0\n      Asia\n      74.249\n      2441.576404\n    \n  \n\n\n\n\nWe see that when we created and sorted this new object, the index values are those that were associated with the original country_metrics dataframe.\nDepending on what we want to do with countries_2000s going forward, we might want to create a new “default” index for it instead of the existing one. That’s where the .reset_index() function comes into play.\nIn the code below, we reset the index.\n\ncountries_2000s = countries_2000s.reset_index(drop=False)\ncountries_2000s.head()\n\n\n\n\n\n  \n    \n      \n      index\n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      1679\n      Yemen Rep.\n      2007\n      22211743.0\n      Asia\n      62.698\n      2280.769906\n    \n    \n      1\n      1678\n      Yemen Rep.\n      2002\n      18701257.0\n      Asia\n      60.308\n      2234.820827\n    \n    \n      2\n      1667\n      West Bank and Gaza\n      2007\n      4018332.0\n      Asia\n      73.422\n      3025.349798\n    \n    \n      3\n      1666\n      West Bank and Gaza\n      2002\n      3389578.0\n      Asia\n      72.370\n      4515.487575\n    \n    \n      4\n      1655\n      Vietnam\n      2007\n      85262356.0\n      Asia\n      74.249\n      2441.576404\n    \n  \n\n\n\n\nNotice how the original index now is turned into a column itself called “index”. Depending on the use case, we might want to keep this column in the new dataframe. For example, if we ultimately wanted to join this dataframe back with the original it could act as a key. If we know, however, that it is of no value anymore to us, we can and probably should delete it. To do this, we would have set the drop parameter above to True.\nLet’s use the iloc() method to look at the positional content of this countries_2000s object and confirm that Yeman shows up in row 0 as we expect.\n\ncountries_2000s.iloc[0:4,0:5]\n\n\n\n\n\n  \n    \n      \n      index\n      country\n      year\n      pop\n      continent\n    \n  \n  \n    \n      0\n      1679\n      Yemen Rep.\n      2007\n      22211743.0\n      Asia\n    \n    \n      1\n      1678\n      Yemen Rep.\n      2002\n      18701257.0\n      Asia\n    \n    \n      2\n      1667\n      West Bank and Gaza\n      2007\n      4018332.0\n      Asia\n    \n    \n      3\n      1666\n      West Bank and Gaza\n      2002\n      3389578.0\n      Asia"
  },
  {
    "objectID": "21_exploring_data_structures.html#putting-multiple-methods-together",
    "href": "21_exploring_data_structures.html#putting-multiple-methods-together",
    "title": "6  Exploring Data Structures",
    "section": "6.5 Putting multiple methods together",
    "text": "6.5 Putting multiple methods together\nThe next step is usually to bring several of these commands together to get a nicely refined look at the data. Essentially you will need to invoke a number of calls sequentially to some object, with each one in turn performing some action on it.\nA common approach is to put the object name on each sucessive line of code along with the = operator as well as the modifying code to the right. Imagine we were given the following task:\n\nSelect only the rows where the country is equal to Ireland AND where the year is 1992 or greater\nTake these rows and sort them by year, going from high to low values, top to bottom\n\nThe example below shows how we would employ the line-by-line approach:\n\nchained2_df = country_metrics[(country_metrics['country'] == 'Ireland')]\nchained2_df = chained2_df[(chained2_df['year'] >= 1992)]\nchained2_df = chained2_df.sort_values('year', ascending=False)\nchained2_df.head(3)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      755\n      Ireland\n      2007\n      4109086.0\n      Europe\n      78.885\n      40675.99635\n    \n    \n      754\n      Ireland\n      2002\n      3879155.0\n      Europe\n      77.783\n      34077.04939\n    \n    \n      753\n      Ireland\n      1997\n      3667233.0\n      Europe\n      76.122\n      24521.94713\n    \n  \n\n\n\n\nYou can read fairly clearly what is happening in each line.\nYou can also put multiple methods in the same line of code, as long as you separate each of the elements. In pandas, this is sometimes referred to as joining or chaining, as you are essentially creating a joining/chaining actions together.\nThe code below accomplishes what the more verbose code above does but in a more efficient way:\n\nchained_df = country_metrics[\n    (country_metrics['country'] == 'Ireland') & \n    (country_metrics['year'] >= 1992)\n    ].sort_values('year', ascending = False)\nchained_df.head(3)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      755\n      Ireland\n      2007\n      4109086.0\n      Europe\n      78.885\n      40675.99635\n    \n    \n      754\n      Ireland\n      2002\n      3879155.0\n      Europe\n      77.783\n      34077.04939\n    \n    \n      753\n      Ireland\n      1997\n      3667233.0\n      Europe\n      76.122\n      24521.94713\n    \n  \n\n\n\n\nEach line is executed in sequence, so be careful when constructing code that the order in which you modify your dataframe is what you intend. Take the example below, in which we want to find European values for GDP per capita. In it, we first have a line of code that selects the rows that contain “Europe” in the first line, then looks at the column values for “year”, “country”, and “gdpPercap”.\n\nyear_country_gdp = country_metrics[\n    (country_metrics['continent'] == 'Europe')\n    ]\nyear_country_gdp = year_country_gdp[['year', 'country', 'gdpPercap']]\nyear_country_gdp.head(3)\n\n\n\n\n\n  \n    \n      \n      year\n      country\n      gdpPercap\n    \n  \n  \n    \n      12\n      1952\n      Albania\n      1601.056136\n    \n    \n      13\n      1957\n      Albania\n      1942.284244\n    \n    \n      14\n      1962\n      Albania\n      2312.888958\n    \n  \n\n\n\n\nHad we reversed the lines of code, and selected the “year”, “country”, and “gdpPercap” columns first to put into our new object, we would not have been able to sort for “Europe” in the “continent” column, as the “continent” would have been essentially removed from the dataframe in the step above. So an error would have been thrown.\n\n\n\n\n\n\nChallenge 3\n\n\n\nYour director has come back to you and wondered about whether the life expectancy of people changed during the 1920s in Cambodia. Use what you know about selecting rows and sorting data to get the data you need to answer the question, sorted from most recent at the top.\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nUsing code to select the country Cambodia, then call sort_values() method and chain them together and sorting by year in descending fashion. Looking at the data we see that in the 1920s there was a sharp decline in life expectancy in Cambodia. We also see, thankfully, that it has recovered strongly since then.\n\nchallenge3_df = country_metrics[\n    (country_metrics['country'] == 'Cambodia')\n    ].sort_values('year', ascending = False)\nchallenge3_df.head(10)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      227\n      Cambodia\n      2007\n      14131858.0\n      Asia\n      59.723\n      1713.778686\n    \n    \n      226\n      Cambodia\n      2002\n      12926707.0\n      Asia\n      56.752\n      896.226015\n    \n    \n      225\n      Cambodia\n      1997\n      11782962.0\n      Asia\n      56.534\n      734.285170\n    \n    \n      224\n      Cambodia\n      1992\n      10150094.0\n      Asia\n      55.803\n      682.303175\n    \n    \n      223\n      Cambodia\n      1987\n      8371791.0\n      Asia\n      53.914\n      683.895573\n    \n    \n      222\n      Cambodia\n      1982\n      7272485.0\n      Asia\n      50.957\n      624.475478\n    \n    \n      221\n      Cambodia\n      1977\n      6978607.0\n      Asia\n      31.220\n      524.972183\n    \n    \n      220\n      Cambodia\n      1972\n      7450606.0\n      Asia\n      40.317\n      421.624026\n    \n    \n      219\n      Cambodia\n      1967\n      6960067.0\n      Asia\n      45.415\n      523.432314\n    \n    \n      218\n      Cambodia\n      1962\n      6083619.0\n      Asia\n      43.415\n      496.913648"
  },
  {
    "objectID": "21_exploring_data_structures.html#creating-new-columns-of-data",
    "href": "21_exploring_data_structures.html#creating-new-columns-of-data",
    "title": "6  Exploring Data Structures",
    "section": "6.6 Creating new columns of data",
    "text": "6.6 Creating new columns of data\nVery often we have some data in our dataset that we want to transform to give us additional information. In Excel this is something that is done all the time by creating a formula in a cell that refers to other columns and applies some sort of logic or mathematical expression to it.\nThere are different ways you can go about this in pandas. The most straightforward way is to define a new column on the left of the = and then reference the existing column and whatever additional conditions you would like on the right. In the example below, the existing population variable “pop” is converted to a value that shows population in millions.\n\nnew_cols_df = country_metrics\nnew_cols_df['pop_millions'] = new_cols_df['pop']/1000000\nnew_cols_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n    \n  \n\n\n\n\n Another way is to call the pandas function apply() in which other, more complicated functions can be passed for a given column. This could be a complicated mathematical formula, a function acting on strings or dates, or any other function that cannot be represented by a simple multiplication or division. In the example below, the len() function is applied to each row in the “country” column and a new column with the number of characters for that row is returned:\n\nnew_cols_df['country_name_chars'] = new_cols_df.country.apply(len)\nnew_cols_df.sample(5)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n      country_name_chars\n    \n  \n  \n    \n      67\n      Australia\n      1987\n      16257249.0\n      Oceania\n      76.320\n      21888.889030\n      16.257249\n      9\n    \n    \n      130\n      Benin\n      2002\n      7026113.0\n      Africa\n      54.406\n      1372.877931\n      7.026113\n      5\n    \n    \n      1583\n      Turkey\n      2007\n      71158647.0\n      Europe\n      71.777\n      8458.276384\n      71.158647\n      6\n    \n    \n      26\n      Algeria\n      1962\n      11000948.0\n      Africa\n      48.303\n      2550.816880\n      11.000948\n      7\n    \n    \n      166\n      Botswana\n      2002\n      1630347.0\n      Africa\n      46.634\n      11003.605080\n      1.630347\n      8\n    \n  \n\n\n\n\n There is also a special python function called lambda. It is known as an anonymous function, appears in a single line of code, and is not given a name other than lambda. It can take any number of arguments in an expression.\nThe .assign() method looks at this expression with lambda in it and returns the value it is asked to do and assigns it to the variable name given it. This is how it comes all together to give us actual GDP for each row in our dataframe:\n\nGDP_df = new_cols_df.assign(GDP=lambda x: x['pop'] * x['gdpPercap'])\nGDP_df.head(5)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n      country_name_chars\n      GDP\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n      11\n      6.567086e+09\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n      11\n      7.585449e+09\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n      10.267083\n      11\n      8.758856e+09\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n      11.537966\n      11\n      9.648014e+09\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n      13.079460\n      11\n      9.678553e+09\n    \n  \n\n\n\n\nSo now we can see the newly created column “GDP” and see the value for each country in our new object. Notice that the GDP data is in scientific notation (i.e. the decimal number times x number of zeros), so it’s a bit hard to read. If we wanted readers to consume that data we would go ahead and change the data type for it. But for current purposes we’ll leave that alone."
  },
  {
    "objectID": "21_exploring_data_structures.html#joining-datasets-together",
    "href": "21_exploring_data_structures.html#joining-datasets-together",
    "title": "6  Exploring Data Structures",
    "section": "6.7 Joining datasets together",
    "text": "6.7 Joining datasets together\nOne of the most important tasks in data analysis is to be able to join multiple datasets together. With pandas, there are functions called .merge() and .join() that are similar to each other. As .merge() is perhaps the more used, intuitive, and powerful of the two, we will introduce that method in this tutorial in some depth. There is also a cool function called .concat() that will be introduced below as well.\nBut first, let’s get a second file that gives us country size in square kilometers by country. We will use this data to put together with our country_metrics dataframe.\n\ncountry_size_url = \"https://raw.githubusercontent.com/bcgov/\"\\\n    \"ds-intro-to-python/main/data/countrysize.csv\"\ncountry_size = pd.read_csv(country_size_url, encoding= 'unicode_escape')\ncountry_size.info()\ncountry_size.sample(2)\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 235 entries, 0 to 234\nData columns (total 2 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   nation           235 non-null    object\n 1   area_square_kms  235 non-null    int64 \ndtypes: int64(1), object(1)\nmemory usage: 3.8+ KB\n\n\n\n\n\n\n  \n    \n      \n      nation\n      area_square_kms\n    \n  \n  \n    \n      76\n      Gabon\n      267668\n    \n    \n      3\n      United States\n      9372610\n    \n  \n\n\n\n\nOk, this country_size dataset looks like we expect - a list of countries alongside the land size of that country in square kilometers. There are some omissions in this list - not all countries in our country_metrics dataset are present in this country_size object. But for our present purposes this is ok. We just want to get that square kilometers data into a combined dataset and it is sufficient for that.\nEssentially what we want to do is a classic “left-join” operation of the sort in the diagram below. Conceptually, the country_metrics dataset is like the purple table and the country_size dataset is like the red one.\n\n\n\nTypes of Joins\n\n\n\nThe .merge() method (with reference material available here) works similarly to how table joining works in SQL or how the VLOOKUP function works in Excel. One needs to specify both dataframes, the key variables on which to join them, and the kind of join desired.\nSo let’s look at the example below to see how it all comes together in code.\n\ncombined_df = country_metrics.merge(\n    country_size, \n    left_on='country', \n    right_on='nation', \n    how='left'\n    )\ncombined_df.head(2)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n      country_name_chars\n      nation\n      area_square_kms\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n      11\n      Afghanistan\n      652230.0\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n      11\n      Afghanistan\n      652230.0\n    \n  \n\n\n\n\n When you run the code above you will notice that both the nation key column and the area_square_kms column have been joined together in the new combined_df object. One can keep that nation column in there for control purposes, or it can be removed by using the .drop() method we used earlier:\n\ncombined_df = combined_df.drop('nation', axis=1) \ncombined_df.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n      pop_millions\n      country_name_chars\n      area_square_kms\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n      8.425333\n      11\n      652230.0\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n      9.240934\n      11\n      652230.0\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n      10.267083\n      11\n      652230.0\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n      11.537966\n      11\n      652230.0\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n      13.079460\n      11\n      652230.0\n    \n  \n\n\n\n\nAnother innovative way to put “merge” data together in pandas is with the .concat() function. Conceptually you can think if it like “stacking” two data objects on top of each other or side-by-side as shown in the diagram below.\n\n\n\nWays to stack data\n\n\n\nTo illustrate, let’s fetch two simple dataframes. Each contains the average scores for three subjects by year for two separate schools.\n\nschool1_url  = \"https://raw.githubusercontent.com/bcgov/\"\\\n    \"ds-intro-to-python/main/data/school1.csv\"\nschool2_url  = \"https://raw.githubusercontent.com/bcgov/\"\\\n    \"ds-intro-to-python/main/data/school2.csv\"\nschool1_df = pd.read_csv(school1_url)\nschool2_df = pd.read_csv(school2_url)\n\nLet’s take a quick look at the two dataframes.\n\nschool1_df\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      67\n      88\n      89\n    \n    \n      1\n      English\n      86\n      67\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      75\n    \n  \n\n\n\n\n\nschool2_df\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      32\n      88\n      90\n    \n    \n      1\n      English\n      88\n      44\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      53\n    \n  \n\n\n\n\nWe call .concat() and pass in the objects that we want to stack vertically. This is a similiar operation to union in SQL.\n\nvertical_stack_df = pd.concat([school1_df, school2_df])\nvertical_stack_df\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      67\n      88\n      89\n    \n    \n      1\n      English\n      86\n      67\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      75\n    \n    \n      0\n      Science\n      32\n      88\n      90\n    \n    \n      1\n      English\n      88\n      44\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      53\n    \n  \n\n\n\n\nIt is also possible to stack the data horizontally. Here it is necessary to specify the columnar axis (axis=1) as the default setting is for rows (axis=0).\n\nhorizontal_stack = pd.concat([school1_df, school2_df], axis=1)\nhorizontal_stack\n\n\n\n\n\n  \n    \n      \n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n      Subject\n      Avg Score 2018\n      Avg Score 2019\n      Avg Score 2020\n    \n  \n  \n    \n      0\n      Science\n      67\n      88\n      89\n      Science\n      32\n      88\n      90\n    \n    \n      1\n      English\n      86\n      67\n      77\n      English\n      88\n      44\n      77\n    \n    \n      2\n      Math\n      84\n      73\n      75\n      Math\n      84\n      73\n      53\n    \n  \n\n\n\n\nThis introduction only scratches the surface of how to leverage this way of joining datasets together. But it can be a powerful tool in the toolkit for the right use case. More detail found here."
  },
  {
    "objectID": "21_exploring_data_structures.html#grouping-and-summarizing",
    "href": "21_exploring_data_structures.html#grouping-and-summarizing",
    "title": "6  Exploring Data Structures",
    "section": "6.8 Grouping and summarizing",
    "text": "6.8 Grouping and summarizing\nSometimes of course one would prefer to group rows together for the purpose of summarizing them in various ways.\nIn pandas, we can accomplish this using the .groupby() method. A .groupby() operation involves some combination of splitting the object, applying a function, and combining the results. It is used together with one or more aggregation functions:\n\ncount(): Total number of items\nfirst(), last(): First and last item\nmean(), median(): Mean and median\nmin(), max(): Minimum and maximum\nstd(), var(): Standard deviation and variance\nmad(): Mean absolute deviation\nprod(): Product of all items\nsum(): Sum of all items\n\nFirst let us look at a simple example where we want to get the mean life expectancy for each continent in the data. To do this, we would use the groupby() function to call the appropriate segment (i.e. continent), metric (i.e. lifeExp), and type of aggregation (i.e. mean):\n\nsimple_mean = country_metrics.groupby('continent').lifeExp.mean()\nsimple_mean\n\ncontinent\nAfrica      48.865330\nAmericas    64.658737\nAsia        60.064903\nEurope      71.903686\nOceania     74.326208\nName: lifeExp, dtype: float64\n\n\nFortunately, if we want to look at several aggregations at once, we can do that too. To extend our example above, we would specify the “continent” column in the .groupby() function, then pass additional aggregation functions (in our case, we will add “mean”, “min”, and “max”) as a dictionary within the .agg() function.\n\n\n\n\n\n\nRecall: Dictionaries in Python\n\n\n\nRemember that a dictionary in Python language is a particular type of data structure that contains a collection of key: value pairs. It is analogous to a regular word dictionary you are familiar with which is a collection of words to their meanings. Python dictionaries allow us to associate a value to a unique key, and then to quickly access this value. They are generally created within curly braces {} and have a specified key name and value (e.g. basic form for a python dictionary {\"key1\": \"value1\"})\n\n\n This dictionary takes the column that we are aggregating - in this case life expectancy - as a key and a list of aggregation functions as its value. We also add a line of code that gives each of the columns a new name with the .columns() function.\n\ngrouped_single = country_metrics.groupby('continent') \\\n    .agg({'lifeExp': ['mean', 'min', 'max']})\ngrouped_single.columns = ['lifeExp_mean', 'lifeExp_min', 'lifeExp_max']\ngrouped_single\n\n\n\n\n\n  \n    \n      \n      lifeExp_mean\n      lifeExp_min\n      lifeExp_max\n    \n    \n      continent\n      \n      \n      \n    \n  \n  \n    \n      Africa\n      48.865330\n      23.599\n      76.442\n    \n    \n      Americas\n      64.658737\n      37.579\n      80.653\n    \n    \n      Asia\n      60.064903\n      28.801\n      82.603\n    \n    \n      Europe\n      71.903686\n      43.585\n      81.757\n    \n    \n      Oceania\n      74.326208\n      69.120\n      81.235\n    \n  \n\n\n\n\nIf you look closely at grouped_single you see that the variable “continent” is on a different line than are the columns that are aggregated. That is because continent in this instance is actually an index and not a column.\nWe can nest additional “groups within groups” by creating a list of column names and passing them to the .groupby() function instead of passing a single value.\nThe example below adds more granularity with the introduction of ‘country’ and the creation of a list to hold both ‘continent’ and ‘country’.\n\ngrouped_multiple = country_metrics.groupby(\n    ['continent', 'country']).agg({'pop': ['mean', 'min', 'max']}\n    )\ngrouped_multiple.columns = ['pop_mean', 'pop_min', 'pop_max']\ngrouped_multiple.head(10)\n\n\n\n\n\n  \n    \n      \n      \n      pop_mean\n      pop_min\n      pop_max\n    \n    \n      continent\n      country\n      \n      \n      \n    \n  \n  \n    \n      Africa\n      Algeria\n      1.987541e+07\n      9279525.0\n      33333216.0\n    \n    \n      Angola\n      7.309390e+06\n      4232095.0\n      12420476.0\n    \n    \n      Benin\n      4.017497e+06\n      1738315.0\n      8078314.0\n    \n    \n      Botswana\n      9.711862e+05\n      442308.0\n      1639131.0\n    \n    \n      Burkina Faso\n      7.548677e+06\n      4469979.0\n      14326203.0\n    \n    \n      Burundi\n      4.651608e+06\n      2445618.0\n      8390505.0\n    \n    \n      Cameroon\n      9.816648e+06\n      5009067.0\n      17696293.0\n    \n    \n      Central African Republic\n      2.560963e+06\n      1291695.0\n      4369038.0\n    \n    \n      Chad\n      5.329256e+06\n      2682462.0\n      10238807.0\n    \n    \n      Comoros\n      3.616839e+05\n      153936.0\n      710960.0\n    \n  \n\n\n\n\nAgain will notice that this dataframe looks a little different than some others we have seen in that there are blank spaces below each grouped value of continent in that index. This object is a multi-indexed dataframe. It presents the data nicely to consume visually, but it is not ideal to work with if you want to do further manipulation.\nThis is likely again a good time to use the reset_index() function. This is a step that resets the index to an integer based index and re-creates a non-indexed pandas dataframe.\nThe code block below is identical to the one we just ran except for the line of code that resets the index:\n\ngrouped_multiple = country_metrics.groupby(\n    ['continent', 'country']).agg({'pop': ['mean', 'min', 'max']}\n    )\ngrouped_multiple.columns = ['pop_mean', 'pop_min', 'pop_max']\ngrouped_multiple = grouped_multiple.reset_index()  # resets the index\ngrouped_multiple.head(10)\n\n\n\n\n\n  \n    \n      \n      continent\n      country\n      pop_mean\n      pop_min\n      pop_max\n    \n  \n  \n    \n      0\n      Africa\n      Algeria\n      1.987541e+07\n      9279525.0\n      33333216.0\n    \n    \n      1\n      Africa\n      Angola\n      7.309390e+06\n      4232095.0\n      12420476.0\n    \n    \n      2\n      Africa\n      Benin\n      4.017497e+06\n      1738315.0\n      8078314.0\n    \n    \n      3\n      Africa\n      Botswana\n      9.711862e+05\n      442308.0\n      1639131.0\n    \n    \n      4\n      Africa\n      Burkina Faso\n      7.548677e+06\n      4469979.0\n      14326203.0\n    \n    \n      5\n      Africa\n      Burundi\n      4.651608e+06\n      2445618.0\n      8390505.0\n    \n    \n      6\n      Africa\n      Cameroon\n      9.816648e+06\n      5009067.0\n      17696293.0\n    \n    \n      7\n      Africa\n      Central African Republic\n      2.560963e+06\n      1291695.0\n      4369038.0\n    \n    \n      8\n      Africa\n      Chad\n      5.329256e+06\n      2682462.0\n      10238807.0\n    \n    \n      9\n      Africa\n      Comoros\n      3.616839e+05\n      153936.0\n      710960.0\n    \n  \n\n\n\n\nThis looks more like the structure of the dataframes we know already and will be easier to manipulate further.\n\n\n\n\n\n\nChallenge 4\n\n\n\nYou would like to summarize population as well as life expectancy by year, grouped by continent. Pick some aggregations that would make sense to look at for this task. Don’t worry about re-setting the index or about creating new labels for the result.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nUsing what we learned about how to select rows, we should limit the dataframe to rows where the year is greater than or equal to 1992. Next we should create a multi-column call to the .groupby() function. Finally we should select some aggregations such as mean and max among others could make sense here.\n\nchallenge4_df = country_metrics.groupby(\n    ['continent', 'year']).agg({'pop' : ['mean', 'max'], 'lifeExp' : ['mean', 'max']}\n    )\nchallenge4_df\n\n\n\n\n\n  \n    \n      \n      \n      pop\n      lifeExp\n    \n    \n      \n      \n      mean\n      max\n      mean\n      max\n    \n    \n      continent\n      year\n      \n      \n      \n      \n    \n  \n  \n    \n      Africa\n      1952\n      4.570010e+06\n      3.311910e+07\n      39.135500\n      52.724\n    \n    \n      1957\n      5.093033e+06\n      3.717334e+07\n      41.266346\n      58.089\n    \n    \n      1962\n      5.702247e+06\n      4.187135e+07\n      43.319442\n      60.246\n    \n    \n      1967\n      6.447875e+06\n      4.728775e+07\n      45.334538\n      61.557\n    \n    \n      1972\n      7.305376e+06\n      5.374008e+07\n      47.450942\n      64.274\n    \n    \n      1977\n      8.328097e+06\n      6.220917e+07\n      49.580423\n      67.064\n    \n    \n      1982\n      9.602857e+06\n      7.303938e+07\n      51.592865\n      69.885\n    \n    \n      1987\n      1.105450e+07\n      8.155152e+07\n      53.344788\n      71.913\n    \n    \n      1992\n      1.267464e+07\n      9.336424e+07\n      53.629577\n      73.615\n    \n    \n      1997\n      1.430448e+07\n      1.062078e+08\n      53.598269\n      74.772\n    \n    \n      2002\n      1.603315e+07\n      1.199013e+08\n      53.325231\n      75.744\n    \n    \n      2007\n      1.787576e+07\n      1.350312e+08\n      54.806038\n      76.442\n    \n    \n      Americas\n      1952\n      1.380610e+07\n      1.575530e+08\n      53.279840\n      68.750\n    \n    \n      1957\n      1.547816e+07\n      1.719840e+08\n      55.960280\n      69.960\n    \n    \n      1962\n      1.733081e+07\n      1.865380e+08\n      58.398760\n      71.300\n    \n    \n      1967\n      1.922986e+07\n      1.987120e+08\n      60.410920\n      72.130\n    \n    \n      1972\n      2.117537e+07\n      2.098960e+08\n      62.394920\n      72.880\n    \n    \n      1977\n      2.312271e+07\n      2.202390e+08\n      64.391560\n      74.210\n    \n    \n      1982\n      2.521164e+07\n      2.321878e+08\n      66.228840\n      75.760\n    \n    \n      1987\n      2.731016e+07\n      2.428035e+08\n      68.090720\n      76.860\n    \n    \n      1992\n      2.957096e+07\n      2.568942e+08\n      69.568360\n      77.950\n    \n    \n      1997\n      3.187602e+07\n      2.729118e+08\n      71.150480\n      78.610\n    \n    \n      2002\n      3.399091e+07\n      2.876755e+08\n      72.422040\n      79.770\n    \n    \n      2007\n      3.595485e+07\n      3.011399e+08\n      73.608120\n      80.653\n    \n    \n      Asia\n      1952\n      4.228356e+07\n      5.562635e+08\n      46.314394\n      65.390\n    \n    \n      1957\n      4.735699e+07\n      6.374080e+08\n      49.318544\n      67.840\n    \n    \n      1962\n      5.140476e+07\n      6.657700e+08\n      51.563223\n      69.390\n    \n    \n      1967\n      5.774736e+07\n      7.545500e+08\n      54.663640\n      71.430\n    \n    \n      1972\n      6.518098e+07\n      8.620300e+08\n      57.319269\n      73.420\n    \n    \n      1977\n      7.225799e+07\n      9.434550e+08\n      59.610556\n      75.380\n    \n    \n      1982\n      7.909502e+07\n      1.000281e+09\n      62.617939\n      77.110\n    \n    \n      1987\n      8.700669e+07\n      1.084035e+09\n      64.851182\n      78.670\n    \n    \n      1992\n      9.494825e+07\n      1.164970e+09\n      66.537212\n      79.360\n    \n    \n      1997\n      1.025238e+08\n      1.230075e+09\n      68.020515\n      80.690\n    \n    \n      2002\n      1.091455e+08\n      1.280400e+09\n      69.233879\n      82.000\n    \n    \n      2007\n      1.155138e+08\n      1.318683e+09\n      70.728485\n      82.603\n    \n    \n      Europe\n      1952\n      1.393736e+07\n      6.914595e+07\n      64.408500\n      72.670\n    \n    \n      1957\n      1.459635e+07\n      7.101907e+07\n      66.703067\n      73.470\n    \n    \n      1962\n      1.534517e+07\n      7.373912e+07\n      68.539233\n      73.680\n    \n    \n      1967\n      1.603930e+07\n      7.636845e+07\n      69.737600\n      74.160\n    \n    \n      1972\n      1.668784e+07\n      7.871709e+07\n      70.775033\n      74.720\n    \n    \n      1977\n      1.723882e+07\n      7.816077e+07\n      71.937767\n      76.110\n    \n    \n      1982\n      1.770890e+07\n      7.833527e+07\n      72.806400\n      76.990\n    \n    \n      1987\n      1.810314e+07\n      7.771830e+07\n      73.642167\n      77.410\n    \n    \n      1992\n      1.860476e+07\n      8.059776e+07\n      74.440100\n      78.770\n    \n    \n      1997\n      1.896480e+07\n      8.201107e+07\n      75.505167\n      79.390\n    \n    \n      2002\n      1.927413e+07\n      8.235067e+07\n      76.700600\n      80.620\n    \n    \n      2007\n      1.953662e+07\n      8.240100e+07\n      77.648600\n      81.757\n    \n    \n      Oceania\n      1952\n      5.343003e+06\n      8.691212e+06\n      69.255000\n      69.390\n    \n    \n      1957\n      5.970988e+06\n      9.712569e+06\n      70.295000\n      70.330\n    \n    \n      1962\n      6.641759e+06\n      1.079497e+07\n      71.085000\n      71.240\n    \n    \n      1967\n      7.300207e+06\n      1.187226e+07\n      71.310000\n      71.520\n    \n    \n      1972\n      8.053050e+06\n      1.317700e+07\n      71.910000\n      71.930\n    \n    \n      1977\n      8.619500e+06\n      1.407410e+07\n      72.855000\n      73.490\n    \n    \n      1982\n      9.197425e+06\n      1.518420e+07\n      74.290000\n      74.740\n    \n    \n      1987\n      9.787208e+06\n      1.625725e+07\n      75.320000\n      76.320\n    \n    \n      1992\n      1.045983e+07\n      1.748198e+07\n      76.945000\n      77.560\n    \n    \n      1997\n      1.112072e+07\n      1.856524e+07\n      78.190000\n      78.830\n    \n    \n      2002\n      1.172741e+07\n      1.954679e+07\n      79.740000\n      80.370\n    \n    \n      2007\n      1.227497e+07\n      2.043418e+07\n      80.719500\n      81.235\n    \n  \n\n\n\n\n\n\n\n\nBy now you should be able to select the rows and columns, join together dataframes, and create some basic summarizations of data. The next section will look at some of the more sophisticated and elegant tools for understanding and presenting your data. But the building blocks you have just learned about are the meat and potatoes of data analysis in the python world."
  },
  {
    "objectID": "22_graphical_depictions_of_data.html#introduction-to-matplotlib",
    "href": "22_graphical_depictions_of_data.html#introduction-to-matplotlib",
    "title": "7  Graphical Depictions of Data",
    "section": "7.1 Introduction to matplotlib",
    "text": "7.1 Introduction to matplotlib\nmatplotlib is the most widely used scientific plotting library in Python. For many data related purposes, the sub-library called matplotlib.pyplot is all that is necessary to use. A fantastic feature of the Jupyter environments is that anytime we create a plot, we can view it directly inline with our code, allowing us to make adjustments quickly and easily as we go along.\nWe will typically import this library with the name plt:\n\nimport matplotlib.pyplot as plt\n\nSimple plots are then straightforward to create:\n\nx = [1, 2, 3, 4, 5]\ny = [1, 8, 27, 64, 125]\n\nplt.plot(x, y)\n\n\n\n\n\n\n\n\n\n\nplt.show()\n\n\n\nIn Jupyter environments, running the cell that produces a plot will generate the figure directly below the code, and then the figure is saved with the notebook document for future viewing. However, other Python environments, such as an interactive Python session started from a terminal, or a Python script executed at the command line or within VSCode, require an additional command to display the figure. To do this, we use the basic call:\nplt.show()\nDirectly after creating our plot using plt.plot(...).\nThis command can also be used within a Notebook - for instance to display multiple figures if they are created within a single cell:\n\n# create some lists of data\nx = [1, 2, 3, 4, 5]\ny1 = [2, 4, 6, 8, 10] \ny2 = [1, 4, 9, 16, 25]\n\n# plot and display x vs y1\nplt.plot(x, y1)\nplt.show()\n\n# plot and display x vs y2\nplt.plot(x, y2)\nplt.show()\n\n\n\n\n\n\n\nWe recommend as accepted practice to always include a call to plt.show() if you ever intend to show a plot, in a notebook or otherwise.\n\n\nPlots typically require some set of values to supply to the x value, and an equal length set of values to supply to the y value. If we supply mismatched data, we will get an error:\n\nx = [1, 2, 3, 4, 5]\ny = [1, 8, 27, 64]\nplt.plot(x, y)\n\nValueError: x and y must have same first dimension, but have shapes (5,) and (4,)\n\n\n\n\n\nIn the above examples, we used lists of numbers to supply for our dataset. However, we can also use values directly from pandas dataframes. So let’s work on a new dataset in a pandas dataframe. We will load this dataset from the Seaborn package. Seaborn is another plotting library which we will learn how to use in subsequent sections - but it also has a great built-in dataset that we are going to use to demonstrate matplotlib now, and seaborn later. This dataset looks at species of penguins, and compares various anatomical body part sizes to their species, location and sex.\n\n\n\nload_dataset('penguins')\n\n\n\nfrom seaborn import load_dataset\n\npenguins = load_dataset('penguins')\npenguins.sample(10)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      228\n      Gentoo\n      Biscoe\n      43.3\n      13.4\n      209.0\n      4400.0\n      Female\n    \n    \n      258\n      Gentoo\n      Biscoe\n      44.0\n      13.6\n      208.0\n      4350.0\n      Female\n    \n    \n      328\n      Gentoo\n      Biscoe\n      43.3\n      14.0\n      208.0\n      4575.0\n      Female\n    \n    \n      283\n      Gentoo\n      Biscoe\n      54.3\n      15.7\n      231.0\n      5650.0\n      Male\n    \n    \n      273\n      Gentoo\n      Biscoe\n      50.1\n      15.0\n      225.0\n      5000.0\n      Male\n    \n    \n      264\n      Gentoo\n      Biscoe\n      50.5\n      15.9\n      222.0\n      5550.0\n      Male\n    \n    \n      175\n      Chinstrap\n      Dream\n      50.6\n      19.4\n      193.0\n      3800.0\n      Male\n    \n    \n      322\n      Gentoo\n      Biscoe\n      47.2\n      15.5\n      215.0\n      4975.0\n      Female\n    \n    \n      317\n      Gentoo\n      Biscoe\n      46.9\n      14.6\n      222.0\n      4875.0\n      Female\n    \n    \n      74\n      Adelie\n      Torgersen\n      35.5\n      17.5\n      190.0\n      3700.0\n      Female\n    \n  \n\n\n\n\nLet’s try comparing the length of the penguin bill to the depth of the bill.\n\nplt.plot(penguins['bill_length_mm'], penguins['bill_depth_mm'])\nplt.show()\n\n\n\n\nOkay, that doesn’t look great. That is because the default behaviour of plot is to draw a line plot, which connects all the data points in the order in which they are given. For this type of comparison, a different matplotlib plot may work better. Some of the basic plots include:\n\nplt.plot(x, y) - produces a line plot of x versus y\nplt.scatter(x ,y) - produces a scatter plot of x versus y\nplt.bar(x, height) - produces a bar plot with bars of height ‘height’ positioned at x. Typically reserved for aggregated data!\nplt.hist(x) - produces a simple box histogram for a single column of data\n\nLet’s try to look at the bills again, but this time with a more appropriate comparison plot:\n\nplt.scatter(penguins['bill_length_mm'], penguins['bill_depth_mm'])\nplt.show()\n\n\n\n\nThat’s looking better. But we still need to clean up our plot a little bit. Matplotlib includes methods for adding axis labels, titles, legends and so on. The trick is in figuring out how to apply these methods to the right plot…\n\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.title('Penguin Beaks')\nplt.show()\n\n\n\n\nWhat happened here! Matplotlib drew an axis for us, but neglected to include the data we had provided above. The key to this is in the way that matplotlib interprets the current figure. Anytime we begin a plot method with plt.method()..., matplotlib recognizes this plot instruction to be part of the current figure. Matplotlib will then go through all of the plot instructions one-by-one, adding each individual piece to the current figure. This continues until we hit the plt.show() line, which tells the program we are done adding to this plot, and any new commands should belong to a new plot.\n\n\n\n\n\n\nJupyter vs. Python\n\n\n\nThere is a slight distinction between how Jupyter will handle figures vs. how a Python script wil handle figures. When Jupyter hits the end of a cell or codeblock, it will automatically show the figure at the end of the output, regardless of whether plt.show() was called or not. This is equivalent to calling plt.show() however - a new codeblock will not recognize the code used to create this plot in a new cell.\n\n\nLet’s finally get this all together on one plot:\n\nplt.scatter(penguins['bill_length_mm'], penguins['bill_depth_mm'])\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.title('Penguin Beaks')\nplt.show()\n\n\n\n\nNote that because of this matplotlib behaviour (include every instruction to plt up until it reaches a plt.show() or end of Jupyter cell), we can take advantage to draw multiple plots on a single axis! Some further behaviour includes:\n\nIf a second plot of the same type as the first is created within the same figure, matplotlib will automatically assign it a new colour\nTo distinguish between plots, the argument label = 'plot name' can be provided during creation of the plot. A final call to plt.legend() after creating all plots will produce a legend that consists of the given labels.\n\n\nChallenge 1\n\n\n\n\n\n\nChallenge 1\n\n\n\nUsing what we have learned so far about pandas filtering and matplotlib functionality to produce a plot of bill lengths vs. depths, but colours the three penguin species (Adelie, Chinstrap, and Gentoo) different colours. Include a legend that identifies which species is which colour!\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nWe first split up our data into three separate pieces, each filtered to a single species of penguin. Next we plot all three species without displaying the figure. Finally, we add some tidy up code to add the legend and axis titles, and we are good to go!\nOne thing to note is that it is often good practice to use more than just colour to distinguish between different groups of data. Because of this, adding a variation on the marker style is a good idea as well. We have included that in the solution to this challenge. Be sure to explore all the other style settings available within each matplotlib plot!\n\nadelie = penguins[penguins.species == 'Adelie']\nchinstrap = penguins[penguins.species == 'Chinstrap']\ngentoo = penguins[penguins.species == 'Gentoo']\n\n# start creating our plot, individualizing each plot as necessary\nplt.scatter(adelie.bill_length_mm, adelie.bill_depth_mm, marker='o', label='Adelie')\nplt.scatter(chinstrap.bill_length_mm, chinstrap.bill_depth_mm, marker='x', label='Chinstrap')\nplt.scatter(gentoo.bill_length_mm, gentoo.bill_depth_mm, marker='^', label='Gentoo')\n\n# apply overall plot commands after\nplt.xlabel('Bill Length (mm)')\nplt.ylabel('Bill Depth (mm)')\nplt.title('Penguin Beaks')\nplt.legend()\n\n# show our plot\nplt.show()"
  },
  {
    "objectID": "22_graphical_depictions_of_data.html#plotting-directly-from-pandas",
    "href": "22_graphical_depictions_of_data.html#plotting-directly-from-pandas",
    "title": "7  Graphical Depictions of Data",
    "section": "7.2 Plotting Directly From Pandas",
    "text": "7.2 Plotting Directly From Pandas\nThat was a lot of work to make a relatively straightforward plot. While matplotlib is extremely versatile, this same versatility comes at the expense of having to tweak the underlying code frequently to make the plots look the way that we want. Luckily, there exist other libraries that have been built on top of matplotlib that take care of much of the abstraction, so we can get directly to looking at our results as fast as possible!\nOne extremely straightforward way to plot is directly from pandas, which implicitly uses matplotlib.pyplot in the background.\n\npenguins.plot.scatter(x = 'bill_length_mm', y = 'bill_depth_mm')\nplt.show()\n\n\n\n\nFor some plot types, we are also given the ability to provide a grouping category.\n\npenguins.plot.hist(column='flipper_length_mm', by='sex')\nplt.show()\n\n\n\n\nWhile pandas provides a straightforward way to quickly look at the data, it is fairly limited, and we will still end up needing to default back to matplotlib methods to get high-quality plots. The next package will help with this!"
  },
  {
    "objectID": "22_graphical_depictions_of_data.html#introduction-to-seaborn",
    "href": "22_graphical_depictions_of_data.html#introduction-to-seaborn",
    "title": "7  Graphical Depictions of Data",
    "section": "7.3 Introduction to Seaborn",
    "text": "7.3 Introduction to Seaborn\nSeaborn is a library for making statistical graphics in Python. It builds on top of matplotlib but is uniquely built to integrate closely with pandas data structures. Seaborn is dataset-oriented, and was built specifically to let you focus on what the different elements of a plot mean, rather than on the details of how to draw it. Leveraging seaborn allows us to build high quality graphics in a minimal amount of code. This not only allows us to spend more time analysing outputs and results, but also makes the code more transparent and easier for others to follow and understand.\nSeaborn is usually imported with the shorthand sns. On top of this, it is often nice to reset the default matplotlib colour theme and layout to the default seaborn layout, which has more subdued colours and tones. This can be done globally via the .set_theme() method. Changing the theme will apply to any other plots created in a notebook/script, even if they are created specifically by matplotlib. More options for the theme or style (which can also be set via .set_style()) can be found in the Seaborn aesthetics tutorial.\n\nimport seaborn as sns\nsns.set_theme()\n\nWithin seaborn, we will focus on three main plot types:\n\nRelational plots\nDistributions\nCategorical plots\n\nAll three of these modules have a higher figure-level interface (relplot, displot, catplot) that have options to produce the different subvarieties. They also have an axes-level interface (for example, histplot inside the displot category) that allows for more control over the matplotlib backend being used to produce the plot. For our purposes, we will stick to the high level interface as it is capable of producing beautiful plots with a few simple commands.\n\n\n\nSeaborn plot categories.\n\n\n\n\n\n\n\n\nAdvanced Tip - Figure vs. Axes Level?\n\n\n\n\n\n\n\n\nMatplotlib Figures\n\n\nYou will notice that I mentioned figure vs. axes level interfaces for seaborn. This is a small but subtle distinction that is worth explaining in more detail. When we introduced matplotlib, we explained that you can create multiple different plots all on the same figure. This is because matplotlib plots are individually drawn onto a common axis, but each individual plot does not own the axis, titles, legends, and so on. Matplotlib produces a base axis, then draws each individual plot on top, and then adds the extra pieces as requested. All the accompanying fluff (title, legend) that surrounds the axis is part of the overall figure, which we set with commands such as plt.xlabel(), plt.title() and so on.\nSo a figure can be thought of as the overall container holding each plot and accompanying axis labels. In seaborn, the figure-level interface we will be using is just this - figure-level. It is a ‘finished product.’ Producing a plot via the relplot, displot, and catplot methods will produce an entire figure, complete with titles, legends, and anything else we might specify, such as the figure size. However, this entire figure is no longer easily accessible like a matplotlib axis is (we cannot add a plot like this to a different set of matplotlib plots, for example).\nThis can be useful as it will simplify the amount of code needed to create a report-ready figure. However, the drawback is that it reduces the customizability we have to work with. So if we wish to work with customizable axes that we can quickly drop into more complex matplotlib plots, we should use the axes-level interface (eg. using sns.scatterplot() instead of sns.relplot()). If we want to produce standard statistical plots, the figure-level interface is the recommended tool as they produce cleaner plots.\nCheck out the Seaborn tutorial for more details on the differences between these methods!\n\n\n\nBefore we dive into the particulars, here’s a wonderful one line command to show just how powerful seaborn can be!\n\nsns.pairplot(penguins, hue='species', height=1.75)\nplt.show()\n\n\n\n\n\nRelational Plots\n\n \n\nCorrelation \\(\\neq\\) Causation\n\n\n\nRelational plots let us quickly look for patterns in our dataset between two different features. The most common approach is to use scatter or lineplots, depending on the type of data we have. A timeseries, for example, will often be displayed with a lineplot, while any two numerical features can be plotted against each other with a scatterplot.\nWhen we use the high-level relplot method, we need to supply our pandas dataframe, as well as which columns we are interested in plotting. The default plot type is to use a scatterplot:\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm')\nplt.show()\n\n\n\n\nHowever, we can force the line plot by including the optional kind argument:\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', kind='line')\nplt.show()\n\n\n\n\nWhen we utilize the lineplot option, Seaborn will default to automatically aggregating data along the x-axis, displaying uncertainty in the y-axis with confidence intervals. Note that it also doesn’t care about the initial order of the data. Seaborn automatically assumes that we wanted a nicely flowing line from left to right, and will sort the x values accordingly.\n\nChallenge 2\n\n\n\n\n\n\nChallenge 2\n\n\n\nUsing the Seaborn load_dataset method, import a dataset called ‘dowjones’. Answer the following questions:\n\nWhat does this dataset include/represent?\nOn what date does the data represented here reach its highest price?\nCreate a line plot to depict the data. Does the plot agree with the answer you came up with in part 2?\n\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\n\nLet’s get our dataset first and take a quick peek\n\n\ndowjones = load_dataset('dowjones')\ndowjones.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n    \n  \n  \n    \n      0\n      1914-12-01\n      55.00\n    \n    \n      1\n      1915-01-01\n      56.55\n    \n    \n      2\n      1915-02-01\n      56.00\n    \n    \n      3\n      1915-03-01\n      58.30\n    \n    \n      4\n      1915-04-01\n      66.45\n    \n  \n\n\n\n\nIt looks like this is a list of dates and prices. Context clues from the name of the dataset hint that this is probably the Dow Jones Industrial Average. From the looks of the Date column, it is probably a monthly average price!\n\nWe could do this one of two ways. We could sort the dataframe by price and then look at the first data in the sorted dataframe. Alternatively, we can use aggregation and filtering. Note that because we do not want to get statistics per varying group, we do not need to do any groupby() before looking for the max value.\n\n\n# option 1: sort and grab first row\ndate_of_max = dowjones.sort_values(by='Price', ascending=False).head(1)\ndisplay(date_of_max)\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n    \n  \n  \n    \n      613\n      1966-01-01\n      985.93\n    \n  \n\n\n\n\n\n# option 2: aggregate and filter \n# determine the max price in the dataset\nmax_price = dowjones['Price'].max()\n\n# use this max price to filter to the date it occured on\ndate_of_max = dowjones[dowjones['Price']==max_price]\n\n# check out the result \ndisplay(date_of_max)\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n    \n  \n  \n    \n      613\n      1966-01-01\n      985.93\n    \n  \n\n\n\n\n\nWe will use Seaborn to build our lineplot.\n\n\nimport datetime as dt\nsns.relplot(\n    data=dowjones,\n    x='Date',\n    y='Price',\n    kind='line'\n)\nplt.show()\n\n\n\n\nIt does indeed look like the plot is peaking around 1966, which matches what we found earlier! It also looks like you can see the stock market crash that started the Great Depression…\n\n\n\nAlthough relational plots are two dimensional in their presentation, we can add a third dimension to the data by applying the hue, size, and style arguments to secondary columns.\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', hue='species')\nplt.show()\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', style='sex')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWhen used on a non-categorical column, the hue and size arguments will provide sequential coloring and sizing:\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', hue='bill_length_mm')\nplt.show()\n\nsns.relplot(data=penguins, x='body_mass_g', y='flipper_length_mm', size='bill_length_mm')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe can also mix and match multiple different styling choices. Note that here, we want to be cautious about creating our plots using too many different styling choices. While informative, they can be tricky to interpret if we use too many all at once.\n\nsns.relplot(\n    data=penguins, x='body_mass_g', y='flipper_length_mm', \n    hue='species', style='species'\n    )\nplt.show()\n\nsns.relplot(\n    data=penguins, x='body_mass_g', y='flipper_length_mm', \n    hue='sex', size='bill_length_mm'\n    )\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReminder: Use Multiple Lines!\n\n\n\nIn that last example you’ll notice that I broke the code up for the plot onto multiple lines. When you start to have many arguments being supplied to a function, it is often a good idea to put different arguments on different lines for readability. As long as argument stays with the brackets, you can use as many lines or indentations as you want!\n\n\nIf we want to avoid using too many different styling choices, but we would still like to understand how our features vary across different categories, we can make multiple plots using the built in row and col arguments of the relplot objects. By supplying a category to row (col), Seaborn is told to split the dataset into all the different values in the category. Then each subset of the data is plotted on a different row (column) of the data. The end result is a grid of plots, with each plot having a unique subset of the category represented.\n\ng = sns.relplot(\n    data=penguins, x='body_mass_g', y='flipper_length_mm',\n    row='island', col='species', hue='sex',\n    height=2.5\n)\ng.set_titles(size=8)  # include this line because the titles will overlap otherwise!\nplt.show()\n\n\n\n\nFinally, a frequent goal of creating a scatter plot is to identify if there is a trend in the data. Seaborn offers an extra method, lmplot, to allow for quick viewing of best fit lines (including confidence interval estimates) in our datasets:\n\nsns.lmplot(data=penguins, x='body_mass_g', y='flipper_length_mm', hue='sex')\nplt.show()\n\n\n\n\nNote that because we included a third categorical dimension (hue='sex'), we have actually displayed two different best fit lines: the best relationship between body mass and flipper length for male penguins, and the best relationship for female penguins as well.\nThis method is capable of doing more than the default linear regression! Some such arguments available include:\n\nlogistic: boolean. If True, will estimate a logistic regression.\nrobust: boolean. If True, will de-weight far outliers in performing the regression.\norder: integer. If supplied, will fit a polynomial regression with the given order.\n\n\n\nSummary\nThat is a lot to take in. To summarize the relational plots available in Seaborn:\n\nrelplot is the go-to tool to create a statistical comparison plot between two numerical columns\nThese plots default to scatter, but can be made into lineplots with kind='line'\nstyle, hue, and size can all be adjusted to provide insight into both numerical and categorical features\nrow and col will split our plot out into multiple facets, according to the categories found in the datasets\nlmplot can be used to provide basic regression fits to the data\n\n\n\nChallenge 3\n\n\n\n\n\n\nChallenge 3\n\n\n\nChoose any two numerical features in the penguins dataset. Produce a scatter plot that highlights the differences between species. Separate the results into two plots - one for male and one for female penguins.\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\nYou might produce code that looks similar to this. col and row could be interchanged, and maybe you chose to use just one of style or hue. Explore your options!\n\ng = sns.relplot(\n    data=penguins, x='flipper_length_mm', y='bill_length_mm',\n    hue='species', style='species', row='sex'\n)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDistribution Plots\n\n\n\nSpoooky\n\n\nAnother early step that we often take when analyzing our data is to understand how the features are distributed. We can quickly answer questions such as:\n\nWhat is the range of observations in the dataset?\nIs the data skewed?\nDo we have outliers?\nHow do different subsets compare?\n\nUsing the displot method, we can access multiple different styles of plots to answer these questions. While Seaborn can (and does) get into multivariate distributions, let us stick to univariate distributions: histograms and kernel density estimation (KDE).\nThe default displot option will produce a histogram of whichever column we choose in our dataset:\n\nsns.displot(data=penguins, x='flipper_length_mm')\nplt.show()\n\n\n\n\nSeaborn automatically chooses what it feels is a reasonable number of bins for the dataset, but this is of course customizable via either the binwidth or the bins arguments, which will force the width or number of bins, respectively.\n\nsns.displot(data=penguins, x='flipper_length_mm', binwidth=1)\nplt.show()\n\nsns.displot(data=penguins, x='flipper_length_mm', bins=5)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nJust like with our relational plots, we can also condition our histograms on other features, colouring each member of the category separately.\n\nsns.displot(data=penguins, x='flipper_length_mm', hue='sex')\nplt.show()\n\n\n\n\nThe default here is to create an overlapping histogram, but we could create stacked histograms, dodged (side-by-side) histograms, or even use the col or row options to produce multiple plots!\n\nsns.displot(data=penguins, x='flipper_length_mm', hue='sex', multiple='stack')\nplt.show()\n\nsns.displot(data=penguins, x='flipper_length_mm', hue='sex', multiple='dodge')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nsns.displot(data=penguins, x='flipper_length_mm', col='sex', height=3.5)\nplt.show()\n\n\n\n\nWhile the histogram option provides the exact count of the underlying values contained in our dataset, sometimes we may wish to approximate the distribution of data. This is done using kernel density estimation, which plots a smooth, continuous density estimate. Just like with the relational plots, we can access this new plot type using the kind='kde' option.\n\nsns.displot(data=penguins, x='flipper_length_mm', kind='kde')\nplt.show()\n\n\n\n\nJust like we were able to adjust the bin sizes for the histogram, we can adjust the ‘bandwidth’ of our estimation. This will vary the amount of smoothing that is applied in the end distribution:\n\nsns.displot(data=penguins, x='flipper_length_mm', kind='kde', bw_adjust=0.25)\nplt.show()\n\nsns.displot(data=penguins, x='flipper_length_mm', kind='kde', bw_adjust=4)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nFinally, we can have both types of plots in action at the same time by including the optional kde argument:\n\nsns.displot(data=penguins, x='flipper_length_mm', kde=True, hue='species')\nplt.show()\n\n\n\n\n\nSummary\nTo summarize distributions available in Seaborn:\n\ndisplot is the go-to tool to create a statistical comparison plot between two numerical columns\nThese plots default to histograms, but other options are available:\n\nkde will produce an estimation of the underlying density\necdf will produce a cumulative distribution function\n\nA shortcut to include both a histogram and a KDE can be used by setting the argument kde=True\nhue can be adjusted to provide insight into extra categorical features\nrow and col will split our plot out into multiple facets, according to the categories found in the datasets\n\n\n\nChallenge 4\n\n\n\n\n\n\nChallenge 4\n\n\n\nCreate a KDE plot of body mass. Does it appear to be bi-modal (2 peaks)? If so, create another plot (or more) to identify what may be the cause. Are male penguins heavier? Is it a specific species? Does the island matter?\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nFirst let’s just look at the body mass variable alone.\n\nsns.displot(\n    data=penguins, x='body_mass_g', kind='kde'\n    )\nplt.show()\n\n\n\n\nHmm.. it’s not immediately obvious that there might be two peaks here, but it does seem to skew a little bit to lower values. Let’s see if looking at the different sexes gives us any more information:\n\nsns.displot(\n    data=penguins, x='body_mass_g', kind='kde', hue='sex',\n    fill=True  # Use fill to fill the KDE plot!\n    )\nplt.show()\n\n\n\n\nOkay, weird. Splitting by sex does start to show two peaks, but it shows up in both male and female! So maybe it’s the species where the peak is showing up?\n\nsns.displot(\n    data=penguins, x='body_mass_g', kind='kde', hue='species', col='sex',\n    fill=True \n    )\nplt.show()\n\n\n\n\nAha - looks like we found a key difference here! While males tend to be heavier than the females on average, there are distinctly different distributions for the three species, with the Gentoo penguins being heavier than both the Adelie and Chinstrap. Doing this sort of exploration via plot is always a useful tool to learn more about our data!\n\n\n\n\n\n\nCategorical Plots\n\n\n\nSuspicious…\n\n\nRelational plots let us quickly view relationships between two sets of numerical features. Categorical plots will allow us to do the same where one (or both) of the features is categorical (divided into discrete groups).\nThe catplot method will provide us a unified high-level interface to a variety of different plots. The default behaviour of catplot is to produce a non-aggregated view of the categories in a strip plot.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm')\nplt.show()\n\n\n\n\nThis is a categorical version of a scatter plot, in which all of the data for a single category is plotted at the same horizontal position, but with a small amount of horizontal ‘jitter’ added so that data points with identical numerical values do not completely overlap. This gives us a general idea of how the data is distributed within a category: do most of the penguins have similar flipper lengths within a species, or do they spread out to cover a wide range?\nThe most commonly used categorical plot however, is the bar plot. Bar plots are something that we are likely all familiar with, as human beings living in a society of.. people. However, typically when you see a bar plot, it is representing some sort of aggregation of our data. What is the average flipper length of the various penguin species? How many penguins live on each island? As the data stands, we do not have this information directly - it is non-aggregated, row level data. We could use our skills with Pandas to group the data according to the categories of interest and apply some statistic measures to the numerical columns of interest, and then use a matplotlib or Seaborn plot to display the results of that aggregation. However, Seaborn has developed tools that allow us to skip the grouping steps ourselves and allow the plotting package to do the grouping and statistical analysis behind the scenes. This has two advantages:\n\nThe code is shorter and more precise, leading to easier understanding for others. This helps with code transparency.\nSeaborn also provides confidence intervals to include in its aggregated bar plots, which would be an extra level of complexity added to a manual grouping of the data.\n\nIn Seaborn, a barplot operates on the full dataset to obtain an estimate of some aggregation function (which is the mean by default). It will also default to providing an estimation of the confidence interval on its estimates. Just like with all of the other Seaborn plots, we can add an additional feature for free via the hue styling choice.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex')\nplt.show()\n\n\n\n\nIf we wish to summarize a different aggregate statistic, there are built in options: estimator='mean', 'median','min', and 'max' will all work.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex', estimator='max')\nplt.show()\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex', estimator='min')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Tip - Callable Estimators!\n\n\n\n\n\nIn addition to the built in estimators, we can also use estimators for external functions. The function is required to take as an input a vector of values, and output a single value that summarizes that vector. The Python library numpy is the go-to resource for mathematical functions in Python. If we use numpy, we can send any sort of estimator to the barplot! Common statistical functions in numpy include sum, prod, mean, std, and var.\n\nimport numpy as np\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex', estimator=np.sum)\nplt.show()\n\n\n\n\n\n\n\nOne last unique piece that you might be interested in is the orientation of the plot. In the above examples, we gave our categorical category to the x feature, and y held the numerical values. If we wish to have a horizontal set of bars instead, we can simply swap these:\n\nsns.catplot(\n    data=penguins, x='flipper_length_mm', y='species',\n    kind='bar', hue='sex')\nplt.show()\n\n\n\n\n\nSummary\nTo summarize the categorical plots available in Seaborn:\n\ncatplot is the go-to tool to create a statistical comparison plot between a numerical and categorical column\nThese plots default to categorical swarmplots (no, I do not know why bar is not the default) but other options are available. These options can be organized into three broad subcategories.\n\nCategorical scatterplots:\n\nswarm\nstrip\n\nCategorical distribution plots:\n\nbox\nviolin\nboxen\n\nCategorical estimate plots:\n\npoint\nbar\ncount\n\n\nhue can be adjusted to provide insight into extra categorical features\nrow and col will split our plot out into multiple facets, according to the categories found in the datasets\nBy swapping the x and y variables, we can choose which orientation the plot will display in\n\n\n\nChallenge 5\n\n\n\n\n\n\nChallenge 5\n\n\n\nWhich penguin species was most frequently included in this dataset? Display your result visually!\n\n\n\n\n\n\n\n\nSolution to Challenge 5\n\n\n\n\n\nTo get the count for each of the species, we need to make a countplot:\n\nsns.catplot(data=penguins, x='species', kind='count')\nplt.show()\n\n\n\n\nDone, in one line of code!\nAs a bonus, if we want to plot the data in descending order, we can pass a list of strings containing the categories to the catplot method:\n\nsns.catplot(\n    data=penguins, x='species', \n    kind='count', \n    order=['Adelie', 'Gentoo', 'Chinstrap']\n    )\nplt.show()"
  },
  {
    "objectID": "22_graphical_depictions_of_data.html#saving-your-plots",
    "href": "22_graphical_depictions_of_data.html#saving-your-plots",
    "title": "7  Graphical Depictions of Data",
    "section": "7.4 Saving your Plots",
    "text": "7.4 Saving your Plots\nWe’ve now done all this work to make amazing incredible charts that produce stunning visuals for our datasets. Chances are, however that you do not want these images to live solely in your Jupyter notebook. You might want it for a presentation or a report. So, we need to save these images so we can access them outside of our Python environment. To do this we use the plt.savefig() method. This method will save the currently open figure. This is the figure that has not yet been shown on screen! Any call to save a plot must come before we display it via plt.show(), as this actually closes the figure/plotting device so we can start our next one!\n\n\n\n\n\n\nNo Show and Tell!\n\n\n\nThis point is important enough that I am repeating it here. Always save your figure before showing it!\n\n\nLet’s create a plot, view it, and save it. We will save it to the same outputs folder we used in a prior section.\n\nsns.catplot(\n    data=penguins, x='species', y='flipper_length_mm', \n    kind='bar', hue='sex'\n    )\nplt.savefig('../outputs/penguins.png', bbox_inches='tight')\nplt.show()\n\n\n\n\nYou will notice that I used an optional argument here called bbox_inches. This is entirely optional, but when used it will cut out any empty whitespace on the sides of the figure before saving. I find it makes for cleaner looking images at the end, so I will often use this argument. And that’s it, we now have our image saved and ready for use elsewhere!\n\n\n\n\n\n\nAdvanced Tip - Changing the Figure Size\n\n\n\n\n\nWe might want to vary the shape or size of the plot we are producing. There are two ways to do this, depending on the type of figure we have created.\n\nAxes-Level (Matplotlib Style) Figures\nIf we are working directly with matplotlib or the axes-level seaborn functions, this is done by initializing the figure with a figsize command before creating any plot elements:\nplt.figure(figsize=(10, 8))\nplt.plot(...)\nsns.scatterplot(...)\nplt.savefig()\nplt.show()\nThe figsize argument inside the plt.figure method specifies the required width and height in inches.\n\n\nFigure-Level (Seaborn Style) Figures\nIf we are working with the figure-level Seaborn interface, we cannot access the figure attributes externally. However, Seaborn has given us arguments we can pass directly to the plot method itself to set the figure size via the height and aspect arguments.\nsns.relplot(data=..., height=5, aspect=0.5)\nplt.savefig()\nplt.show()\nThe height argument indicates the required height of each subplot in inches, while the aspect argument supplies the aspect ratio such that the width = aspect * height. Note that height applies to every individual subplot in the plot, not the overall image!"
  },
  {
    "objectID": "22_graphical_depictions_of_data.html#online-galleries",
    "href": "22_graphical_depictions_of_data.html#online-galleries",
    "title": "7  Graphical Depictions of Data",
    "section": "7.5 Online Galleries",
    "text": "7.5 Online Galleries\nAll of the tools described above are a great way to start visualizing your data, but there are many other libraries and tools available. If you ever need inspiration for a new visual, or simply need a reminder on what code you would need to create a specific chart, the Python Graph Gallery is a fantastic resource. It contains examples as well as code for over 40 different types of charts using Python code."
  },
  {
    "objectID": "30_bcdata.html#installing",
    "href": "30_bcdata.html#installing",
    "title": "8  Introduction to bcdata",
    "section": "8.1 Installing",
    "text": "8.1 Installing\nUnlike the rest of the packages we have downloaded (using conda commands), the bcdata package is not available for download via conda (this is often the case for smaller packages that are not widely distributed globally). This is one of those instances where we need to mix our use of pip and conda. However, the installation process proceeds in the same fashion as before. These commands can be done from an anaconda prompt, or from the command prompt (cmd) utility inside VS Code.\n\n\nAnaconda Prompt\n\n> conda activate ds-env\n> pip install bcdata\n\nAs before, make sure that you are installing this package in the ds-env environment! To make sure that this has successfully downloaded, we can try importing it into our Python session.\n\nimport bcdata\nbcdata.__version__\n\n'0.7.4'"
  },
  {
    "objectID": "30_bcdata.html#basic-usage",
    "href": "30_bcdata.html#basic-usage",
    "title": "8  Introduction to bcdata",
    "section": "8.2 Basic Usage",
    "text": "8.2 Basic Usage\n\nIntroduction to the Catalogue\nTo use the bcdata package, we will need to familiarize ourselves with the B.C. Data Catalogue. If we enter a search term in the catalogue, it will bring up a variety of data sets that might meet our needs. However, the python bcdata package is only for geospatial datsets. These will typically be data types with a resource storage format (one of the filter options) that matches:\n\narcgis_rest\nkml\ngeojson\nmultiple\nwms\n\nIf we filter to only include these options for the format, we will find some geospatial datasets. Let’s try looking for a list of the locations of all B.C. hospitals.\n\n\n\nB.C. Hospitals\n\n\nThe first search result here, BC Health Care Facilities (Hospital), looks like it might be what we are looking for. If we click on it, we see that there are options for geographic downloads on the right hand side of the page. This is good - it means that we have found a geographic dataset!\n\n\n\nGeographic Download Options\n\n\nIf we now click on the View button beside the BC Geographic Warehouse Custom Download resource, we will get some more information about the dataset. This includes details such as how frequently the dataset is refreshed, when it was last refreshed, the type of geometry used, the details of the columns included in the data and more.\nAt this point in a ‘pre-programming’ world, we would have to request to access/download the dataset, wait for the file to arrive, store it locally in a folder we hopefully don’t misplace, and then find some tool capable of viewing the data. While each of these steps individually is not terribly difficult, putting them altogther, and documenting the entire process can be prohibitive and mistake prone. What if we forget where we stored the file? Or forget which file we were using when we need to refresh the dataset? When datasets are acquired manually like this, having a reproducible workflow becomes so much more difficult!\nIn a python world, all of these steps can be done directly within a single script! This essentially guarantees the reproducibility and transparency of the workflow.\n\n\nGetting the Data into Python\nOkay, we have a geospatial dataset in mind, now what? First, we will call on the bcdata package to import our chosen dataset into python. To do this, we need to use the get_data() method, and use 2 arguments:\n\ndataset: this is the name of the dataset we wish to pull from the catalog. This can be either the id or Object Name:\n\nid: last portion of the URL - in this case ‘bc-health-care-facilities-hospital’\nObject Name: under Object Description - in this case ‘WHSE_IMAGERY_AND_BASE_MAPS.GSR_HOSPITALS_SVW’\n\nas_gdf: a boolean argument that specifies to load the data into a geopandas DataFrame (a specialized pandas dataframe capable of holding geospatial data)\n\nLet’s go ahead and use these arguments to pull the dataset into Python!\n\nhospitals = bcdata.get_data(\n    dataset='bc-health-care-facilities-hospital', \n    as_gdf=True\n    )\nhospitals.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      CUSTODIAN_ORG_DESCRIPTION\n      BUSINESS_CATEGORY_CLASS\n      BUSINESS_CATEGORY_DESCRIPTION\n      OCCUPANT_TYPE_DESCRIPTION\n      SOURCE_DATA_ID\n      SUPPLIED_SOURCE_ID_IND\n      OCCUPANT_NAME\n      DESCRIPTION\n      PHYSICAL_ADDRESS\n      ...\n      SITE_GEOCODED_IND\n      GEOCODING_METHOD_DESCRIPTION\n      HEALTH_AUTHORITY_CODE\n      HEALTH_AUTHORITY_NAME\n      HEALTH_SERVICE_DLVR_AREA_CODE\n      HEALTH_SERVICE_DLVR_AREA_NAME\n      LOCAL_HEALTH_AREA_CODE\n      LOCAL_HEALTH_AREA_NAME\n      SEQUENCE_ID\n      SE_ANNO_CAD_DATA\n    \n  \n  \n    \n      0\n      POINT (-120.81634 56.25604)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      701\n      Y\n      Fort St. John General Hospital & Peace Villa\n      Hospital\n      8407 112 Avenue , Fort St. John, BC, Peace Riv...\n      ...\n      None\n      Air Photo\n      5\n      Northern Health Authority\n      53\n      Northeast\n      60\n      Fort St. John\n      45\n      None\n    \n    \n      1\n      POINT (-121.42390 49.37725)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      606\n      Y\n      Fraser Canyon Hospital\n      Hospital\n      1275 7th Avenue, Hope, BC, Hope, BC\n      ...\n      None\n      Air Photo\n      2\n      Fraser Health Authority\n      21\n      Fraser East\n      32\n      Hope\n      47\n      None\n    \n    \n      2\n      POINT (-122.49954 52.98134)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      705\n      Y\n      G.R. Baker Memorial Hospital\n      Hospital\n      543 Front Street, Quesnel, BC, Quesnel, BC\n      ...\n      None\n      Air Photo\n      5\n      Northern Health Authority\n      52\n      Northern Interior\n      28\n      Quesnel\n      49\n      None\n    \n    \n      3\n      POINT (-116.96655 51.29723)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      409\n      Y\n      Golden and District General Hospital\n      Hospital\n      835 - 9th Avenue South, Golden, BC, Golden, BC\n      ...\n      None\n      Air Photo\n      1\n      Interior Health Authority\n      11\n      East Kootenay\n      18\n      Golden\n      51\n      None\n    \n    \n      4\n      POINT (-132.07058 53.25481)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      907\n      Y\n      Haida Gwaii Hospital and Health Centre - Xaayd...\n      Hospital\n      3209 Oceanview Drive, Queen Charlotte, BC, Que...\n      ...\n      None\n      Air Photo\n      5\n      Northern Health Authority\n      51\n      Northwest\n      50\n      Queen Charlotte\n      53\n      None\n    \n  \n\n5 rows × 33 columns\n\n\n\n\nSuccess! We have our data. Notice, in particular that we have a column called geometry. This is a special column for spatial datasets that will hold different types of geography depending on what the dataset is. In this case, we have specific locations of hospitals, so the geometry is a list of point objects that contain latitude/longitude coordinates.\nLet’s try a different dataset that will give us area geometries. Area geometries will show up as ‘(multi)polygons’ in the datset, meaning that they are a list of points that define the boundary of some area. This could be school district boundaries, health authority areas, or any other region we might be interested in. For fun, let’s look at the B.C. Wildfire Fire Zones.\n\n\nChallenge 1\n\n\n\n\n\n\nChallenge 1\n\n\n\nBring the B.C. Wildfire Fire Zones dataset into python.\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\n\nfires = bcdata.get_data(\n    dataset='bc-wildfire-fire-zones',\n    as_gdf=True\n)\nfires.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      MOF_FIRE_ZONE_ID\n      MOF_FIRE_CENTRE_NAME\n      MOF_FIRE_ZONE_NAME\n      HEADQUARTERS_CITY_NAME\n      OBJECTID\n      SE_ANNO_CAD_DATA\n      FEATURE_AREA_SQM\n      FEATURE_LENGTH_M\n    \n  \n  \n    \n      0\n      POLYGON ((-118.37713 49.91304, -118.37772 49.9...\n      441\n      Southeast Fire Centre\n      Boundary Fire Zone\n      Grand Forks\n      772\n      None\n      6.590044e+09\n      4.570555e+05\n    \n    \n      1\n      MULTIPOLYGON (((-131.15968 54.00000, -131.6926...\n      442\n      Coastal Fire Centre\n      Fraser Fire Zone\n      Abbotsford\n      773\n      None\n      5.844112e+10\n      2.029064e+06\n    \n    \n      2\n      POLYGON ((-119.73636 50.20392, -119.75076 50.2...\n      443\n      Kamloops Fire Centre\n      Penticton Fire Zone\n      Penticton\n      774\n      None\n      9.302813e+09\n      7.757480e+05\n    \n    \n      3\n      POLYGON ((-121.27589 50.51785, -121.27600 50.5...\n      444\n      Kamloops Fire Centre\n      Merritt Fire Zone\n      Merritt\n      775\n      None\n      1.118309e+10\n      9.195318e+05\n    \n    \n      4\n      POLYGON ((-123.73771 50.84113, -123.73840 50.8...\n      445\n      Coastal Fire Centre\n      Pemberton Fire Zone\n      Pemberton\n      776\n      None\n      1.098514e+10\n      1.024656e+06\n    \n  \n\n\n\n\n\n\n\n\n\nPre-Filtering the Data\nIt might be the case that the data we wish to access is only a subset of the overall dataset. While we could filter the data after it comes into python, it might be faster, more efficient, or simply be less of a memory hog if we filter it before it comes into our session. In this case, we can add the optional query argument when we fetch the data. We can supply a string to this argument that will act like a simple SQL ‘where’ clause. As an example, let’s filter the hospital dataset to include only those in the Vancouver Island Health Authority:\n\nhospital_filtered = bcdata.get_data(\n    dataset='bc-health-care-facilities-hospital', \n    as_gdf=True,\n    query=\"HEALTH_AUTHORITY_NAME='Vancouver Island Health Authority'\"\n)\nhospital_filtered.head()\n\n\n\n\n\n  \n    \n      \n      geometry\n      CUSTODIAN_ORG_DESCRIPTION\n      BUSINESS_CATEGORY_CLASS\n      BUSINESS_CATEGORY_DESCRIPTION\n      OCCUPANT_TYPE_DESCRIPTION\n      SOURCE_DATA_ID\n      SUPPLIED_SOURCE_ID_IND\n      OCCUPANT_NAME\n      DESCRIPTION\n      PHYSICAL_ADDRESS\n      ...\n      SITE_GEOCODED_IND\n      GEOCODING_METHOD_DESCRIPTION\n      HEALTH_AUTHORITY_CODE\n      HEALTH_AUTHORITY_NAME\n      HEALTH_SERVICE_DLVR_AREA_CODE\n      HEALTH_SERVICE_DLVR_AREA_NAME\n      LOCAL_HEALTH_AREA_CODE\n      LOCAL_HEALTH_AREA_NAME\n      SEQUENCE_ID\n      SE_ANNO_CAD_DATA\n    \n  \n  \n    \n      0\n      POINT (-123.50867 48.86185)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      206\n      Y\n      Lady Minto Gulf Islands Hospital\n      Hospital\n      135 Crofton Road, Saltspring Island, BC, Gulf ...\n      ...\n      None\n      Air Photo\n      4\n      Vancouver Island Health Authority\n      41\n      South Vancouver Island\n      64\n      Salt Spring Island\n      65\n      None\n    \n    \n      1\n      POINT (-123.96995 49.18515)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      501\n      Y\n      Nanaimo Regional General Hospital\n      Hospital\n      1200 Dufferin Crescent, Nanaimo, BC, Nanaimo, BC\n      ...\n      None\n      Air Photo\n      4\n      Vancouver Island Health Authority\n      42\n      Central Vancouver Island\n      68\n      Nanaimo\n      83\n      None\n    \n    \n      2\n      POINT (-125.24263 50.00876)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      508\n      Y\n      North Island Hospital, Campbell River & District\n      Hospital\n      375 2nd Avenue, Campbell River, BC, Campbell R...\n      ...\n      None\n      Air Photo\n      4\n      Vancouver Island Health Authority\n      43\n      North Vancouver Island\n      72\n      Campbell River\n      87\n      None\n    \n    \n      3\n      POINT (-125.90858 49.15142)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      854\n      Y\n      Tofino General Hospital\n      Hospital\n      261 Neill Street, Tofino, BC, Alberni, BC\n      ...\n      None\n      Air Photo\n      4\n      Vancouver Island Health Authority\n      42\n      Central Vancouver Island\n      70\n      Tofino\n      139\n      None\n    \n    \n      4\n      POINT (-123.43261 48.46672)\n      Ministry of Health - Provincial Health Service...\n      hospitals\n      Hospitals\n      Hospitals\n      202\n      Y\n      Victoria General Hospital\n      Hospital\n      1 Hospital Way, Victoria, BC, Greater Victoria...\n      ...\n      None\n      Air Photo\n      4\n      Vancouver Island Health Authority\n      41\n      South Vancouver Island\n      61\n      Victoria\n      149\n      None\n    \n  \n\n5 rows × 33 columns\n\n\n\n\n\n\n\n\n\nSQL Syntax\n\n\n\nIn the query above, note that the column in the query is NOT put in an extra set of quotations, whereas the column value is for character columns. This is in line with standard SQL syntax, but if you are not familiar with SQL then it might look a little confusing at first! General rule of thumb for the query is that it will be something of the form:\n\"COLUMN_A = 'some string value' OR COLUMN_B = 42\""
  },
  {
    "objectID": "30_bcdata.html#geospatial-plots",
    "href": "30_bcdata.html#geospatial-plots",
    "title": "8  Introduction to bcdata",
    "section": "8.3 Geospatial Plots",
    "text": "8.3 Geospatial Plots\nAlright, we have some geospatial data in Python now. That’s great. I really love\nPOLYGON ((-118.37713213 49.91303648, -118.37772348 49.91308665, -118.37819007 49.91293438, -118.37864102 49.91273297, -118.37885065 49.91240661, -118.37894791 49.91201733, -118.3791819 49.91159333, -118.37960692 49.91128489, -118.3800468 49.91108959, -118.38056154 49.91089751, -118.38113866 49.91078258, -118.38179033 49.91067428, -118.38237742 49.91061745, -118.38289869 49.91056691, -118.38332771 49.91048198, -118.38381897 49.91036744, -118.38447835 49.91018407, -118.38517458 49.91007327, -118.38585128 49.91001193, -118.38650275 49.90998635, -118.38735549 49.90993369, -118.38790802 49.90991568, -118.38842369 49.90990548, -118.38892747 49.90988955, -118.38972597 49.90985294, -118.39035971 49.90986504, -118.39098773 49.9099297, -118.39161337 49.90996627, -118.39214558 49.91004415, -118.39273105 49.91006478, -118.3932995 49.90999734, -118.39412967 49.90993219, -118.39464161 49.90987966, -118.39535483 49.90980076, -118.39601118 49.90972691, …)\nthis time of year, don’t you?\n… no? No clue where that is or what it might look like? Me neither. Luckily for us, we can checkout these datasets with some geospatial plots! The geopandas dataframe that the data was loaded into can automatically display a map with points, borders, or coloured regions.\nLet’s look at the basic plot that is displayed for a ‘point’ style geometry vs. a ‘polygon’ style geometry. Don’t forget to import matplotlib to display the plots!\n\nimport matplotlib.pyplot as plt\n\nhospitals.plot()\nplt.show()\n\nfires.plot()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWay better! With polygon areas, we also have a few more options for how we wish to colour the regions.\nWe can colour based on a categorical variable:\n\nfires.plot(column='MOF_FIRE_CENTRE_NAME')\nplt.show()\n\n\n\n\nOr on a numerical variable:\n\nfires.plot(column='FEATURE_AREA_SQM', legend=True)\nplt.show()\n\n\n\n\nOr if we only want to display the boundaries of our regions, we can do that too:\n\nfires.boundary.plot()\nplt.show()\n\n\n\n\nYou’ll notice that we can easily see the shape of B.C. in these region plots, but it is less obvious for the hospitals dataset. Because the hospitals dataset only contains ‘point’ objects, it will just place these points on an axis, but with no region boundaries to reference, it might just look like a scattering of points on a plot.\nLuckily, we can combine our region plot and point plot to give context to point locations. Because the underlying plotting tools used by these dataframes is matplotlib, we can plot both plots to the same axes before showing the overall figure!\n\nax = fires.boundary.plot()  # assign the output of this first plot to a variable \n\nhospitals.plot(\n    ax=ax, # use the output of the first plot as the basis for the next \n    marker='.',  # apply some styling options \n    color='red'\n    )  \n\n# use the axis again later to turn off the annoying axis labels\n# we don't always want/need those for geospatial plots! \nax.set_axis_off()\n\n# display your work!\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAdvanced Tip: Managing Projections\n\n\n\n\n\nWe were able to plot these two spatial datasets together because they were imported into python using the same Coordinate Reference System (CRS). The CRS tells the polygon or point shapes how to relate to actual physical locations on Earth. On the B.C. Catalogue Reference Page for both datasets, the spatial reference system is given as EPSG:3005 - NAD83/BC Albers.\nWe can check what the CRS is for a given dataset in Python by looking at the geometry series:\n\ndisplay(hospitals['geometry'].crs)\ndisplay(fires['geometry'].crs)\n\n<Geographic 2D CRS: +init=epsg:4326 +type=crs>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- lon[east]: Longitude (degree)\n- lat[north]: Latitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\n<Geographic 2D CRS: +init=epsg:4326 +type=crs>\nName: WGS 84\nAxis Info [ellipsoidal]:\n- lon[east]: Longitude (degree)\n- lat[north]: Latitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nIf for some reason, the two spatial datasets do not have the same reference system, the geopandas dataset gives an option for converting one CRS to another:\nmy_geoseries = my_geoseries.to_crs(\"EPSG:4326\")\nmy_geoseries = my_geoseries.to_crs(epsg=4326)\n\n\n\nWell there you have it. You can find more information on making geopandas plots here, as well as more information on the bcdata package here. We know that this is a much more technical topic than some of the previous sections, but hopefully it will get you excited about all the new ways of exploring data that python allows us to explore!"
  },
  {
    "objectID": "31_advanced_pandas.html#conditional-column-assignment",
    "href": "31_advanced_pandas.html#conditional-column-assignment",
    "title": "9  Advanced Pandas",
    "section": "9.1 Conditional Column Assignment",
    "text": "9.1 Conditional Column Assignment\nIn our previous sections on Pandas, we introduced the idea of column creation based on the data we currently have. We limited ourselves to fairly standard examples that are likely to crop up in typical data analysis: arithmetic operations, applying column specific functions, and the use of the lambda function inside a call to either apply or assign. Here we will expand our column possibilities to include conditional assignments, where the value of the column is dependent on the value/state of some other column.\nLet’s look at the penguins dataset we considered previously. This dataset has a good mix of categorical and numerical datatypes for us to create new columns from.\n\nimport pandas as pd\nfrom seaborn import load_dataset\n\npenguins = load_dataset('penguins')\npenguins.head()\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      Male\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      Female\n    \n    \n      2\n      Adelie\n      Torgersen\n      40.3\n      18.0\n      195.0\n      3250.0\n      Female\n    \n    \n      3\n      Adelie\n      Torgersen\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      Adelie\n      Torgersen\n      36.7\n      19.3\n      193.0\n      3450.0\n      Female\n    \n  \n\n\n\n\n\nBinary If/Else\nThe first type of conditional assignment we will consider is the simplest: if original column is value A, then assign value x, otherwise assign value y. The most straightforward way to do this is via the apply method again, combined with a lambda function acting on the column of interest. For this example, we are going to make a shorthand version of the sex column - if the sex column is Female, then assign F, otherwise assign M.\n\npenguins['if_else_column'] = penguins.sex.apply(lambda x: 'F' if x=='Female' else 'M')\npenguins[['sex', 'if_else_column']].sample(5)\n\n\n\n\n\n  \n    \n      \n      sex\n      if_else_column\n    \n  \n  \n    \n      307\n      Male\n      M\n    \n    \n      56\n      Female\n      F\n    \n    \n      33\n      Male\n      M\n    \n    \n      294\n      Female\n      F\n    \n    \n      174\n      Female\n      F\n    \n  \n\n\n\n\nWhile apply is robust and can be used in many different applications and settings, it is often the slowest method available in Pandas. If you are doing very specific operations, Pandas often provides faster, vectorized methods to do the same thing. For instance, this if/else example can also be calculated using the Pandas where and mask methods. These methods complement each other, and you may find one more intuitive than the other based on the specific circumstances of the data you are working with.\nBoth methods assume you already have some column with a default value in place. For example, we can assign the value ‘M’ to an entire column. where and mask then work by updating the values of this column where some other condition is met. The syntax for where is:\nIf condition is met, keep the default value, otherwise update to the other value:\n\npenguins['where_column'] = 'M'\npenguins['where_column'] = penguins.where_column.where(penguins.sex=='Male', 'F')\npenguins[['sex', 'if_else_column', 'where_column']].sample(5)\n\n\n\n\n\n  \n    \n      \n      sex\n      if_else_column\n      where_column\n    \n  \n  \n    \n      310\n      Female\n      F\n      F\n    \n    \n      181\n      Male\n      M\n      M\n    \n    \n      27\n      Female\n      F\n      F\n    \n    \n      307\n      Male\n      M\n      M\n    \n    \n      73\n      Male\n      M\n      M\n    \n  \n\n\n\n\nWhile the syntax for mask is:\nIf condition is met, update to other value, otherwise keep the default value:\n\npenguins['mask_column'] = 'M'\npenguins['mask_column'] = penguins.mask_column.mask(penguins.sex=='Female', 'F')\npenguins[['sex', 'if_else_column', 'where_column', 'mask_column']].sample(5)\n\n\n\n\n\n  \n    \n      \n      sex\n      if_else_column\n      where_column\n      mask_column\n    \n  \n  \n    \n      154\n      Male\n      M\n      M\n      M\n    \n    \n      319\n      Male\n      M\n      M\n      M\n    \n    \n      233\n      Male\n      M\n      M\n      M\n    \n    \n      339\n      NaN\n      M\n      F\n      M\n    \n    \n      9\n      NaN\n      M\n      F\n      M\n    \n  \n\n\n\n\nAgain, while these all result in the same column, you may find one method more intuitive, efficient, or faster for the data you are working with. Explore and use what works best for you!\n\n\nMultiple Conditions\nSometimes, a simple if/else statement doesn’t cut it. Perhaps there are multiple options, and creating an if…elseif…elseif…else block will not only only be less intuitive but can easily be used incorrectly, leading to undesired outputs. In this case, the mapping functionality of Pandas provides a vectorized efficient method to create multiple changes to a column. The map method, which we apply to a single series, is optimized for mapping elements from one domain to another. The best example of this is using a dictionary to provide a mapping of your keys (initial column values) to your new value set.\nLet’s see this in action with the penguins. Perhaps we do not wish to bias our analysis by having knowledge of which island each penguin came from. In this case, we wish to replace the actual names of the three islands with generic island IDs. First, we supply our mapping dictionary that will transform each island to its generic ID. Then, we supply this dictionary to our map method:\n\nmy_mapping = {\n    'Torgersen': 'Island 1',\n    'Biscoe': 'Island 2',\n    'Dream': 'Island 3'\n}\npenguins['mapped_islands'] = penguins.island.map(my_mapping)\npenguins[['island', 'mapped_islands']].sample(10)\n\n\n\n\n\n  \n    \n      \n      island\n      mapped_islands\n    \n  \n  \n    \n      51\n      Biscoe\n      Island 2\n    \n    \n      4\n      Torgersen\n      Island 1\n    \n    \n      42\n      Dream\n      Island 3\n    \n    \n      283\n      Biscoe\n      Island 2\n    \n    \n      306\n      Biscoe\n      Island 2\n    \n    \n      180\n      Dream\n      Island 3\n    \n    \n      162\n      Dream\n      Island 3\n    \n    \n      97\n      Dream\n      Island 3\n    \n    \n      313\n      Biscoe\n      Island 2\n    \n    \n      119\n      Torgersen\n      Island 1\n    \n  \n\n\n\n\n\n\nComplex Conditions\nThe above example used the map method, which is inherently a method to be used for Series only. As such, we cannot use it for more complicated conditions that may involve multiple columns. The final example we will consider is a more complicated set of conditions: we wish to create new categories based on the penguins body mass: underweight, overweight, or average weight. However, we also wish to consider the penguin species when creating these categories: what is underweight for the Gentoo species may be overweight for the Adelie! Thus, in theory we have 9 new categories to account for that depend on two columns.\nThe best (read: efficient) way to approach this multi-column multi-conditional scenario is actually to go outside of the Pandas toolkit and use a tool that has been implemented by numpy. Numpy is a computational tool-kit for Python that utilizes the efficiency of powerful computational languages such as Fortran and C to provide fast and versatile computational tools for the vast majority of numerical and scientific anaylsis in Python. We have seen it already, having used numpy’s statistical functions such as mean and median within our aggregations. Here, we wish to make use of the numpy.select() method.\nIn numpy, the select method is fairly simple. We supply a list of conditions we wish to consider, and a list of choices for what we wish to happen for each of the conditions. We may also supply a default value, for if none of the conditions are met. select will go through the list one by one, and will apply the choice for the first condition that is met. Let’s see what this looks like for our penguins:\n\nimport numpy as np\n\n# list all possible combos of species and specific body masses\n# note that we don't include the overweight combos, as we can catch these\n# with the default option in select!\nconditions = [\n    (penguins.species=='Adelie') & (penguins.body_mass_g<3300),\n    (penguins.species=='Adelie') & (penguins.body_mass_g<3700),\n    (penguins.species=='Chinstrap') & (penguins.body_mass_g<3333),\n    (penguins.species=='Chinstrap') & (penguins.body_mass_g<3733),\n    (penguins.species=='Gentoo') & (penguins.body_mass_g<4676),\n    (penguins.species=='Gentoo') & (penguins.body_mass_g<5076)\n]\n\n# for each listed condition, tell select what value to use\nchoices = [\n    'Underweight',\n    'Average weight', \n    'Underweight', \n    'Average weight',\n    'Underweight',\n    'Average weight'\n]\n\n# use select to create our new column\npenguins['general_weight'] = np.select(conditions, choices, default='Overweight')\n\n# display results\npenguins[['species', 'body_mass_g', 'general_weight']].sample(10)\n\n\n\n\n\n  \n    \n      \n      species\n      body_mass_g\n      general_weight\n    \n  \n  \n    \n      157\n      Chinstrap\n      3950.0\n      Overweight\n    \n    \n      19\n      Adelie\n      4200.0\n      Overweight\n    \n    \n      257\n      Gentoo\n      5250.0\n      Overweight\n    \n    \n      205\n      Chinstrap\n      4050.0\n      Overweight\n    \n    \n      320\n      Gentoo\n      4850.0\n      Average weight\n    \n    \n      317\n      Gentoo\n      4875.0\n      Average weight\n    \n    \n      258\n      Gentoo\n      4350.0\n      Underweight\n    \n    \n      124\n      Adelie\n      3050.0\n      Underweight\n    \n    \n      197\n      Chinstrap\n      4450.0\n      Overweight\n    \n    \n      240\n      Gentoo\n      5700.0\n      Overweight\n    \n  \n\n\n\n\n\n\n\n\n\n\nOrder Matters!\n\n\n\nWhen using the select method, the order of the conditions matters! Numpy always goes through the list in order, looking for the first TRUE condition. This ordering will matter, for example, if creating a list of bins for some numerical category. Conditions of the form:\n\nconditions = [\n    penguins.body_mass_g < 5_000,\n    penguins.body_mass_g < 4_000,\n    penguins.body_mass_g < 3_000,\n    penguins.body_mass_g < 2_000,\n    penguins.body_mass_g < 1_000\n]\n\nWill never evaluate the choices for the conditions 2-5, as the first condition will be true as well. Instead we should flip this list and write our conditions starting from 1,000 and ending at 5,000. Keep this in mind when creating your conditions!\n\n\n\n\nChallenge 1\n\n\n\n\n\n\nChallenge 1\n\n\n\nCreate a column for 5-10 flipper length bins and plot the resulting frequencies (counts for each bin).\n\n\n\n\n\n\n\n\nSolution to Challenge 1\n\n\n\n\n\nHere we’re going to pull together some Seaborn code and our new-found conditional knowledge to create the bins.\n\n# make sure seaborn is imported\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# check roughly where we could place bins\ndisplay(penguins.agg({'flipper_length_mm': ['mean', 'std', 'min', 'max']}))\n\n# create some conditions for bins\nconditions = [\n    penguins.flipper_length_mm < 180,\n    penguins.flipper_length_mm < 190,\n    penguins.flipper_length_mm < 200,\n    penguins.flipper_length_mm < 210,\n    penguins.flipper_length_mm < 220,\n    penguins.flipper_length_mm < 230\n]\n\n# assign names to each condition\nchoices = [\n    '0 - 180',\n    '180 - 190',\n    '190 - 200',\n    '200 - 210',\n    '210 - 220',\n    '220 - 230'\n]\n\n# add the new column\npenguins['flipper_length_bins'] = np.select(conditions, choices, default='230 +')\n\n# plot this data using the 'count' option\nsns.catplot(\n    penguins, \n    x='flipper_length_bins', \n    kind='count',\n    order = choices,\n    color = 'blue'\n)\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n  \n    \n      \n      flipper_length_mm\n    \n  \n  \n    \n      mean\n      200.915205\n    \n    \n      std\n      14.061714\n    \n    \n      min\n      172.000000\n    \n    \n      max\n      231.000000"
  },
  {
    "objectID": "31_advanced_pandas.html#advanced-grouping",
    "href": "31_advanced_pandas.html#advanced-grouping",
    "title": "9  Advanced Pandas",
    "section": "9.2 Advanced Grouping",
    "text": "9.2 Advanced Grouping\n\n\n\ngroupby cuteness\n\n\nIn our previous section on Pandas, we explored the basic tools we need to group our data by various columns, apply a host of statistical aggregation operations, and look at these outcomes. While incredibly powerful as is, these tools lack some functionality that you might wish to have. Some examples include:\n\nCombining row level values and group level aggregations in a single command.\nFiltering and subsetting a dataframe based on group values.\nCombining aggregations across different columns.\n\nIn order to handle these operations, we need to introduce two new methods: transform and apply.\n\n\n\n\n\n\nApply… again?\n\n\n\nYou might question whether apply is actually a new function, seeing as we have used it previously when creating new columns in a dataframe. In fact, the effect of apply within pandas is subtly different depending on how and where we apply it. The effect of apply is different when used on:\n\nA dataframe as a whole - dataframe.apply(...)\nA single series - series.apply(...)\nA grouped dataframe object - dataframe.groupby('column').apply(...)\n\nMake sure you keep track of what kind of apply you are using when doing your dataframe manipulations, as identical functions in different scenarios can lead to unusually different results! In this section, we are focusing on the grouped dataframe objects.\n\n\nBoth transform and apply can be used on a grouped dataframe object. This is the object that is created when we use the .groupby() method. Let’s create a small dataset and see what this object looks like:\n\nstarwars = pd.DataFrame(\n    {\n        'unit' : [\n            'Blaster', 'Blaster', 'Blaster', \n            'Lightsaber', 'Lightsaber', 'Lightsaber', 'Lightsaber', \n            'Stick', 'Stick', 'Stick'\n            ], \n        'cost' : [42, 60, 40, 900, 4000, 2000, 100, 10, 1, 5],\n        'sale_price' : [50, 75, 42, 1000, 5000, 2000, 4242, 4, 2, 20]\n    }\n)\n\nstarwars\n\n\n\n\n\n  \n    \n      \n      unit\n      cost\n      sale_price\n    \n  \n  \n    \n      0\n      Blaster\n      42\n      50\n    \n    \n      1\n      Blaster\n      60\n      75\n    \n    \n      2\n      Blaster\n      40\n      42\n    \n    \n      3\n      Lightsaber\n      900\n      1000\n    \n    \n      4\n      Lightsaber\n      4000\n      5000\n    \n    \n      5\n      Lightsaber\n      2000\n      2000\n    \n    \n      6\n      Lightsaber\n      100\n      4242\n    \n    \n      7\n      Stick\n      10\n      4\n    \n    \n      8\n      Stick\n      1\n      2\n    \n    \n      9\n      Stick\n      5\n      20\n    \n  \n\n\n\n\nHere we have a column that looks categorical (unit), and a couple that look numerical (cost and sale_price). We are going to focus on grouping our data according to the categorical column, and see what we can do with the numerical columns.\n\ngrouped_starwars = starwars.groupby('unit')\ngrouped_starwars\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f04357ffe20>\n\n\nNote that this object is no longer a dataframe, but some sort of DataFrameGroupBy object. By itself, it is not very useful, but we can use it to get all sorts of interesting information. Recall briefly how we used this earlier. Maybe we want the average buy and sell prices of each type of unit:\n\ngrouped_starwars.mean().reset_index()\n\n\n\n\n\n  \n    \n      \n      unit\n      cost\n      sale_price\n    \n  \n  \n    \n      0\n      Blaster\n      47.333333\n      55.666667\n    \n    \n      1\n      Lightsaber\n      1750.000000\n      3060.500000\n    \n    \n      2\n      Stick\n      5.333333\n      8.666667\n    \n  \n\n\n\n\nWhat if we wanted to know how much each particular unit contributed to it’s groups overall sale? Or perhaps how much each group profited, on average? This is where transform and apply come in. Let’s look at what each of these do.\n\nTransform\n\nUsed to apply a function to a dataframe that produces a new dataframe with the same shape as the original.\nWhile an entire dataframe can be passed to transform, it only ever sees (operates on) a single column/series at a time.\nThe function must either return a scalar value or a sequence that is the same length as the original dataframe.\nUseful for getting groups aggregations back into the original dataframe\nUseful for filtering based on group values\n\n\nEx. 1: Percent of Group\nLet’s see each this in action. We will use transform to determine how much each unit contributed to the total sales of its group.\n\npct_of_sales = (\n    grouped_starwars\n    .sale_price\n    .transform(lambda series_: series_/series_.sum())\n)\npct_of_sales\n\n0    0.299401\n1    0.449102\n2    0.251497\n3    0.081686\n4    0.408430\n5    0.163372\n6    0.346512\n7    0.153846\n8    0.076923\n9    0.769231\nName: sale_price, dtype: float64\n\n\nNotice here that we used the lambda function again, which allows us to define a function inline acting on a given argument (in this case, the argument is named series_). This let us allow the value of the series to be the sum of the series. Because we are acting on a grouped object, Pandas knows that the sum we wish to employ is that of the group, not the overall sum! This new object will be in the same order (have the same indexing) as our initial dataframe, and so we can add it back to the dataframe as an extra column if we wish. This works explicitly because the transform method is required to return a series of the same length as the original object, giving us exactly what we want!\n\nstarwars['percent_of_sales'] = pct_of_sales\nstarwars\n\n\n\n\n\n  \n    \n      \n      unit\n      cost\n      sale_price\n      percent_of_sales\n    \n  \n  \n    \n      0\n      Blaster\n      42\n      50\n      0.299401\n    \n    \n      1\n      Blaster\n      60\n      75\n      0.449102\n    \n    \n      2\n      Blaster\n      40\n      42\n      0.251497\n    \n    \n      3\n      Lightsaber\n      900\n      1000\n      0.081686\n    \n    \n      4\n      Lightsaber\n      4000\n      5000\n      0.408430\n    \n    \n      5\n      Lightsaber\n      2000\n      2000\n      0.163372\n    \n    \n      6\n      Lightsaber\n      100\n      4242\n      0.346512\n    \n    \n      7\n      Stick\n      10\n      4\n      0.153846\n    \n    \n      8\n      Stick\n      1\n      2\n      0.076923\n    \n    \n      9\n      Stick\n      5\n      20\n      0.769231\n    \n  \n\n\n\n\n\n\nEx 2: Grouped Aggregate\nTransform can be used exclusively with an aggregation function as well, but it will still return a value for every row of the initial dataset. This allows us to add grouped level values as columns to the data:\n\nstarwars['average_unit_cost'] = grouped_starwars.cost.transform('mean')\nstarwars\n\n\n\n\n\n  \n    \n      \n      unit\n      cost\n      sale_price\n      percent_of_sales\n      average_unit_cost\n    \n  \n  \n    \n      0\n      Blaster\n      42\n      50\n      0.299401\n      47.333333\n    \n    \n      1\n      Blaster\n      60\n      75\n      0.449102\n      47.333333\n    \n    \n      2\n      Blaster\n      40\n      42\n      0.251497\n      47.333333\n    \n    \n      3\n      Lightsaber\n      900\n      1000\n      0.081686\n      1750.000000\n    \n    \n      4\n      Lightsaber\n      4000\n      5000\n      0.408430\n      1750.000000\n    \n    \n      5\n      Lightsaber\n      2000\n      2000\n      0.163372\n      1750.000000\n    \n    \n      6\n      Lightsaber\n      100\n      4242\n      0.346512\n      1750.000000\n    \n    \n      7\n      Stick\n      10\n      4\n      0.153846\n      5.333333\n    \n    \n      8\n      Stick\n      1\n      2\n      0.076923\n      5.333333\n    \n    \n      9\n      Stick\n      5\n      20\n      0.769231\n      5.333333\n    \n  \n\n\n\n\n\n\nEx 3: Multiple Series\nInstead of passing a single column to the transform method, we could also pass the entire dataframe (or some subset of the columns in the dataframe), and calculate values for each series:\n\ngrouped_starwars.transform(lambda series_: (series_-series_.mean())/series_.std())\n\n\n\n\n\n  \n    \n      \n      cost\n      sale_price\n    \n  \n  \n    \n      0\n      -0.484182\n      -0.329183\n    \n    \n      1\n      1.149932\n      1.123095\n    \n    \n      2\n      -0.665750\n      -0.793912\n    \n    \n      3\n      -0.502909\n      -1.099922\n    \n    \n      4\n      1.331229\n      1.035331\n    \n    \n      5\n      0.147914\n      -0.566109\n    \n    \n      6\n      -0.976235\n      0.630700\n    \n    \n      7\n      1.034910\n      -0.473016\n    \n    \n      8\n      -0.960988\n      -0.675737\n    \n    \n      9\n      -0.073922\n      1.148754\n    \n  \n\n\n\n\nThis has created two columns that have both had the same function applied. Note that if we wanted to merge these in with the original dataframe, we should be careful to rename the columns so that they do not overlap!\n\n\nEx 4: Filtering Original Dataframe\nAnother use case for transform is in filtering a dataframe. Maybe we want to zero in on those items where the sale price of the object was less than the average unit cost. Let’s filter our dataframe to only those rows:\n\ncondition = starwars.sale_price < grouped_starwars.cost.transform(lambda x: x.mean())\nstarwars[condition]\n\n\n\n\n\n  \n    \n      \n      unit\n      cost\n      sale_price\n      percent_of_sales\n      average_unit_cost\n    \n  \n  \n    \n      2\n      Blaster\n      40\n      42\n      0.251497\n      47.333333\n    \n    \n      3\n      Lightsaber\n      900\n      1000\n      0.081686\n      1750.000000\n    \n    \n      7\n      Stick\n      10\n      4\n      0.153846\n      5.333333\n    \n    \n      8\n      Stick\n      1\n      2\n      0.076923\n      5.333333\n    \n  \n\n\n\n\n\n\n\nApply\n\nUsed to apply a function (aggregated or otherwise) across multiple columns\nImplicitly passes all the columns of the dataframe as a dataframe to the function, allowing for column interactions\nThe function can return a scalar or a sequence of any length.\n\n\nEx 1. Aggregate over Multiple Columns\nLet’s try something with apply. What if we want to know the average overall profit of each group. We could produce a profit column, group up on the unit, and then calculate the mean.\n\nstarwars['profit'] = starwars.sale_price - starwars.cost\nstarwars.groupby('unit').profit.mean()\n\nunit\nBlaster          8.333333\nLightsaber    1310.500000\nStick            3.333333\nName: profit, dtype: float64\n\n\nWe could also do this using the apply function, applied to our grouped object:\n\ngrouped_starwars.apply(lambda df_: (df_.sale_price - df_.cost).mean())\n\nunit\nBlaster          8.333333\nLightsaber    1310.500000\nStick            3.333333\ndtype: float64\n\n\nAgain, we made use of that lambda function, this time applying it to a df_ argument. You might notice that here I used a df_ argument instead of series_: this is merely notation, and we could have used anything (series_, df_, x, this_is_my_argument would all work the same). However, we have used df_ and series_ so that we can remind ourselves exactly what type of object we are acting on.\nTo see how transform differs from apply, let’s try to do that exact same operation:\n\ngrouped_starwars.transform(lambda _df: (_df.sale_price - _df.cost).mean())\n\nAttributeError: 'Series' object has no attribute 'sale_price'\n\n\nThis fails because the object being acted on inside transform is itself just a series object, not the entire dataframe! As such, it has no attribute for sale_price or buy_price like our original dataframe does. Instead, it acts on the sale_price series, then the buy_price series, and returns the results.\n\n\nEx 2. Mix Row/Aggregate Levels\nWhile the first apply example returns a rolled up aggregated dataframe, we can also use apply to return the individual rows of the dataframe by mixing aggregation functions with row level functions.\n\ngrouped_starwars.apply(lambda df_: df_.sale_price - df_.cost.mean())\n\nunit         \nBlaster     0       2.666667\n            1      27.666667\n            2      -5.333333\nLightsaber  3    -750.000000\n            4    3250.000000\n            5     250.000000\n            6    2492.000000\nStick       7      -1.333333\n            8      -3.333333\n            9      14.666667\nName: sale_price, dtype: float64\n\n\nIn this way we have to be careful: transform will always return a dataframe that is the same size as the original, while apply will return something that varies with the type of function we have utilized.\n\n\nEx 3. Partial Aggregates\nApply lets us play some cool tricks as well. Suppose we only wanted to know about the two most expensive sales in each category. How could we filter to show this? We can use the nlargest (or smallest) method in conjunction with apply. nlargest does exactly what we might expect, returning the rows with the n largest values according to the provided column(s):\n\nstarwars.nlargest(2, 'sale_price')\n\n\n\n\n\n  \n    \n      \n      unit\n      cost\n      sale_price\n      percent_of_sales\n      average_unit_cost\n      profit\n    \n  \n  \n    \n      4\n      Lightsaber\n      4000\n      5000\n      0.408430\n      1750.0\n      1000\n    \n    \n      6\n      Lightsaber\n      100\n      4242\n      0.346512\n      1750.0\n      4142\n    \n  \n\n\n\n\nBut if we mix this with apply and our grouping dataframe, we can get the largest for each group!\n\ngrouped_starwars.apply(lambda df_: df_.nlargest(2, 'sale_price'))\n\n\n\n\n\n  \n    \n      \n      \n      unit\n      cost\n      sale_price\n      percent_of_sales\n      average_unit_cost\n      profit\n    \n    \n      unit\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      Blaster\n      1\n      Blaster\n      60\n      75\n      0.449102\n      47.333333\n      15\n    \n    \n      0\n      Blaster\n      42\n      50\n      0.299401\n      47.333333\n      8\n    \n    \n      Lightsaber\n      4\n      Lightsaber\n      4000\n      5000\n      0.408430\n      1750.000000\n      1000\n    \n    \n      6\n      Lightsaber\n      100\n      4242\n      0.346512\n      1750.000000\n      4142\n    \n    \n      Stick\n      9\n      Stick\n      5\n      20\n      0.769231\n      5.333333\n      15\n    \n    \n      7\n      Stick\n      10\n      4\n      0.153846\n      5.333333\n      -6\n    \n  \n\n\n\n\nNote that when apply produces some different sized aggregate than the original dataframe, it tacks on an extra index indicating what the grouper was. We do not always care about this, and can eliminate it in the initial creation of our grouped dataframe via the group_keys argument:\n\n(\n    starwars\n    .groupby('unit', group_keys=False)\n    .apply(lambda df_: df_.nlargest(2, 'sale_price'))\n)\n\n\n\n\n\n  \n    \n      \n      unit\n      cost\n      sale_price\n      percent_of_sales\n      average_unit_cost\n      profit\n    \n  \n  \n    \n      1\n      Blaster\n      60\n      75\n      0.449102\n      47.333333\n      15\n    \n    \n      0\n      Blaster\n      42\n      50\n      0.299401\n      47.333333\n      8\n    \n    \n      4\n      Lightsaber\n      4000\n      5000\n      0.408430\n      1750.000000\n      1000\n    \n    \n      6\n      Lightsaber\n      100\n      4242\n      0.346512\n      1750.000000\n      4142\n    \n    \n      9\n      Stick\n      5\n      20\n      0.769231\n      5.333333\n      15\n    \n    \n      7\n      Stick\n      10\n      4\n      0.153846\n      5.333333\n      -6\n    \n  \n\n\n\n\n\n\nEx 4. Create Multiple Columns\nWe can use the dataframe behaviour of apply to create multiple additional columns all at once within an apply. Let’s create a function that will act on each dataframe group, create new values, and return the dataframe.\n\ndef create_multiple_columns(df_):\n    df_['average_group_profit'] = (df_.sale_price - df_.cost).mean()\n    df_['profit_relative_to_group'] = df_['profit']/df_['average_group_profit']\n    return df_\n\ngrouped_starwars.apply(create_multiple_columns)\n\n\n\n\n\n  \n    \n      \n      unit\n      cost\n      sale_price\n      percent_of_sales\n      average_unit_cost\n      profit\n      average_group_profit\n      profit_relative_to_group\n    \n  \n  \n    \n      0\n      Blaster\n      42\n      50\n      0.299401\n      47.333333\n      8\n      8.333333\n      0.960000\n    \n    \n      1\n      Blaster\n      60\n      75\n      0.449102\n      47.333333\n      15\n      8.333333\n      1.800000\n    \n    \n      2\n      Blaster\n      40\n      42\n      0.251497\n      47.333333\n      2\n      8.333333\n      0.240000\n    \n    \n      3\n      Lightsaber\n      900\n      1000\n      0.081686\n      1750.000000\n      100\n      1310.500000\n      0.076307\n    \n    \n      4\n      Lightsaber\n      4000\n      5000\n      0.408430\n      1750.000000\n      1000\n      1310.500000\n      0.763068\n    \n    \n      5\n      Lightsaber\n      2000\n      2000\n      0.163372\n      1750.000000\n      0\n      1310.500000\n      0.000000\n    \n    \n      6\n      Lightsaber\n      100\n      4242\n      0.346512\n      1750.000000\n      4142\n      1310.500000\n      3.160626\n    \n    \n      7\n      Stick\n      10\n      4\n      0.153846\n      5.333333\n      -6\n      3.333333\n      -1.800000\n    \n    \n      8\n      Stick\n      1\n      2\n      0.076923\n      5.333333\n      1\n      3.333333\n      0.300000\n    \n    \n      9\n      Stick\n      5\n      20\n      0.769231\n      5.333333\n      15\n      3.333333\n      4.500000\n    \n  \n\n\n\n\n When done this way, we are explicitly returning the dataframe, regardless of if it was grouped or not. Even if all of the new columns are aggregates, this will still produce a non-aggregated output:\n\ndef create_multiple_aggregated_columns(df_):\n    df_['average_group_profit'] = (df_.sale_price - df_.cost).mean()\n    df_['average_sale_price'] = df_.sale_price.mean()\n    return df_\n\ngrouped_starwars.apply(create_multiple_aggregated_columns)\n\n\n\n\n\n  \n    \n      \n      unit\n      cost\n      sale_price\n      percent_of_sales\n      average_unit_cost\n      profit\n      average_group_profit\n      average_sale_price\n    \n  \n  \n    \n      0\n      Blaster\n      42\n      50\n      0.299401\n      47.333333\n      8\n      8.333333\n      55.666667\n    \n    \n      1\n      Blaster\n      60\n      75\n      0.449102\n      47.333333\n      15\n      8.333333\n      55.666667\n    \n    \n      2\n      Blaster\n      40\n      42\n      0.251497\n      47.333333\n      2\n      8.333333\n      55.666667\n    \n    \n      3\n      Lightsaber\n      900\n      1000\n      0.081686\n      1750.000000\n      100\n      1310.500000\n      3060.500000\n    \n    \n      4\n      Lightsaber\n      4000\n      5000\n      0.408430\n      1750.000000\n      1000\n      1310.500000\n      3060.500000\n    \n    \n      5\n      Lightsaber\n      2000\n      2000\n      0.163372\n      1750.000000\n      0\n      1310.500000\n      3060.500000\n    \n    \n      6\n      Lightsaber\n      100\n      4242\n      0.346512\n      1750.000000\n      4142\n      1310.500000\n      3060.500000\n    \n    \n      7\n      Stick\n      10\n      4\n      0.153846\n      5.333333\n      -6\n      3.333333\n      8.666667\n    \n    \n      8\n      Stick\n      1\n      2\n      0.076923\n      5.333333\n      1\n      3.333333\n      8.666667\n    \n    \n      9\n      Stick\n      5\n      20\n      0.769231\n      5.333333\n      15\n      3.333333\n      8.666667\n    \n  \n\n\n\n\n\n\n\nChallenge 2\n\n\n\n\n\n\nChallenge 2\n\n\n\nDetermine which of the units in each group had the greatest profit, and return a dataframe containing only the rows corresponding to these units.\n\n\n\n\n\n\n\n\nSolution to Challenge 2\n\n\n\n\n\nThere are a variety of different ways this could be approached. We have done a similar question using apply already, so let’s do this via the transform function.\n\ncondition = starwars.profit == starwars.groupby('unit').profit.transform('max')\n\nstarwars[condition]\n\n\n\n\n\n  \n    \n      \n      unit\n      cost\n      sale_price\n      percent_of_sales\n      average_unit_cost\n      profit\n    \n  \n  \n    \n      1\n      Blaster\n      60\n      75\n      0.449102\n      47.333333\n      15\n    \n    \n      6\n      Lightsaber\n      100\n      4242\n      0.346512\n      1750.000000\n      4142\n    \n    \n      9\n      Stick\n      5\n      20\n      0.769231\n      5.333333\n      15"
  },
  {
    "objectID": "31_advanced_pandas.html#windowing-functions",
    "href": "31_advanced_pandas.html#windowing-functions",
    "title": "9  Advanced Pandas",
    "section": "9.3 Windowing Functions",
    "text": "9.3 Windowing Functions\n\n\n\n\n\nOften when we are looking at our data, we may have a question about how one value compares to the values around it. This is typically useful for timeseries data, where we can organize our data by date (or year, month, etc). Once sorted on a date, we might wish to know things such as:\n\nDid this year have a lower or higher average than the previous year?\nWhat is the roling average over the last 5 years?\n\nOr many other comparative questions. We can answer some of these questions using the windowing capabilities of Pandas. Windowing is the act of looking at a single ‘window’ of the data, asking our question (what is the average value for the time period in this window?), and then sliding the window to do it again. You have likely heard the term rolling average - this is what windowing allows us to calculate.\n\nRolling\nTo calculate a rolling average, we use two methods. First, we indicate that we want rolling statistics by using the rolling() method, and include in the argument the number of periods (rows) to include. After the rolling method, we indicate which aggregation we would like to use. Let’s try this out on a timeseries dataset:\n\ndowjones = load_dataset('dowjones')\ndowjones.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n    \n  \n  \n    \n      0\n      1914-12-01\n      55.00\n    \n    \n      1\n      1915-01-01\n      56.55\n    \n    \n      2\n      1915-02-01\n      56.00\n    \n    \n      3\n      1915-03-01\n      58.30\n    \n    \n      4\n      1915-04-01\n      66.45\n    \n  \n\n\n\n\nThis is a monthly aggregate of the Dow Jones index. Let’s compare this monthly aggregate to a 6 month rolling average:\n\ndowjones['six_month_avg'] = dowjones.rolling(6).Price.mean()\ndowjones.head(10)\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n      six_month_avg\n    \n  \n  \n    \n      0\n      1914-12-01\n      55.00\n      NaN\n    \n    \n      1\n      1915-01-01\n      56.55\n      NaN\n    \n    \n      2\n      1915-02-01\n      56.00\n      NaN\n    \n    \n      3\n      1915-03-01\n      58.30\n      NaN\n    \n    \n      4\n      1915-04-01\n      66.45\n      NaN\n    \n    \n      5\n      1915-05-01\n      65.95\n      59.708333\n    \n    \n      6\n      1915-06-01\n      68.40\n      61.941667\n    \n    \n      7\n      1915-07-01\n      71.85\n      64.491667\n    \n    \n      8\n      1915-08-01\n      79.25\n      68.366667\n    \n    \n      9\n      1915-09-01\n      85.50\n      72.900000\n    \n  \n\n\n\n\n\n\n\n\n\n\nSort your Data!\n\n\n\nWhen using the rolling method this way, it assumes that the data is already sorted in the order you want! The dowjones dataset comes pre-sorted, but this isn’t always the case. Before doing any sort of windowing function, it is always a good idea to pre-sort your data! For the dowjones dataset, we would sort on the Date column:\n\ndowjones = dowjones.sort_values(by='Date')\n\n\n\nYou’ll notice that for the first 5 months, this method returns NaN - this is because there were not enough values to include in the average, as the method assumes you always wish to have exactly 6 values in each rolling window. We can alter this however, using the min_periods argument:\n\ndowjones['six_month_avg'] = dowjones.rolling(6, min_periods=1).Price.mean()\ndowjones.head(10)\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n      six_month_avg\n    \n  \n  \n    \n      0\n      1914-12-01\n      55.00\n      55.000000\n    \n    \n      1\n      1915-01-01\n      56.55\n      55.775000\n    \n    \n      2\n      1915-02-01\n      56.00\n      55.850000\n    \n    \n      3\n      1915-03-01\n      58.30\n      56.462500\n    \n    \n      4\n      1915-04-01\n      66.45\n      58.460000\n    \n    \n      5\n      1915-05-01\n      65.95\n      59.708333\n    \n    \n      6\n      1915-06-01\n      68.40\n      61.941667\n    \n    \n      7\n      1915-07-01\n      71.85\n      64.491667\n    \n    \n      8\n      1915-08-01\n      79.25\n      68.366667\n    \n    \n      9\n      1915-09-01\n      85.50\n      72.900000\n    \n  \n\n\n\n\nLet’s see what these rolling averages look like visually:\n\nimport matplotlib.pyplot as plt\n\n# add more averages\ndowjones['five_year'] = dowjones.rolling(60, min_periods=1).Price.mean()\n\n# plot the original data\nplt.plot(dowjones.Date, dowjones.Price, label='Actual Data')\n\n# plot the smoothed data\nplt.plot(dowjones.Date, dowjones.six_month_avg, label='Six Month')\nplt.plot(dowjones.Date, dowjones.five_year, label='Five Year')\n\n# add a legend\nplt.legend()\n\nplt.show()\n\n\n\n\nThe six month average is starting to smooth out the jagged peaks in the original data, while the five-year average has completely smoothed out all the wiggles!\n\n\nCumulative\nWhile the rolling capabilities will provide us a fixed window into which we can calculate aggregate statistics, we may also wish to calculate cumulative statistics. The most common cumulative statistic is the cumulative sum, which calculates the sum of the current row and all preceding rows. In Pandas, we can calculate this with the cumsum method.\n\ndowjones['cumulative_price'] = dowjones.Price.cumsum()\ndowjones.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n      six_month_avg\n      five_year\n      cumulative_price\n    \n  \n  \n    \n      0\n      1914-12-01\n      55.00\n      55.0000\n      55.0000\n      55.00\n    \n    \n      1\n      1915-01-01\n      56.55\n      55.7750\n      55.7750\n      111.55\n    \n    \n      2\n      1915-02-01\n      56.00\n      55.8500\n      55.8500\n      167.55\n    \n    \n      3\n      1915-03-01\n      58.30\n      56.4625\n      56.4625\n      225.85\n    \n    \n      4\n      1915-04-01\n      66.45\n      58.4600\n      58.4600\n      292.30\n    \n  \n\n\n\n\n\n\nShifts\nFinally, knowing the value preceding or following the current value is sometimes of interest. Forecast modeling will often use the current value and past values to predict future values, for example. To retrieve lead and lag values, Pandas has the shift method. This takes as an argument the number of periods by which we wish to shift our dataset (defaulting to 1). The periods can be negative, letting us get future values as well! Let’s look at this with the dowjones dataset again:\n\ndowjones['year_prior'] = dowjones.Price.shift(12)\ndowjones['year_after'] = dowjones.Price.shift(-12)\n\ndowjones.head()\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n      six_month_avg\n      five_year\n      cumulative_price\n      year_prior\n      year_after\n    \n  \n  \n    \n      0\n      1914-12-01\n      55.00\n      55.0000\n      55.0000\n      55.00\n      NaN\n      97.00\n    \n    \n      1\n      1915-01-01\n      56.55\n      55.7750\n      55.7750\n      111.55\n      NaN\n      94.70\n    \n    \n      2\n      1915-02-01\n      56.00\n      55.8500\n      55.8500\n      167.55\n      NaN\n      93.55\n    \n    \n      3\n      1915-03-01\n      58.30\n      56.4625\n      56.4625\n      225.85\n      NaN\n      93.30\n    \n    \n      4\n      1915-04-01\n      66.45\n      58.4600\n      58.4600\n      292.30\n      NaN\n      89.75\n    \n  \n\n\n\n\nTo show that this is actually the year before and after, let’s filter to a year somewhere in the middle.\n\n\n\n\n\n\nAdvanced Tip: Date Columns\n\n\n\n\n\nIn order to filter to the middle year, we need to be sure of the type of data we are working with. Let’s take a quick look at the Date column, and see what the datatype is:\n\nprint(dowjones.Date.dtype)\n\ndatetime64[ns]\n\n\nThis is a new column type that we have not encountered in detail before, but it tells us that this column contains date information. If we were to look at a single value in the Series, we would see that it is not simply a ‘YYYY-MM-DD’ string as displayed when we view the dataframe:\n\nsingle_date = dowjones.Date[0]\nprint(single_date)\nprint(type(single_date))\n\n1914-12-01 00:00:00\n<class 'pandas._libs.tslibs.timestamps.Timestamp'>\n\n\nBecause this is not a string, we cannot do a simple string comparison between two dates:\n\nprint(single_date == '1914-12-01')\n\nFalse\n\n\nTo filter based on date columns then, we should instead create a date type object in Python. The easiest way to do that is using the built in python library datetime.\n\nfrom datetime import datetime\n\n# create a date-like object\nnew_date = datetime(1914, 12, 1)\nprint(new_date)\n\n# compare with the single date\nprint(single_date == new_date)\n\n1914-12-01 00:00:00\nTrue\n\n\nFor more information on dates in Pandas, see the time series/date functionality page for Pandas, and the numpy documentation for datetime64.\n\n\n\n\nfrom datetime import datetime\n\n# create dates for the filter\ncurrent_year = datetime(1942, 8, 1)\nprevious_year = datetime(1941, 8, 1)\nnext_year = datetime(1943, 8, 1)\n\n# filter the date column\ndowjones[\n    dowjones.Date.isin([current_year, previous_year, next_year])\n][\n    ['Date', 'Price', 'year_prior', 'year_after']\n]\n\n\n\n\n\n  \n    \n      \n      Date\n      Price\n      year_prior\n      year_after\n    \n  \n  \n    \n      320\n      1941-08-01\n      126.55\n      125.35\n      106.20\n    \n    \n      332\n      1942-08-01\n      106.20\n      126.55\n      136.22\n    \n    \n      344\n      1943-08-01\n      136.22\n      106.20\n      146.93\n    \n  \n\n\n\n\nIf we look at these three lines, we see that the year_prior and year_after columns do in fact give us the corresponding values from the initial dataset. And if we plot this, we will see three copies of the data, each shifted by a year:\n\nplt.plot(dowjones.Date, dowjones.year_prior, label='past')\nplt.plot(dowjones.Date, dowjones.Price, label='current')\nplt.plot(dowjones.Date, dowjones.year_after, label='future')\n\nplt.legend()\n\nplt.show()\n\n\n\n\n\n\nChallenge 3\n\n\n\n\n\n\nChallenge 3\n\n\n\nFind the average, minimum, and maximum Dow Jones prices on a yearly basis. Plot all three values to see how they compare! (Recall: this dataset is currently monthly)\n\n\n\n\n\n\n\n\nSolution to Challenge 3\n\n\n\n\n\n\n# first let's find the three aggregations we want\ndowjones_rolling = dowjones.rolling(12, min_periods=1).agg(\n    {'Price': ['min', 'max', 'mean']}\n)\ndisplay(dowjones_rolling.head())\n\n# get rid of hierarchical naming scheme\ndowjones_rolling.columns = ['price_min', 'price_max', 'price_mean']\n\n# plot these (note that this DF doesn't have the dates, so we use the old DF dates)\n# here's a fancy plot that will show a coloured band!\nplt.fill_between(\n    dowjones.Date, \n    dowjones_rolling.price_min, \n    dowjones_rolling.price_max, \n    alpha=0.3 # alpha makes the band more transparent\n) \nplt.plot(dowjones.Date, dowjones_rolling.price_mean)\nplt.show()\n\n\n\n\n\n  \n    \n      \n      Price\n    \n    \n      \n      min\n      max\n      mean\n    \n  \n  \n    \n      0\n      55.0\n      55.00\n      55.0000\n    \n    \n      1\n      55.0\n      56.55\n      55.7750\n    \n    \n      2\n      55.0\n      56.55\n      55.8500\n    \n    \n      3\n      55.0\n      58.30\n      56.4625\n    \n    \n      4\n      55.0\n      66.45\n      58.4600"
  },
  {
    "objectID": "31_advanced_pandas.html#reshaping-data",
    "href": "31_advanced_pandas.html#reshaping-data",
    "title": "9  Advanced Pandas",
    "section": "9.4 Reshaping Data",
    "text": "9.4 Reshaping Data\n\n\n\nWide or Long?\n\n\nAnother fairly common task in data analysis is pivoting our data. Typically this means we are creating a wide-form table that has more columns than originally. This may be near the end of a particular analysis and we want to prepare a final table for a report, or perhaps we wish to pick out specific values from a column for more detailed inspection. Another common use of a pivot table might be for timeseries data, and we wish to separate different variables into their own individual columns. In these cases, we can use the pivot and pivot_table methods within Pandas.\nIn the other direction, we may wish to ‘unpivot’ a dataset. This will often occur early on in an analysis, where we wish to create a long-form table where multiple columns have been combined into a single set of id and value columns. This will be done with the melt method.\nLet’s explore these ideas with the gapminder dataset.\n\nurl = \"https://raw.githubusercontent.com/bcgov/\"\\\n    \"ds-intro-to-python/main/data/gapfinder.csv\"\n\ngapminder = pd.read_csv(url)\ngapminder.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n  \n\n\n\n\n\nLong to Wide\n\n\n\nPivoting a Table\n\n\n\npivot\nThe first thing we may wish to do is a simple pivot in which we make the table wider than it was before. The simplest way to do this is by the pivot method. This takes as input three arguments:\n\ncolumns: the only required argument! Column(s) we wish to use for the new columns of our dataframe. If more than one is provided, a hierarchy of columns is created.\nindex: the column(s) we wish to use for the index of our new dataframe. This is entirely optional, and will just default to the pre-existing index if not supplied.\nvalues: the value(s) we wish to retain for the new dataframe. This is also optional, and will use all leftover columns if nothing is provided. If more than one column is used here, it again creates a hierarchy in the columns.\n\nThe output of the pivot method is a new dataframe that has the requested index/columns, with the corresponding value associated with each index-column pair in the dataframe.\nFor instance, perhaps we wish to explore the population of each country individually overtime. In this case, we can pivot our gapminder dataset so that each of the countries in the original country column become their own column, and compare the population value against the year:\n\ngapminder_year_country = gapminder.pivot(\n    index='year',\n    columns='country',\n    values='pop'\n)\ngapminder_year_country.head()\n\n\n\n\n\n  \n    \n      country\n      Afghanistan\n      Albania\n      Algeria\n      Angola\n      Argentina\n      Australia\n      Austria\n      Bahrain\n      Bangladesh\n      Belgium\n      ...\n      Uganda\n      United Kingdom\n      United States\n      Uruguay\n      Venezuela\n      Vietnam\n      West Bank and Gaza\n      Yemen Rep.\n      Zambia\n      Zimbabwe\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      8425333.0\n      1282697.0\n      9279525.0\n      4232095.0\n      17876956.0\n      8691212.0\n      6927772.0\n      120447.0\n      46886859.0\n      8730405.0\n      ...\n      5824797.0\n      50430000.0\n      157553000.0\n      2252965.0\n      5439568.0\n      26246839.0\n      1030585.0\n      4963829.0\n      2672000.0\n      3080907.0\n    \n    \n      1957\n      9240934.0\n      1476505.0\n      10270856.0\n      4561361.0\n      19610538.0\n      9712569.0\n      6965860.0\n      138655.0\n      51365468.0\n      8989111.0\n      ...\n      6675501.0\n      51430000.0\n      171984000.0\n      2424959.0\n      6702668.0\n      28998543.0\n      1070439.0\n      5498090.0\n      3016000.0\n      3646340.0\n    \n    \n      1962\n      10267083.0\n      1728137.0\n      11000948.0\n      4826015.0\n      21283783.0\n      10794968.0\n      7129864.0\n      171863.0\n      56839289.0\n      9218400.0\n      ...\n      7688797.0\n      53292000.0\n      186538000.0\n      2598466.0\n      8143375.0\n      33796140.0\n      1133134.0\n      6120081.0\n      3421000.0\n      4277736.0\n    \n    \n      1967\n      11537966.0\n      1984060.0\n      12760499.0\n      5247469.0\n      22934225.0\n      11872264.0\n      7376998.0\n      202182.0\n      62821884.0\n      9556500.0\n      ...\n      8900294.0\n      54959000.0\n      198712000.0\n      2748579.0\n      9709552.0\n      39463910.0\n      1142636.0\n      6740785.0\n      3900000.0\n      4995432.0\n    \n    \n      1972\n      13079460.0\n      2263554.0\n      14760787.0\n      5894858.0\n      24779799.0\n      13177000.0\n      7544201.0\n      230800.0\n      70759295.0\n      9709100.0\n      ...\n      10190285.0\n      56079000.0\n      209896000.0\n      2829526.0\n      11515649.0\n      44655014.0\n      1089572.0\n      7407075.0\n      4506497.0\n      5861135.0\n    \n  \n\n5 rows × 142 columns\n\n\n\nNote that doing this moves ‘year’ into the index of the dataframe, and gives the columns the name of ‘country’. We can move ‘year’ back into the core of the dataframe using reset_index(), and remove the column name using the name attribute of the columns:\n\ngapminder_year_country = gapminder_year_country.reset_index()\ngapminder_year_country.columns.name = None\ngapminder_year_country.head()\n\n\n\n\n\n  \n    \n      \n      year\n      Afghanistan\n      Albania\n      Algeria\n      Angola\n      Argentina\n      Australia\n      Austria\n      Bahrain\n      Bangladesh\n      ...\n      Uganda\n      United Kingdom\n      United States\n      Uruguay\n      Venezuela\n      Vietnam\n      West Bank and Gaza\n      Yemen Rep.\n      Zambia\n      Zimbabwe\n    \n  \n  \n    \n      0\n      1952\n      8425333.0\n      1282697.0\n      9279525.0\n      4232095.0\n      17876956.0\n      8691212.0\n      6927772.0\n      120447.0\n      46886859.0\n      ...\n      5824797.0\n      50430000.0\n      157553000.0\n      2252965.0\n      5439568.0\n      26246839.0\n      1030585.0\n      4963829.0\n      2672000.0\n      3080907.0\n    \n    \n      1\n      1957\n      9240934.0\n      1476505.0\n      10270856.0\n      4561361.0\n      19610538.0\n      9712569.0\n      6965860.0\n      138655.0\n      51365468.0\n      ...\n      6675501.0\n      51430000.0\n      171984000.0\n      2424959.0\n      6702668.0\n      28998543.0\n      1070439.0\n      5498090.0\n      3016000.0\n      3646340.0\n    \n    \n      2\n      1962\n      10267083.0\n      1728137.0\n      11000948.0\n      4826015.0\n      21283783.0\n      10794968.0\n      7129864.0\n      171863.0\n      56839289.0\n      ...\n      7688797.0\n      53292000.0\n      186538000.0\n      2598466.0\n      8143375.0\n      33796140.0\n      1133134.0\n      6120081.0\n      3421000.0\n      4277736.0\n    \n    \n      3\n      1967\n      11537966.0\n      1984060.0\n      12760499.0\n      5247469.0\n      22934225.0\n      11872264.0\n      7376998.0\n      202182.0\n      62821884.0\n      ...\n      8900294.0\n      54959000.0\n      198712000.0\n      2748579.0\n      9709552.0\n      39463910.0\n      1142636.0\n      6740785.0\n      3900000.0\n      4995432.0\n    \n    \n      4\n      1972\n      13079460.0\n      2263554.0\n      14760787.0\n      5894858.0\n      24779799.0\n      13177000.0\n      7544201.0\n      230800.0\n      70759295.0\n      ...\n      10190285.0\n      56079000.0\n      209896000.0\n      2829526.0\n      11515649.0\n      44655014.0\n      1089572.0\n      7407075.0\n      4506497.0\n      5861135.0\n    \n  \n\n5 rows × 143 columns\n\n\n\nWe could make a slightly more complex dataset by including the continent as well in our new column scheme:\n\ngapminder_year_country_continent = gapminder.pivot(\n    index='year',\n    columns=['country', 'continent'],\n    values='pop'\n)\ngapminder_year_country_continent.head()\n\n\n\n\n\n  \n    \n      country\n      Afghanistan\n      Albania\n      Algeria\n      Angola\n      Argentina\n      Australia\n      Austria\n      Bahrain\n      Bangladesh\n      Belgium\n      ...\n      Uganda\n      United Kingdom\n      United States\n      Uruguay\n      Venezuela\n      Vietnam\n      West Bank and Gaza\n      Yemen Rep.\n      Zambia\n      Zimbabwe\n    \n    \n      continent\n      Asia\n      Europe\n      Africa\n      Africa\n      Americas\n      Oceania\n      Europe\n      Asia\n      Asia\n      Europe\n      ...\n      Africa\n      Europe\n      Americas\n      Americas\n      Americas\n      Asia\n      Asia\n      Asia\n      Africa\n      Africa\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      8425333.0\n      1282697.0\n      9279525.0\n      4232095.0\n      17876956.0\n      8691212.0\n      6927772.0\n      120447.0\n      46886859.0\n      8730405.0\n      ...\n      5824797.0\n      50430000.0\n      157553000.0\n      2252965.0\n      5439568.0\n      26246839.0\n      1030585.0\n      4963829.0\n      2672000.0\n      3080907.0\n    \n    \n      1957\n      9240934.0\n      1476505.0\n      10270856.0\n      4561361.0\n      19610538.0\n      9712569.0\n      6965860.0\n      138655.0\n      51365468.0\n      8989111.0\n      ...\n      6675501.0\n      51430000.0\n      171984000.0\n      2424959.0\n      6702668.0\n      28998543.0\n      1070439.0\n      5498090.0\n      3016000.0\n      3646340.0\n    \n    \n      1962\n      10267083.0\n      1728137.0\n      11000948.0\n      4826015.0\n      21283783.0\n      10794968.0\n      7129864.0\n      171863.0\n      56839289.0\n      9218400.0\n      ...\n      7688797.0\n      53292000.0\n      186538000.0\n      2598466.0\n      8143375.0\n      33796140.0\n      1133134.0\n      6120081.0\n      3421000.0\n      4277736.0\n    \n    \n      1967\n      11537966.0\n      1984060.0\n      12760499.0\n      5247469.0\n      22934225.0\n      11872264.0\n      7376998.0\n      202182.0\n      62821884.0\n      9556500.0\n      ...\n      8900294.0\n      54959000.0\n      198712000.0\n      2748579.0\n      9709552.0\n      39463910.0\n      1142636.0\n      6740785.0\n      3900000.0\n      4995432.0\n    \n    \n      1972\n      13079460.0\n      2263554.0\n      14760787.0\n      5894858.0\n      24779799.0\n      13177000.0\n      7544201.0\n      230800.0\n      70759295.0\n      9709100.0\n      ...\n      10190285.0\n      56079000.0\n      209896000.0\n      2829526.0\n      11515649.0\n      44655014.0\n      1089572.0\n      7407075.0\n      4506497.0\n      5861135.0\n    \n  \n\n5 rows × 142 columns\n\n\n\nOr by insisting that we retain information about more than just the population:\n\ngapminder_year_country_two_vals = gapminder.pivot(\n    index='year',\n    columns='country',\n    values=['pop', 'lifeExp']\n)\ngapminder_year_country_two_vals.head()\n\n\n\n\n\n  \n    \n      \n      pop\n      ...\n      lifeExp\n    \n    \n      country\n      Afghanistan\n      Albania\n      Algeria\n      Angola\n      Argentina\n      Australia\n      Austria\n      Bahrain\n      Bangladesh\n      Belgium\n      ...\n      Uganda\n      United Kingdom\n      United States\n      Uruguay\n      Venezuela\n      Vietnam\n      West Bank and Gaza\n      Yemen Rep.\n      Zambia\n      Zimbabwe\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      8425333.0\n      1282697.0\n      9279525.0\n      4232095.0\n      17876956.0\n      8691212.0\n      6927772.0\n      120447.0\n      46886859.0\n      8730405.0\n      ...\n      39.978\n      69.18\n      68.44\n      66.071\n      55.088\n      40.412\n      43.160\n      32.548\n      42.038\n      48.451\n    \n    \n      1957\n      9240934.0\n      1476505.0\n      10270856.0\n      4561361.0\n      19610538.0\n      9712569.0\n      6965860.0\n      138655.0\n      51365468.0\n      8989111.0\n      ...\n      42.571\n      70.42\n      69.49\n      67.044\n      57.907\n      42.887\n      45.671\n      33.970\n      44.077\n      50.469\n    \n    \n      1962\n      10267083.0\n      1728137.0\n      11000948.0\n      4826015.0\n      21283783.0\n      10794968.0\n      7129864.0\n      171863.0\n      56839289.0\n      9218400.0\n      ...\n      45.344\n      70.76\n      70.21\n      68.253\n      60.770\n      45.363\n      48.127\n      35.180\n      46.023\n      52.358\n    \n    \n      1967\n      11537966.0\n      1984060.0\n      12760499.0\n      5247469.0\n      22934225.0\n      11872264.0\n      7376998.0\n      202182.0\n      62821884.0\n      9556500.0\n      ...\n      48.051\n      71.36\n      70.76\n      68.468\n      63.479\n      47.838\n      51.631\n      36.984\n      47.768\n      53.995\n    \n    \n      1972\n      13079460.0\n      2263554.0\n      14760787.0\n      5894858.0\n      24779799.0\n      13177000.0\n      7544201.0\n      230800.0\n      70759295.0\n      9709100.0\n      ...\n      51.016\n      72.01\n      71.34\n      68.673\n      65.712\n      50.254\n      56.532\n      39.848\n      50.107\n      55.635\n    \n  \n\n5 rows × 284 columns\n\n\n\nWhat if we wish to know about the average population of each continent over time? If we try the same approach as above, we will immediately run into an issue:\n\ngapminder_year_continent = gapminder.pivot(\n    index='year',\n    columns='continent',\n    values='pop'\n)\n\nValueError: Index contains duplicate entries, cannot reshape\n\n\nHere we find that there is more than one value associated with each year-continent pairing, and so the pivot method does not know how to assign a value to the dataframe. This is because pivot is for unique values only. While it is an excellent simple-use tool, we can go one step further to deal with non-unique values.\n\n\npivot_table\nTo address the year-continent question above, we could first group our data by continent, calculate the average population every year, and then pivot the resultant dataframe. This is a completely valid method! However, pandas also offers a built in method that will do aggregations and pivots all at once. This is similar to the functionality of Excel pivot tables, and includes some additional functionality we would not find in groupby alone. Some of the important arguments that we can use in the pivot_table method include:\n\ncolumns: the only required argument! Column(s) (and values within) we wish to use for the resultant dataframes columns.\nvalues: the column(s) we wish to aggregate. If no columns are provided, all leftover (non column/index columns) columns will be used that make ‘sense’. For example, the average value of a list of strings does not make sense, but the ‘max’ value of a string list could be calculated.\nindex: the column(s) we wish to use for the resultant dataframes index\naggfunc: the method of aggregation we wish to use. Default is to calculate the average.\nfill_value: the value to replace missing values with (after aggregation).\n\nLet’s go through some examples with increasing complexity. The simplest thing we can do is pivot a single column, to which we will get an output for the averages of all numerical columns.\n\ngapminder.pivot_table(\n    columns='continent'\n)\n\n\n\n\n\n  \n    \n      continent\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n    \n  \n  \n    \n      gdpPercap\n      2.193755e+03\n      7.136110e+03\n      7.902150e+03\n      1.446948e+04\n      1.862161e+04\n    \n    \n      lifeExp\n      4.886533e+01\n      6.465874e+01\n      6.006490e+01\n      7.190369e+01\n      7.432621e+01\n    \n    \n      pop\n      9.916003e+06\n      2.450479e+07\n      7.703872e+07\n      1.716976e+07\n      8.874672e+06\n    \n    \n      year\n      1.979500e+03\n      1.979500e+03\n      1.979500e+03\n      1.979500e+03\n      1.979500e+03\n    \n  \n\n\n\n\nNext we may wish to add an index, and focus on a single value for our output. This matches the question we asked above and could not answer with the pivot method:\n\ngapminder_year_continent = gapminder.pivot_table(\n    index='year',\n    columns='continent',\n    values='pop'\n)\ngapminder_year_continent.head()\n\n\n\n\n\n  \n    \n      continent\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n    \n    \n      year\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      4.570010e+06\n      13806097.84\n      4.228356e+07\n      1.393736e+07\n      5343003.0\n    \n    \n      1957\n      5.093033e+06\n      15478156.64\n      4.735699e+07\n      1.459635e+07\n      5970988.0\n    \n    \n      1962\n      5.702247e+06\n      17330810.16\n      5.140476e+07\n      1.534517e+07\n      6641759.0\n    \n    \n      1967\n      6.447875e+06\n      19229864.92\n      5.774736e+07\n      1.603930e+07\n      7300207.0\n    \n    \n      1972\n      7.305376e+06\n      21175368.40\n      6.518098e+07\n      1.668784e+07\n      8053050.0\n    \n  \n\n\n\n\nIf we tried to do the same calculation, but use a string column instead of a numerical column for the values we will get a warning about trying to use invalid columns:\n\ngapminder_year_continent_country = gapminder.pivot_table(\n    index='year',\n    columns='continent',\n    values='country'\n)\ngapminder_year_continent_country.head()\n\n/tmp/ipykernel_2671/2473994706.py:1: FutureWarning: Dropping invalid columns in DataFrameGroupBy.mean is deprecated. In a future version, a TypeError will be raised. Before calling .mean, select only columns which should be valid for the function.\n  gapminder_year_continent_country = gapminder.pivot_table(\n\n\n\n\n\n\n  \n    \n      continent\n    \n    \n      year\n    \n  \n  \n    \n      1952\n    \n    \n      1957\n    \n    \n      1962\n    \n    \n      1967\n    \n    \n      1972\n    \n  \n\n\n\n\nHowever, we can switch our aggregation method to something that is valid for strings (using either string arguments for common aggregations such as ‘max’, or more complicated numpy or user-defined functions as well).\n\ngapminder_year_continent_country = gapminder.pivot_table(\n    index='year',\n    columns='continent',\n    values='country',\n    aggfunc='max'\n)\ngapminder_year_continent_country.head()\n\n\n\n\n\n  \n    \n      continent\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n    \n    \n      year\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      Zimbabwe\n      Venezuela\n      Yemen Rep.\n      United Kingdom\n      New Zealand\n    \n    \n      1957\n      Zimbabwe\n      Venezuela\n      Yemen Rep.\n      United Kingdom\n      New Zealand\n    \n    \n      1962\n      Zimbabwe\n      Venezuela\n      Yemen Rep.\n      United Kingdom\n      New Zealand\n    \n    \n      1967\n      Zimbabwe\n      Venezuela\n      Yemen Rep.\n      United Kingdom\n      New Zealand\n    \n    \n      1972\n      Zimbabwe\n      Venezuela\n      Yemen Rep.\n      United Kingdom\n      New Zealand\n    \n  \n\n\n\n\nFinally, similar to our groupby aggregations, we can also pass a dictionary to aggfunc. The dictionary contains the columns we wish to use for values as keys, and the type of aggregation(s) we wish to do as the values.\n\nimport numpy as np\n\ngapminder_multiple_pivots = gapminder.pivot_table(\n    index='year',\n    columns='continent',\n    aggfunc={\n        'pop': [np.mean, np.max],\n        'lifeExp': 'mean',\n        'country': [min, max]\n    }\n)\ngapminder_multiple_pivots.head()\n\n\n\n\n\n  \n    \n      \n      country\n      ...\n      pop\n    \n    \n      \n      max\n      min\n      ...\n      amax\n      mean\n    \n    \n      continent\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n      ...\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n    \n    \n      year\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      Zimbabwe\n      Venezuela\n      Yemen Rep.\n      United Kingdom\n      New Zealand\n      Algeria\n      Argentina\n      Afghanistan\n      Albania\n      Australia\n      ...\n      33119096.0\n      157553000.0\n      5.562635e+08\n      69145952.0\n      8691212.0\n      4.570010e+06\n      13806097.84\n      4.228356e+07\n      1.393736e+07\n      5343003.0\n    \n    \n      1957\n      Zimbabwe\n      Venezuela\n      Yemen Rep.\n      United Kingdom\n      New Zealand\n      Algeria\n      Argentina\n      Afghanistan\n      Albania\n      Australia\n      ...\n      37173340.0\n      171984000.0\n      6.374080e+08\n      71019069.0\n      9712569.0\n      5.093033e+06\n      15478156.64\n      4.735699e+07\n      1.459635e+07\n      5970988.0\n    \n    \n      1962\n      Zimbabwe\n      Venezuela\n      Yemen Rep.\n      United Kingdom\n      New Zealand\n      Algeria\n      Argentina\n      Afghanistan\n      Albania\n      Australia\n      ...\n      41871351.0\n      186538000.0\n      6.657700e+08\n      73739117.0\n      10794968.0\n      5.702247e+06\n      17330810.16\n      5.140476e+07\n      1.534517e+07\n      6641759.0\n    \n    \n      1967\n      Zimbabwe\n      Venezuela\n      Yemen Rep.\n      United Kingdom\n      New Zealand\n      Algeria\n      Argentina\n      Afghanistan\n      Albania\n      Australia\n      ...\n      47287752.0\n      198712000.0\n      7.545500e+08\n      76368453.0\n      11872264.0\n      6.447875e+06\n      19229864.92\n      5.774736e+07\n      1.603930e+07\n      7300207.0\n    \n    \n      1972\n      Zimbabwe\n      Venezuela\n      Yemen Rep.\n      United Kingdom\n      New Zealand\n      Algeria\n      Argentina\n      Afghanistan\n      Albania\n      Australia\n      ...\n      53740085.0\n      209896000.0\n      8.620300e+08\n      78717088.0\n      13177000.0\n      7.305376e+06\n      21175368.40\n      6.518098e+07\n      1.668784e+07\n      8053050.0\n    \n  \n\n5 rows × 25 columns\n\n\n\n\n\n\nWide to Long\n\n\n\nI’m meeelllting!\n\n\n\nmelt\nThe opposite of pivoting is melting (…obviously?). This is often used to get data back into long-form. Long-form data will typically have all different possible categories for a single measure in a single column (having a country column instead of Algeria, Albania, Canada… Zimbabwe columns). Long-form data is typically the preferred data style when doing your actual analysis. For example, many of the Seaborns methods we considered for creating stunning visuals assume that the data is in long-form, and is optimized to split out categories internally instead of pulling from multiple different columns.\nHowever, as our data does not always come in the way we want it, we might have to unpivot (melt) our data to get it into long-form first. The melt method is how we attack this in Pandas, and it uses the following arguments:\n\nid_vars: This is a required argument! Column(s) that will be used as identifiers. These are columns that we do not wish to unpivot, but leave as is.\nvalue_vars: Column(s) to unpivot. If left out, the default is to use all non-id columns.\n\nLet’s look at one of the pivoted tables we made earlier, and try to get it back into long form:\n\ngapminder_year_country.head()\n\n\n\n\n\n  \n    \n      \n      year\n      Afghanistan\n      Albania\n      Algeria\n      Angola\n      Argentina\n      Australia\n      Austria\n      Bahrain\n      Bangladesh\n      ...\n      Uganda\n      United Kingdom\n      United States\n      Uruguay\n      Venezuela\n      Vietnam\n      West Bank and Gaza\n      Yemen Rep.\n      Zambia\n      Zimbabwe\n    \n  \n  \n    \n      0\n      1952\n      8425333.0\n      1282697.0\n      9279525.0\n      4232095.0\n      17876956.0\n      8691212.0\n      6927772.0\n      120447.0\n      46886859.0\n      ...\n      5824797.0\n      50430000.0\n      157553000.0\n      2252965.0\n      5439568.0\n      26246839.0\n      1030585.0\n      4963829.0\n      2672000.0\n      3080907.0\n    \n    \n      1\n      1957\n      9240934.0\n      1476505.0\n      10270856.0\n      4561361.0\n      19610538.0\n      9712569.0\n      6965860.0\n      138655.0\n      51365468.0\n      ...\n      6675501.0\n      51430000.0\n      171984000.0\n      2424959.0\n      6702668.0\n      28998543.0\n      1070439.0\n      5498090.0\n      3016000.0\n      3646340.0\n    \n    \n      2\n      1962\n      10267083.0\n      1728137.0\n      11000948.0\n      4826015.0\n      21283783.0\n      10794968.0\n      7129864.0\n      171863.0\n      56839289.0\n      ...\n      7688797.0\n      53292000.0\n      186538000.0\n      2598466.0\n      8143375.0\n      33796140.0\n      1133134.0\n      6120081.0\n      3421000.0\n      4277736.0\n    \n    \n      3\n      1967\n      11537966.0\n      1984060.0\n      12760499.0\n      5247469.0\n      22934225.0\n      11872264.0\n      7376998.0\n      202182.0\n      62821884.0\n      ...\n      8900294.0\n      54959000.0\n      198712000.0\n      2748579.0\n      9709552.0\n      39463910.0\n      1142636.0\n      6740785.0\n      3900000.0\n      4995432.0\n    \n    \n      4\n      1972\n      13079460.0\n      2263554.0\n      14760787.0\n      5894858.0\n      24779799.0\n      13177000.0\n      7544201.0\n      230800.0\n      70759295.0\n      ...\n      10190285.0\n      56079000.0\n      209896000.0\n      2829526.0\n      11515649.0\n      44655014.0\n      1089572.0\n      7407075.0\n      4506497.0\n      5861135.0\n    \n  \n\n5 rows × 143 columns\n\n\n\n\ngapminder_year_country.melt(id_vars='year').head()\n\n\n\n\n\n  \n    \n      \n      year\n      variable\n      value\n    \n  \n  \n    \n      0\n      1952\n      Afghanistan\n      8425333.0\n    \n    \n      1\n      1957\n      Afghanistan\n      9240934.0\n    \n    \n      2\n      1962\n      Afghanistan\n      10267083.0\n    \n    \n      3\n      1967\n      Afghanistan\n      11537966.0\n    \n    \n      4\n      1972\n      Afghanistan\n      13079460.0\n    \n  \n\n\n\n\nIf we wish to only keep a subset of the countries, we can do that too. We can also change the name of the resultant variable and value columns:\n\ngapminder_year_country.melt(\n    id_vars='year',\n    value_vars = ['Albania', 'Canada', 'Zimbabwe'],\n    var_name = 'country',\n    value_name = 'pop'\n).head()\n\n\n\n\n\n  \n    \n      \n      year\n      country\n      pop\n    \n  \n  \n    \n      0\n      1952\n      Albania\n      1282697.0\n    \n    \n      1\n      1957\n      Albania\n      1476505.0\n    \n    \n      2\n      1962\n      Albania\n      1728137.0\n    \n    \n      3\n      1967\n      Albania\n      1984060.0\n    \n    \n      4\n      1972\n      Albania\n      2263554.0\n    \n  \n\n\n\n\n\n\n\n\n\n\nOvermelting!\n\n\n\n\n\nWhen melting our data, we want to be careful, and not ‘overmelt’! Think back to the original gapminder dataset:\n\ngapminder.head()\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      pop\n      continent\n      lifeExp\n      gdpPercap\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      8425333.0\n      Asia\n      28.801\n      779.445314\n    \n    \n      1\n      Afghanistan\n      1957\n      9240934.0\n      Asia\n      30.332\n      820.853030\n    \n    \n      2\n      Afghanistan\n      1962\n      10267083.0\n      Asia\n      31.997\n      853.100710\n    \n    \n      3\n      Afghanistan\n      1967\n      11537966.0\n      Asia\n      34.020\n      836.197138\n    \n    \n      4\n      Afghanistan\n      1972\n      13079460.0\n      Asia\n      36.088\n      739.981106\n    \n  \n\n\n\n\nWould the following melt make sense?\n\ngapminder.melt(\n    id_vars=['country', 'year', 'continent']\n)\n\n\n\n\n\n  \n    \n      \n      country\n      year\n      continent\n      variable\n      value\n    \n  \n  \n    \n      0\n      Afghanistan\n      1952\n      Asia\n      pop\n      8.425333e+06\n    \n    \n      1\n      Afghanistan\n      1957\n      Asia\n      pop\n      9.240934e+06\n    \n    \n      2\n      Afghanistan\n      1962\n      Asia\n      pop\n      1.026708e+07\n    \n    \n      3\n      Afghanistan\n      1967\n      Asia\n      pop\n      1.153797e+07\n    \n    \n      4\n      Afghanistan\n      1972\n      Asia\n      pop\n      1.307946e+07\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      5107\n      Zimbabwe\n      1987\n      Africa\n      gdpPercap\n      7.061573e+02\n    \n    \n      5108\n      Zimbabwe\n      1992\n      Africa\n      gdpPercap\n      6.934208e+02\n    \n    \n      5109\n      Zimbabwe\n      1997\n      Africa\n      gdpPercap\n      7.924500e+02\n    \n    \n      5110\n      Zimbabwe\n      2002\n      Africa\n      gdpPercap\n      6.720386e+02\n    \n    \n      5111\n      Zimbabwe\n      2007\n      Africa\n      gdpPercap\n      4.697093e+02\n    \n  \n\n5112 rows × 5 columns\n\n\n\nProbably not. Having the population, GDP, and life expectancy all in the same column might lead to confusing results. As a general rule of thumb, including multiple unique measures that have different units (population is measured in number of people, life expectancy in years, and GDP per capita in dollars per person) in the same column is typically avoided. Another way to think of it is this: if I take the average of the entire column, will that actually make sense? Try not to melt further than necessary! Striking a balance between machine-preferred (entirely long-form) and human-preferred (wide-form) is an important and useful skill when creating scripts to analyse our datasets!\n\n\n\nThese examples only touch the surface of methods to reshape our data. To explore even further, Pandas provides a user guide to reshaping. Check this out for even more reshaping functionality!\n\n\n\nChallenge 4\n\n\n\n\n\n\nChallenge 4\n\n\n\nCreate a pivot table that counts the number of countries in each continent that has a population greater than 1,000,000 in any given year.\n\n\n\n\n\n\n\n\nSolution to Challenge 4\n\n\n\n\n\nFirst, let’s demonstrate this using the methods we’ve learned about to reshape our data. We can create this aggregated pivot table using the pivot_table method, and supplying ‘count’ to the aggfunc method.\n\ngapminder[gapminder['pop']>1_000_000].pivot_table(\n    index='year',\n    columns='continent',\n    values='country',\n    aggfunc='count'\n)\n\n\n\n\n\n  \n    \n      continent\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n    \n    \n      year\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      37\n      22\n      28\n      28\n      2\n    \n    \n      1957\n      37\n      24\n      28\n      28\n      2\n    \n    \n      1962\n      39\n      24\n      29\n      28\n      2\n    \n    \n      1967\n      39\n      24\n      30\n      28\n      2\n    \n    \n      1972\n      40\n      24\n      30\n      28\n      2\n    \n    \n      1977\n      40\n      25\n      32\n      28\n      2\n    \n    \n      1982\n      41\n      25\n      32\n      28\n      2\n    \n    \n      1987\n      43\n      25\n      32\n      28\n      2\n    \n    \n      1992\n      45\n      25\n      32\n      28\n      2\n    \n    \n      1997\n      47\n      25\n      32\n      28\n      2\n    \n    \n      2002\n      47\n      25\n      32\n      28\n      2\n    \n    \n      2007\n      47\n      25\n      32\n      28\n      2\n    \n  \n\n\n\n\nThis type of table is commonly referred to as a crosstable: it gives us the frequency of occurences of items under different groupings. Because it is such a widely used table, it was given its own special method in Pandas: crosstab. Basic functionality is to supply two Series that we wish to cross-tabulate to the method, and let it do all the work. Note that this is a Pandas specific method, not a dataframe specific method!\n\ngapminder_filtered = gapminder[gapminder['pop']>1_000_000]\npd.crosstab(\n    index=gapminder_filtered['year'],\n    columns=gapminder_filtered['continent']\n)\n\n\n\n\n\n  \n    \n      continent\n      Africa\n      Americas\n      Asia\n      Europe\n      Oceania\n    \n    \n      year\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1952\n      37\n      22\n      28\n      28\n      2\n    \n    \n      1957\n      37\n      24\n      28\n      28\n      2\n    \n    \n      1962\n      39\n      24\n      29\n      28\n      2\n    \n    \n      1967\n      39\n      24\n      30\n      28\n      2\n    \n    \n      1972\n      40\n      24\n      30\n      28\n      2\n    \n    \n      1977\n      40\n      25\n      32\n      28\n      2\n    \n    \n      1982\n      41\n      25\n      32\n      28\n      2\n    \n    \n      1987\n      43\n      25\n      32\n      28\n      2\n    \n    \n      1992\n      45\n      25\n      32\n      28\n      2\n    \n    \n      1997\n      47\n      25\n      32\n      28\n      2\n    \n    \n      2002\n      47\n      25\n      32\n      28\n      2\n    \n    \n      2007\n      47\n      25\n      32\n      28\n      2"
  },
  {
    "objectID": "references.html#references",
    "href": "references.html#references",
    "title": "References",
    "section": "References",
    "text": "References\n\n\nAllen Lee, Sourav Singh, Nathan Moore. 2018. “Software Carpentry:\nPlotting and Programming in Python.” https://github.com/swcarpentry/python-novice-inflammation.\n\n\nAzalee Bostroem, Valentina Staneva, Trevor Bekolay. 2016.\n“Software Carpentry: Programming with Python.” Version\n2016.06, 10.5281/zenodo.57492. https://github.com/swcarpentry/python-novice-inflammation.\n\n\nHoltz, Yan. 2018. “The Python Graph Gallery.” https://www.python-graph-gallery.com/.\n\n\nIvan Gonzalez, Nima Hejazi, Daisie Huang. 2019. “Software\nCarpentry: Version Control with Git.” Version 2019.06.1,\n10.5281/zenodo.3264950. https://github.com/swcarpentry/git-novice.\n\n\nNorris, Simon. 2022. “bcdata Python\nPackage.” https://pypi.org/project/bcdata/#description.\n\n\nWaskom, Michael. 2012. “Seaborn Tutorials.” https://seaborn.pydata.org/tutorial.html."
  },
  {
    "objectID": "references.html#cheat-sheets",
    "href": "references.html#cheat-sheets",
    "title": "References",
    "section": "Cheat Sheets",
    "text": "Cheat Sheets\nThere are many useful cheatsheets that will aggregate commonly used functions and commands into one location. datacamp hosts many of these cheatsheets for Python as well as other languages and programs. The cheatsheet itself is typically a 1-2 page PDF that can be easily referenced for standard functions. Some of the cheatsheets that reference materials in this course include:\n\nGetting started with Python\nPython for Data Science\nPandas: Data Wrangling\nMatplotlib\nSeaborn"
  }
]